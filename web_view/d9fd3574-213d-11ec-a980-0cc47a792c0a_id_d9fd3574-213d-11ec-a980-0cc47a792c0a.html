<html>
<head><meta http-equiv=Content-Type content="text/html; charset=UTF-8">
<style type="text/css">
<!--
span.cls_002{font-family:Arial,serif;font-size:12.0px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_002{font-family:Arial,serif;font-size:12.0px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_003{font-family:Arial,serif;font-size:20.7px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_003{font-family:Arial,serif;font-size:20.7px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_004{font-family:Arial,serif;font-size:17.3px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_004{font-family:Arial,serif;font-size:17.3px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_005{font-family:Arial,serif;font-size:14.4px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_005{font-family:Arial,serif;font-size:14.4px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_006{font-family:Arial,serif;font-size:11.0px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_006{font-family:Arial,serif;font-size:11.0px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_046{font-family:Arial,serif;font-size:11.0px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: underline}
div.cls_046{font-family:Arial,serif;font-size:11.0px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_047{font-family:"Verdana Bold",serif;font-size:9.1px;color:rgb(0,0,0);font-weight:bold;font-style:normal;text-decoration: underline}
div.cls_047{font-family:"Verdana Bold",serif;font-size:9.1px;color:rgb(0,0,0);font-weight:bold;font-style:normal;text-decoration: none}
span.cls_009{font-family:"Verdana",serif;font-size:9.1px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_009{font-family:"Verdana",serif;font-size:9.1px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_010{font-family:Arial,serif;font-size:9.1px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_010{font-family:Arial,serif;font-size:9.1px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_048{font-family:"Verdana",serif;font-size:9.1px;color:rgb(4,98,193);font-weight:normal;font-style:normal;text-decoration: underline}
div.cls_048{font-family:"Verdana",serif;font-size:9.1px;color:rgb(4,98,193);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_008{font-family:"Verdana Bold Italic",serif;font-size:9.1px;color:rgb(0,0,0);font-weight:bold;font-style:italic;text-decoration: none}
div.cls_008{font-family:"Verdana Bold Italic",serif;font-size:9.1px;color:rgb(0,0,0);font-weight:bold;font-style:italic;text-decoration: none}
span.cls_049{font-family:"Verdana Bold Italic",serif;font-size:9.1px;color:rgb(0,0,0);font-weight:bold;font-style:italic;text-decoration: underline}
div.cls_049{font-family:"Verdana Bold Italic",serif;font-size:9.1px;color:rgb(0,0,0);font-weight:bold;font-style:italic;text-decoration: none}
span.cls_050{font-family:"Verdana",serif;font-size:9.1px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: underline}
div.cls_050{font-family:"Verdana",serif;font-size:9.1px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_007{font-family:"Verdana Bold",serif;font-size:9.1px;color:rgb(0,0,0);font-weight:bold;font-style:normal;text-decoration: none}
div.cls_007{font-family:"Verdana Bold",serif;font-size:9.1px;color:rgb(0,0,0);font-weight:bold;font-style:normal;text-decoration: none}
span.cls_013{font-family:"Verdana Italic",serif;font-size:9.1px;color:rgb(0,0,0);font-weight:normal;font-style:italic;text-decoration: none}
div.cls_013{font-family:"Verdana Italic",serif;font-size:9.1px;color:rgb(0,0,0);font-weight:normal;font-style:italic;text-decoration: none}
span.cls_015{font-family:"Calibri Italic",serif;font-size:10.0px;color:rgb(0,0,0);font-weight:normal;font-style:italic;text-decoration: none}
div.cls_015{font-family:"Calibri Italic",serif;font-size:10.0px;color:rgb(0,0,0);font-weight:normal;font-style:italic;text-decoration: none}
span.cls_051{font-family:"Calibri Italic",serif;font-size:10.0px;color:rgb(4,98,193);font-weight:normal;font-style:italic;text-decoration: underline}
div.cls_051{font-family:"Calibri Italic",serif;font-size:10.0px;color:rgb(4,98,193);font-weight:normal;font-style:italic;text-decoration: none}
span.cls_017{font-family:Arial,serif;font-size:24.8px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_017{font-family:Arial,serif;font-size:24.8px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_018{font-family:Arial,serif;font-size:10.0px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_018{font-family:Arial,serif;font-size:10.0px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_019{font-family:Arial,serif;font-size:3.6px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_019{font-family:Arial,serif;font-size:3.6px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_020{font-family:Arial,serif;font-size:3.6px;color:rgb(51,51,51);font-weight:bold;font-style:normal;text-decoration: none}
div.cls_020{font-family:Arial,serif;font-size:3.6px;color:rgb(51,51,51);font-weight:bold;font-style:normal;text-decoration: none}
span.cls_021{font-family:Arial,serif;font-size:3.3px;color:rgb(51,51,51);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_021{font-family:Arial,serif;font-size:3.3px;color:rgb(51,51,51);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_023{font-family:Arial,serif;font-size:7.1px;color:rgb(51,51,51);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_023{font-family:Arial,serif;font-size:7.1px;color:rgb(51,51,51);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_024{font-family:Arial,serif;font-size:7.1px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_024{font-family:Arial,serif;font-size:7.1px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_025{font-family:Arial,serif;font-size:9.0px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_025{font-family:Arial,serif;font-size:9.0px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_026{font-family:Arial,serif;font-size:8.0px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_026{font-family:Arial,serif;font-size:8.0px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_027{font-family:Arial,serif;font-size:5.3px;color:rgb(51,51,51);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_027{font-family:Arial,serif;font-size:5.3px;color:rgb(51,51,51);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_028{font-family:Arial,serif;font-size:5.3px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_028{font-family:Arial,serif;font-size:5.3px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_029{font-family:Arial,serif;font-size:4.7px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_029{font-family:Arial,serif;font-size:4.7px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_030{font-family:Arial,serif;font-size:7.0px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_030{font-family:Arial,serif;font-size:7.0px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_031{font-family:Arial,serif;font-size:4.9px;color:rgb(51,51,51);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_031{font-family:Arial,serif;font-size:4.9px;color:rgb(51,51,51);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_032{font-family:Arial,serif;font-size:2.5px;color:rgb(51,51,51);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_032{font-family:Arial,serif;font-size:2.5px;color:rgb(51,51,51);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_033{font-family:Arial,serif;font-size:2.5px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_033{font-family:Arial,serif;font-size:2.5px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_034{font-family:Arial,serif;font-size:2.5px;color:rgb(0,102,0);font-weight:bold;font-style:normal;text-decoration: none}
div.cls_034{font-family:Arial,serif;font-size:2.5px;color:rgb(0,102,0);font-weight:bold;font-style:normal;text-decoration: none}
span.cls_035{font-family:Arial,serif;font-size:6.1px;color:rgb(0,0,0);font-weight:bold;font-style:normal;text-decoration: none}
div.cls_035{font-family:Arial,serif;font-size:6.1px;color:rgb(0,0,0);font-weight:bold;font-style:normal;text-decoration: none}
span.cls_036{font-family:Arial,serif;font-size:4.3px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_036{font-family:Arial,serif;font-size:4.3px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_037{font-family:Arial,serif;font-size:4.3px;color:rgb(0,0,0);font-weight:bold;font-style:normal;text-decoration: none}
div.cls_037{font-family:Arial,serif;font-size:4.3px;color:rgb(0,0,0);font-weight:bold;font-style:normal;text-decoration: none}
span.cls_038{font-family:Arial,serif;font-size:4.3px;color:rgb(51,51,51);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_038{font-family:Arial,serif;font-size:4.3px;color:rgb(51,51,51);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_039{font-family:Arial,serif;font-size:6.9px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_039{font-family:Arial,serif;font-size:6.9px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_040{font-family:Arial,serif;font-size:5.8px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_040{font-family:Arial,serif;font-size:5.8px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_041{font-family:Arial,serif;font-size:6.9px;color:rgb(0,0,0);font-weight:bold;font-style:normal;text-decoration: none}
div.cls_041{font-family:Arial,serif;font-size:6.9px;color:rgb(0,0,0);font-weight:bold;font-style:normal;text-decoration: none}
span.cls_042{font-family:Arial,serif;font-size:5.8px;color:rgb(0,0,0);font-weight:bold;font-style:normal;text-decoration: none}
div.cls_042{font-family:Arial,serif;font-size:5.8px;color:rgb(0,0,0);font-weight:bold;font-style:normal;text-decoration: none}
span.cls_043{font-family:Arial,serif;font-size:4.8px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_043{font-family:Arial,serif;font-size:4.8px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_044{font-family:Arial,serif;font-size:5.3px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_044{font-family:Arial,serif;font-size:5.3px;color:rgb(0,0,0);font-weight:normal;font-style:normal;text-decoration: none}
span.cls_045{font-family:Arial,serif;font-size:5.3px;color:rgb(51,51,51);font-weight:normal;font-style:normal;text-decoration: none}
div.cls_045{font-family:Arial,serif;font-size:5.3px;color:rgb(51,51,51);font-weight:normal;font-style:normal;text-decoration: none}
-->
</style>
<script type="text/javascript" src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/wz_jsgraphics.js"></script>
</head>
<body>
<div style="position:absolute;left:50%;margin-left:-297px;top:0px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background01.jpg" width=595 height=842></div>
<div style="position:absolute;left:246.67px;top:146.69px" class="cls_002"><span class="cls_002">U</span><A HREF="http://www.soton.ac.uk">niversity of Southampton</A> </div>
<div style="position:absolute;left:200.71px;top:177.78px" class="cls_002"><span class="cls_002"> </span><A HREF="http://www.engineering.soton.ac.uk">Faculty of Engineering and Physical Sciences</A> </div>
<div style="position:absolute;left:230.32px;top:208.87px" class="cls_002"><span class="cls_002"> </span><A HREF="http://www.ecs.soton.ac.uk">Electronics and Computer Science</A> </div>
<div style="position:absolute;left:164.41px;top:301.24px" class="cls_003"><span class="cls_003">Artificial Intelligence for Web</span></div>
<div style="position:absolute;left:252.67px;top:331.46px" class="cls_003"><span class="cls_003">Accessibility</span></div>
<div style="position:absolute;left:309.78px;top:472.79px" class="cls_002"><span class="cls_002">by</span></div>
<div style="position:absolute;left:271.41px;top:497.11px" class="cls_004"><span class="cls_004"> </span><A HREF="ss8n18@soton.ac.uk">Shaunak Sen</A> </div>
<div style="position:absolute;left:274.89px;top:524.72px" class="cls_005"><span class="cls_005">January 2020</span></div>
<div style="position:absolute;left:241.71px;top:659.74px" class="cls_002"><span class="cls_002">Supervisor: Prof. Mike Wald</span></div>
<div style="position:absolute;left:202.40px;top:676.66px" class="cls_002"><span class="cls_002">Second Examiner: Prof. Timothy J Norman</span></div>
<div style="position:absolute;left:165.31px;top:705.54px" class="cls_002"><span class="cls_002">A dissertation submitted in partial fulfilment of the degree</span></div>
<div style="position:absolute;left:261.41px;top:722.45px" class="cls_002"><span class="cls_002">of MSC Data Science</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:852px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background02.jpg" width=595 height=842></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:852px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background03.jpg" width=595 height=842></div>
<div style="position:absolute;left:251.37px;top:164.14px" class="cls_006"><span class="cls_006">U</span><A HREF="http://www.soton.ac.uk">niversity of Southampton</A> </div>
<div style="position:absolute;left:285.20px;top:192.53px" class="cls_046"><span class="cls_046">ABSTRACT</span></div>
<div style="position:absolute;left:165.12px;top:220.92px" class="cls_006"><span class="cls_006"> </span><A HREF="http://www.engineering.soton.ac.uk">FACULTY OF ENGINEERING AND PHYSICAL SCIENCES</A> </div>
<div style="position:absolute;left:201.26px;top:237.35px" class="cls_006"><span class="cls_006"> </span><A HREF="http://www.ecs.soton.ac.uk">ELECTRONICS AND COMPUTER SCIENCE</A> </div>
<div style="position:absolute;left:274.09px;top:265.74px" class="cls_046"><span class="cls_046">Master of Science</span></div>
<div style="position:absolute;left:277.62px;top:300.11px" class="cls_006"><span class="cls_006">by </span><A HREF="ss8n18@soton.ac.uk">Shaunak Sen</A> </div>
<div style="position:absolute;left:108.00px;top:349.85px" class="cls_006"><span class="cls_006">This project aims to explore how Artificial Intelligence (AI) techniques like Deep Learn-</span></div>
<div style="position:absolute;left:108.00px;top:366.29px" class="cls_006"><span class="cls_006">ing and Natural Language Processing (NLP) can be applied to improve web accessibility.</span></div>
<div style="position:absolute;left:108.00px;top:382.72px" class="cls_006"><span class="cls_006">Primarily, we have considered two tasks - automatically captioning images and detecting</span></div>
<div style="position:absolute;left:108.00px;top:399.16px" class="cls_006"><span class="cls_006">if the hyperlink text is contextual or not. Various neural network architectures and NLP</span></div>
<div style="position:absolute;left:108.00px;top:415.59px" class="cls_006"><span class="cls_006">techniques have been applied to both these tasks. For the first task, a lot of work has</span></div>
<div style="position:absolute;left:108.00px;top:432.03px" class="cls_006"><span class="cls_006">also been done on applying word embedding techniques to improve evaluation systems</span></div>
<div style="position:absolute;left:108.00px;top:448.46px" class="cls_006"><span class="cls_006">of image captioning models. Also, object detection and image-text similarity have been</span></div>
<div style="position:absolute;left:108.00px;top:464.90px" class="cls_006"><span class="cls_006">considered as possible extensions to the current task.  For the second task, the major</span></div>
<div style="position:absolute;left:108.00px;top:481.33px" class="cls_006"><span class="cls_006">contribution of this project is extending an existing dataset to suit the aim. Also, anal-</span></div>
<div style="position:absolute;left:108.00px;top:497.77px" class="cls_006"><span class="cls_006">ysis has been done on how existing pre-trained embeddings compare with embeddings</span></div>
<div style="position:absolute;left:108.00px;top:514.20px" class="cls_006"><span class="cls_006">trained using the generated dataset.</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:1704px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background04.jpg" width=595 height=842></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:1704px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background05.jpg" width=595 height=842></div>
<div style="position:absolute;left:249.01px;top:238.45px" class="cls_005"><span class="cls_005">Acknowledgements</span></div>
<div style="position:absolute;left:108.00px;top:279.67px" class="cls_006"><span class="cls_006">I would like to express my sincere gratitude to my supervisor prof Mike Wald for moti-</span></div>
<div style="position:absolute;left:108.00px;top:296.10px" class="cls_006"><span class="cls_006">vating and guiding me through the project. I would like to thank the professors of this</span></div>
<div style="position:absolute;left:108.00px;top:312.54px" class="cls_006"><span class="cls_006">college for the modules that they taught on the relevant subjects which provided me</span></div>
<div style="position:absolute;left:108.00px;top:328.98px" class="cls_006"><span class="cls_006">with the foundation knowledge required to complete a project in this field. I would also</span></div>
<div style="position:absolute;left:108.00px;top:345.41px" class="cls_006"><span class="cls_006">like to thank prof Tim Norman for his feedback during the demonstration.</span></div>
<div style="position:absolute;left:108.00px;top:371.24px" class="cls_006"><span class="cls_006">This project would not have been possible without the open-source tools, and the excel-</span></div>
<div style="position:absolute;left:108.00px;top:387.67px" class="cls_006"><span class="cls_006">lent quality of reading materials that are available in the fields of AI. I would especially</span></div>
<div style="position:absolute;left:108.00px;top:404.11px" class="cls_006"><span class="cls_006">like to thank Jason Brownlee’s blog Machine Learning Mastery and Andy Thomas’s blog</span></div>
<div style="position:absolute;left:108.00px;top:420.54px" class="cls_006"><span class="cls_006">Adventures in Machine Learning for the excellent tutorials and posts. I have cited these</span></div>
<div style="position:absolute;left:108.00px;top:436.98px" class="cls_006"><span class="cls_006">tools and tutorials in my work wherever applicable.</span></div>
<div style="position:absolute;left:108.00px;top:462.81px" class="cls_006"><span class="cls_006">I also would like to thank my parents, friends, and classmates who have supported and</span></div>
<div style="position:absolute;left:108.00px;top:479.24px" class="cls_006"><span class="cls_006">motivated me during this phase. Especially I would like to thank my partner, Manisha</span></div>
<div style="position:absolute;left:108.00px;top:495.68px" class="cls_006"><span class="cls_006">for her constant support and advice.</span></div>
<div style="position:absolute;left:313.08px;top:773.49px" class="cls_006"><span class="cls_006">v</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:2556px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background06.jpg" width=595 height=842></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:2556px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background07.jpg" width=595 height=842></div>
<div style="position:absolute;left:70.95px;top:84.46px" class="cls_047"><span class="cls_047">Statement of Originality</span></div>
<div style="position:absolute;left:70.95px;top:104.30px" class="cls_009"><span class="cls_009">-</span><span class="cls_010"> </span><span class="cls_009">   I have read and understood the </span><span class="cls_048">ECS Academic Integrity</span><span class="cls_009"> information and the University’s</span></div>
<div style="position:absolute;left:88.97px;top:116.08px" class="cls_048"><span class="cls_048">Academic Integrity Guidance for Students</span><span class="cls_009">.</span></div>
<div style="position:absolute;left:70.95px;top:127.97px" class="cls_009"><span class="cls_009">-</span><span class="cls_010"> </span><span class="cls_009">   I am aware that failure to act in accordance with the </span><span class="cls_048">Regulations Governing Academic Integrity</span></div>
<div style="position:absolute;left:88.97px;top:139.74px" class="cls_009"><span class="cls_009">may lead to the imposition of penalties which, for the most serious cases, may include</span></div>
<div style="position:absolute;left:88.97px;top:151.51px" class="cls_009"><span class="cls_009">termination of programme.</span></div>
<div style="position:absolute;left:70.95px;top:163.41px" class="cls_009"><span class="cls_009">-    I consent to the University copying and distributing any or all of my work in any form and</span></div>
<div style="position:absolute;left:88.97px;top:175.18px" class="cls_009"><span class="cls_009">using third parties (who may be based outside the EU/EEA) to verify whether my work</span></div>
<div style="position:absolute;left:88.97px;top:187.07px" class="cls_009"><span class="cls_009">contains plagiarised material, and for quality assurance purposes.</span></div>
<div style="position:absolute;left:70.95px;top:204.85px" class="cls_008"><span class="cls_008">You must </span><span class="cls_049">change the statements in the boxes</span><span class="cls_008"> if you do not agree with them.</span></div>
<div style="position:absolute;left:70.95px;top:226.71px" class="cls_009"><span class="cls_009">We expect you to acknowledge all sources of information (e.g. ideas, algorithms, data) using</span></div>
<div style="position:absolute;left:70.95px;top:238.48px" class="cls_009"><span class="cls_009">citations. You must also put quotation marks around any sections of text that you have copied</span></div>
<div style="position:absolute;left:70.95px;top:250.26px" class="cls_009"><span class="cls_009">without paraphrasing. If any figures or tables have been taken or modified from another source,</span></div>
<div style="position:absolute;left:70.95px;top:262.15px" class="cls_009"><span class="cls_009">you must explain this in the caption </span><span class="cls_050">and</span><span class="cls_009"> cite the original source.</span></div>
<div style="position:absolute;left:69.27px;top:282.48px" class="cls_007"><span class="cls_007">I have acknowledged all sources, and identified any content taken from elsewhere.</span></div>
<div style="position:absolute;left:70.95px;top:305.78px" class="cls_009"><span class="cls_009">If you have used any code (e.g. open-source code), reference designs, or similar resources that</span></div>
<div style="position:absolute;left:70.95px;top:317.56px" class="cls_009"><span class="cls_009">have been produced by anyone else, you must list them in the box below. In the report, you must</span></div>
<div style="position:absolute;left:70.95px;top:329.33px" class="cls_009"><span class="cls_009">explain what was used and how it relates to the work you have done.</span></div>
<div style="position:absolute;left:69.27px;top:349.75px" class="cls_007"><span class="cls_007">I have not used any resources produced by anyone else.</span></div>
<div style="position:absolute;left:70.95px;top:372.94px" class="cls_009"><span class="cls_009">You can consult with module teaching staff/demonstrators, but you should not show anyone else</span></div>
<div style="position:absolute;left:70.95px;top:384.83px" class="cls_009"><span class="cls_009">your work (this includes uploading your work to publicly-accessible repositories e.g. Github, unless</span></div>
<div style="position:absolute;left:70.95px;top:396.60px" class="cls_009"><span class="cls_009">expressly permitted by the module leader), or help them to do theirs. For individual assignments,</span></div>
<div style="position:absolute;left:70.95px;top:408.37px" class="cls_009"><span class="cls_009">we expect you to work on your own. For group assignments, we expect that you work only with</span></div>
<div style="position:absolute;left:70.95px;top:420.26px" class="cls_009"><span class="cls_009">your allocated group. You must get permission in writing from the module teaching staff before</span></div>
<div style="position:absolute;left:70.95px;top:432.04px" class="cls_009"><span class="cls_009">you seek outside assistance, e.g. a proofreading service, and declare it here.</span></div>
<div style="position:absolute;left:69.27px;top:452.34px" class="cls_007"><span class="cls_007">I did all the work myself, or with my allocated group, and have not helped anyone else.</span></div>
<div style="position:absolute;left:70.95px;top:475.66px" class="cls_009"><span class="cls_009">We expect that you have not fabricated, modified or distorted any data, evidence, references,</span></div>
<div style="position:absolute;left:70.95px;top:487.44px" class="cls_009"><span class="cls_009">experimental results, or other material used or presented in the report. You must clearly describe</span></div>
<div style="position:absolute;left:70.95px;top:499.33px" class="cls_009"><span class="cls_009">your experiments and how the results were obtained, and include all data, source code and/or</span></div>
<div style="position:absolute;left:70.95px;top:511.10px" class="cls_009"><span class="cls_009">designs (either in the report, or submitted as a separate file) so that your results could be</span></div>
<div style="position:absolute;left:70.95px;top:522.87px" class="cls_009"><span class="cls_009">reproduced.</span></div>
<div style="position:absolute;left:69.27px;top:543.29px" class="cls_007"><span class="cls_007">The material in the report is genuine, and I have included all my data/code/designs.</span></div>
<div style="position:absolute;left:70.95px;top:566.48px" class="cls_009"><span class="cls_009">We expect that you have not previously submitted any part of this work for another assessment.</span></div>
<div style="position:absolute;left:70.95px;top:578.37px" class="cls_009"><span class="cls_009">You must get permission in writing from the module teaching staff before re-using any of your</span></div>
<div style="position:absolute;left:70.95px;top:590.14px" class="cls_009"><span class="cls_009">previously submitted work for this assessment.</span></div>
<div style="position:absolute;left:69.27px;top:610.44px" class="cls_007"><span class="cls_007">I have not submitted any part of this work for another assessment.</span></div>
<div style="position:absolute;left:70.95px;top:633.75px" class="cls_009"><span class="cls_009">If your work involved research/studies (including surveys) on human participants, their cells or</span></div>
<div style="position:absolute;left:70.95px;top:645.52px" class="cls_009"><span class="cls_009">data, or on animals, you must have been granted ethical approval before the work was carried</span></div>
<div style="position:absolute;left:70.95px;top:657.41px" class="cls_009"><span class="cls_009">out, and any experiments must have followed these requirements</span><span class="cls_013">.</span><span class="cls_009"> You must give details of this in</span></div>
<div style="position:absolute;left:70.95px;top:669.22px" class="cls_009"><span class="cls_009">the report, and list the ethical approval reference number(s) in the box below.</span></div>
<div style="position:absolute;left:69.27px;top:689.52px" class="cls_007"><span class="cls_007">My work did not involve human participants, their cells or data, or animals.</span></div>
<div style="position:absolute;left:70.95px;top:713.78px" class="cls_015"><span class="cls_015">ECS Statement of Originality Template, updated August 2018, Alex Weddell </span><span class="cls_051">aiofficer@ecs.soton.ac.uk</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:3408px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background08.jpg" width=595 height=842></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:3408px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background09.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:147.25px" class="cls_017"><span class="cls_017">Contents</span></div>
<div style="position:absolute;left:108.00px;top:226.44px" class="cls_006"><span class="cls_006">Acknowledgements</span></div>
<div style="position:absolute;left:517.29px;top:226.44px" class="cls_006"><span class="cls_006">v</span></div>
<div style="position:absolute;left:108.00px;top:251.89px" class="cls_006"><span class="cls_006">1</span></div>
<div style="position:absolute;left:124.36px;top:251.89px" class="cls_006"><span class="cls_006">Introduction and motivation</span></div>
<div style="position:absolute;left:517.64px;top:251.89px" class="cls_006"><span class="cls_006">1</span></div>
<div style="position:absolute;left:124.36px;top:266.44px" class="cls_006"><span class="cls_006">1.1</span></div>
<div style="position:absolute;left:149.45px;top:266.44px" class="cls_006"><span class="cls_006">Project aim</span></div>
<div style="position:absolute;left:518.46px;top:266.44px" class="cls_006"><span class="cls_006">2</span></div>
<div style="position:absolute;left:124.36px;top:280.98px" class="cls_006"><span class="cls_006">1.2</span></div>
<div style="position:absolute;left:149.45px;top:280.98px" class="cls_006"><span class="cls_006">Objectives and methodology</span></div>
<div style="position:absolute;left:518.46px;top:280.98px" class="cls_006"><span class="cls_006">2</span></div>
<div style="position:absolute;left:124.36px;top:295.53px" class="cls_006"><span class="cls_006">1.3</span></div>
<div style="position:absolute;left:149.45px;top:295.53px" class="cls_006"><span class="cls_006">Risk Analysis</span></div>
<div style="position:absolute;left:518.45px;top:295.53px" class="cls_006"><span class="cls_006">4</span></div>
<div style="position:absolute;left:108.00px;top:320.98px" class="cls_006"><span class="cls_006">2</span></div>
<div style="position:absolute;left:124.36px;top:320.98px" class="cls_006"><span class="cls_006">Preliminaries and research</span></div>
<div style="position:absolute;left:517.64px;top:320.98px" class="cls_006"><span class="cls_006">5</span></div>
<div style="position:absolute;left:124.36px;top:335.53px" class="cls_006"><span class="cls_006">2.1</span></div>
<div style="position:absolute;left:149.45px;top:335.53px" class="cls_006"><span class="cls_006">The role of Convolutional Neural Networks (CNNs) .</span></div>
<div style="position:absolute;left:518.46px;top:335.53px" class="cls_006"><span class="cls_006">5</span></div>
<div style="position:absolute;left:149.46px;top:350.07px" class="cls_006"><span class="cls_006">2.1.1</span></div>
<div style="position:absolute;left:184.36px;top:350.07px" class="cls_006"><span class="cls_006">The working of a basic CNN</span></div>
<div style="position:absolute;left:518.46px;top:350.07px" class="cls_006"><span class="cls_006">5</span></div>
<div style="position:absolute;left:149.46px;top:364.62px" class="cls_006"><span class="cls_006">2.1.2</span></div>
<div style="position:absolute;left:184.36px;top:364.62px" class="cls_006"><span class="cls_006">Transfer Learning</span></div>
<div style="position:absolute;left:518.45px;top:364.62px" class="cls_006"><span class="cls_006">6</span></div>
<div style="position:absolute;left:149.46px;top:379.16px" class="cls_006"><span class="cls_006">2.1.3</span></div>
<div style="position:absolute;left:184.36px;top:379.16px" class="cls_006"><span class="cls_006">Transfer Learning in Word Embeddings  . .</span></div>
<div style="position:absolute;left:518.46px;top:379.16px" class="cls_006"><span class="cls_006">7</span></div>
<div style="position:absolute;left:124.36px;top:393.71px" class="cls_006"><span class="cls_006">2.2</span></div>
<div style="position:absolute;left:149.45px;top:393.71px" class="cls_006"><span class="cls_006">The role of Recurrent Neural Networks (RNN)  . .</span></div>
<div style="position:absolute;left:518.46px;top:393.71px" class="cls_006"><span class="cls_006">7</span></div>
<div style="position:absolute;left:108.00px;top:419.16px" class="cls_006"><span class="cls_006">3</span></div>
<div style="position:absolute;left:124.36px;top:419.16px" class="cls_006"><span class="cls_006">Automatic Image Captioning System</span></div>
<div style="position:absolute;left:511.37px;top:419.16px" class="cls_006"><span class="cls_006">11</span></div>
<div style="position:absolute;left:124.36px;top:433.71px" class="cls_006"><span class="cls_006">3.1</span></div>
<div style="position:absolute;left:149.45px;top:433.71px" class="cls_006"><span class="cls_006">The Problem</span></div>
<div style="position:absolute;left:513.01px;top:433.71px" class="cls_006"><span class="cls_006">11</span></div>
<div style="position:absolute;left:124.36px;top:448.26px" class="cls_006"><span class="cls_006">3.2</span></div>
<div style="position:absolute;left:149.45px;top:448.26px" class="cls_006"><span class="cls_006">The Dataset</span></div>
<div style="position:absolute;left:513.01px;top:448.26px" class="cls_006"><span class="cls_006">11</span></div>
<div style="position:absolute;left:124.36px;top:462.80px" class="cls_006"><span class="cls_006">3.3</span></div>
<div style="position:absolute;left:149.45px;top:462.80px" class="cls_006"><span class="cls_006">Data Cleaning and pre-processing</span></div>
<div style="position:absolute;left:513.00px;top:462.80px" class="cls_006"><span class="cls_006">12</span></div>
<div style="position:absolute;left:124.36px;top:477.35px" class="cls_006"><span class="cls_006">3.4</span></div>
<div style="position:absolute;left:149.45px;top:477.35px" class="cls_006"><span class="cls_006">Model for Image Classification</span></div>
<div style="position:absolute;left:513.01px;top:477.35px" class="cls_006"><span class="cls_006">13</span></div>
<div style="position:absolute;left:149.46px;top:491.89px" class="cls_006"><span class="cls_006">3.4.1</span></div>
<div style="position:absolute;left:184.36px;top:491.89px" class="cls_006"><span class="cls_006">Optimization of the VGG-16 model</span></div>
<div style="position:absolute;left:513.01px;top:491.89px" class="cls_006"><span class="cls_006">14</span></div>
<div style="position:absolute;left:124.36px;top:506.44px" class="cls_006"><span class="cls_006">3.5</span></div>
<div style="position:absolute;left:149.45px;top:506.44px" class="cls_006"><span class="cls_006">Creating the training set</span></div>
<div style="position:absolute;left:513.01px;top:506.44px" class="cls_006"><span class="cls_006">16</span></div>
<div style="position:absolute;left:124.36px;top:520.98px" class="cls_006"><span class="cls_006">3.6</span></div>
<div style="position:absolute;left:149.45px;top:520.98px" class="cls_006"><span class="cls_006">Model for Image Captioning</span></div>
<div style="position:absolute;left:513.01px;top:520.98px" class="cls_006"><span class="cls_006">17</span></div>
<div style="position:absolute;left:124.36px;top:535.53px" class="cls_006"><span class="cls_006">3.7</span></div>
<div style="position:absolute;left:149.45px;top:535.53px" class="cls_006"><span class="cls_006">Generating the captions</span></div>
<div style="position:absolute;left:513.00px;top:535.53px" class="cls_006"><span class="cls_006">20</span></div>
<div style="position:absolute;left:124.36px;top:550.07px" class="cls_006"><span class="cls_006">3.8</span></div>
<div style="position:absolute;left:149.45px;top:550.07px" class="cls_006"><span class="cls_006">Evaluating the model</span></div>
<div style="position:absolute;left:513.01px;top:550.07px" class="cls_006"><span class="cls_006">21</span></div>
<div style="position:absolute;left:149.46px;top:564.62px" class="cls_006"><span class="cls_006">3.8.1</span></div>
<div style="position:absolute;left:184.36px;top:564.62px" class="cls_006"><span class="cls_006">Sample results</span></div>
<div style="position:absolute;left:513.01px;top:564.62px" class="cls_006"><span class="cls_006">21</span></div>
<div style="position:absolute;left:149.46px;top:579.17px" class="cls_006"><span class="cls_006">3.8.2</span></div>
<div style="position:absolute;left:184.36px;top:579.17px" class="cls_006"><span class="cls_006">BLEU metrics - The problem</span></div>
<div style="position:absolute;left:513.00px;top:579.17px" class="cls_006"><span class="cls_006">22</span></div>
<div style="position:absolute;left:149.46px;top:593.71px" class="cls_006"><span class="cls_006">3.8.3</span></div>
<div style="position:absolute;left:184.36px;top:593.71px" class="cls_006"><span class="cls_006">A Proposed Solution</span></div>
<div style="position:absolute;left:513.00px;top:593.71px" class="cls_006"><span class="cls_006">23</span></div>
<div style="position:absolute;left:149.46px;top:608.26px" class="cls_006"><span class="cls_006">3.8.4</span></div>
<div style="position:absolute;left:184.36px;top:608.26px" class="cls_006"><span class="cls_006">Working of a word embedding model - word2vec</span></div>
<div style="position:absolute;left:513.00px;top:608.26px" class="cls_006"><span class="cls_006">23</span></div>
<div style="position:absolute;left:149.46px;top:622.80px" class="cls_006"><span class="cls_006">3.8.5</span></div>
<div style="position:absolute;left:184.36px;top:622.80px" class="cls_006"><span class="cls_006">Word Movers Distance</span></div>
<div style="position:absolute;left:513.00px;top:622.80px" class="cls_006"><span class="cls_006">24</span></div>
<div style="position:absolute;left:149.46px;top:637.35px" class="cls_006"><span class="cls_006">3.8.6</span></div>
<div style="position:absolute;left:184.36px;top:637.35px" class="cls_006"><span class="cls_006">Implementing the proposed solution</span></div>
<div style="position:absolute;left:513.00px;top:637.35px" class="cls_006"><span class="cls_006">24</span></div>
<div style="position:absolute;left:149.46px;top:651.89px" class="cls_006"><span class="cls_006">3.8.7</span></div>
<div style="position:absolute;left:184.36px;top:651.89px" class="cls_006"><span class="cls_006">Results</span></div>
<div style="position:absolute;left:513.01px;top:651.89px" class="cls_006"><span class="cls_006">25</span></div>
<div style="position:absolute;left:124.36px;top:666.44px" class="cls_006"><span class="cls_006">3.9</span></div>
<div style="position:absolute;left:149.45px;top:666.44px" class="cls_006"><span class="cls_006">Some optimizations for deploying</span></div>
<div style="position:absolute;left:513.00px;top:666.44px" class="cls_006"><span class="cls_006">26</span></div>
<div style="position:absolute;left:149.46px;top:680.98px" class="cls_006"><span class="cls_006">3.9.1</span></div>
<div style="position:absolute;left:184.36px;top:680.98px" class="cls_006"><span class="cls_006">Optimizing word embeddings</span></div>
<div style="position:absolute;left:513.01px;top:680.98px" class="cls_006"><span class="cls_006">26</span></div>
<div style="position:absolute;left:149.46px;top:695.53px" class="cls_006"><span class="cls_006">3.9.2</span></div>
<div style="position:absolute;left:184.36px;top:695.53px" class="cls_006"><span class="cls_006">Using model checkpoints</span></div>
<div style="position:absolute;left:513.01px;top:695.53px" class="cls_006"><span class="cls_006">27</span></div>
<div style="position:absolute;left:149.46px;top:710.07px" class="cls_006"><span class="cls_006">3.9.3</span></div>
<div style="position:absolute;left:184.36px;top:710.07px" class="cls_006"><span class="cls_006">File management</span></div>
<div style="position:absolute;left:513.00px;top:710.07px" class="cls_006"><span class="cls_006">27</span></div>
<div style="position:absolute;left:124.36px;top:724.62px" class="cls_006"><span class="cls_006">3.10</span></div>
<div style="position:absolute;left:149.45px;top:724.62px" class="cls_006"><span class="cls_006">Extensions</span></div>
<div style="position:absolute;left:513.00px;top:724.62px" class="cls_006"><span class="cls_006">27</span></div>
<div style="position:absolute;left:149.46px;top:739.17px" class="cls_006"><span class="cls_006">3.10.1</span></div>
<div style="position:absolute;left:184.36px;top:739.17px" class="cls_006"><span class="cls_006">Object detection</span></div>
<div style="position:absolute;left:513.01px;top:739.17px" class="cls_006"><span class="cls_006">27</span></div>
<div style="position:absolute;left:311.56px;top:773.49px" class="cls_006"><span class="cls_006">ix</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:4260px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background10.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">x</span></div>
<div style="position:absolute;left:425.94px;top:47.41px" class="cls_006"><span class="cls_006">CONTENTS</span></div>
<div style="position:absolute;left:113.46px;top:80.43px" class="cls_006"><span class="cls_006">3.10.2</span></div>
<div style="position:absolute;left:148.36px;top:80.43px" class="cls_006"><span class="cls_006">Relevance of image on a web page</span></div>
<div style="position:absolute;left:477.01px;top:80.43px" class="cls_006"><span class="cls_006">30</span></div>
<div style="position:absolute;left:72.00px;top:105.89px" class="cls_006"><span class="cls_006">4</span></div>
<div style="position:absolute;left:88.36px;top:105.89px" class="cls_006"><span class="cls_006">Contextual link text detection</span></div>
<div style="position:absolute;left:475.37px;top:105.89px" class="cls_006"><span class="cls_006">33</span></div>
<div style="position:absolute;left:88.36px;top:120.43px" class="cls_006"><span class="cls_006">4.1</span></div>
<div style="position:absolute;left:113.45px;top:120.43px" class="cls_006"><span class="cls_006">The problem</span></div>
<div style="position:absolute;left:477.01px;top:120.43px" class="cls_006"><span class="cls_006">33</span></div>
<div style="position:absolute;left:88.36px;top:134.98px" class="cls_006"><span class="cls_006">4.2</span></div>
<div style="position:absolute;left:113.45px;top:134.98px" class="cls_006"><span class="cls_006">Proposed solution</span></div>
<div style="position:absolute;left:477.01px;top:134.98px" class="cls_006"><span class="cls_006">33</span></div>
<div style="position:absolute;left:88.36px;top:149.53px" class="cls_006"><span class="cls_006">4.3</span></div>
<div style="position:absolute;left:113.45px;top:149.53px" class="cls_006"><span class="cls_006">Building the dataset</span></div>
<div style="position:absolute;left:477.01px;top:149.53px" class="cls_006"><span class="cls_006">35</span></div>
<div style="position:absolute;left:113.46px;top:164.07px" class="cls_006"><span class="cls_006">4.3.1</span></div>
<div style="position:absolute;left:148.36px;top:164.07px" class="cls_006"><span class="cls_006">Initial dataset</span></div>
<div style="position:absolute;left:477.01px;top:164.07px" class="cls_006"><span class="cls_006">35</span></div>
<div style="position:absolute;left:113.46px;top:178.62px" class="cls_006"><span class="cls_006">4.3.2</span></div>
<div style="position:absolute;left:148.36px;top:178.62px" class="cls_006"><span class="cls_006">Extending the dataset</span></div>
<div style="position:absolute;left:477.00px;top:178.62px" class="cls_006"><span class="cls_006">35</span></div>
<div style="position:absolute;left:148.36px;top:193.16px" class="cls_006"><span class="cls_006">4.3.2.1</span></div>
<div style="position:absolute;left:193.10px;top:193.16px" class="cls_006"><span class="cls_006">Read the file for valid URLs</span></div>
<div style="position:absolute;left:477.01px;top:193.16px" class="cls_006"><span class="cls_006">36</span></div>
<div style="position:absolute;left:148.36px;top:207.71px" class="cls_006"><span class="cls_006">4.3.2.2</span></div>
<div style="position:absolute;left:193.10px;top:207.71px" class="cls_006"><span class="cls_006">Create a list of all link texts, source URLs and target</span></div>
<div style="position:absolute;left:193.09px;top:221.26px" class="cls_006"><span class="cls_006">URLs</span></div>
<div style="position:absolute;left:477.01px;top:221.26px" class="cls_006"><span class="cls_006">36</span></div>
<div style="position:absolute;left:148.36px;top:235.80px" class="cls_006"><span class="cls_006">4.3.2.3</span></div>
<div style="position:absolute;left:193.10px;top:235.80px" class="cls_006"><span class="cls_006">Scrape the data of the source URL and target URL using</span></div>
<div style="position:absolute;left:193.09px;top:249.35px" class="cls_006"><span class="cls_006">the sliding window</span></div>
<div style="position:absolute;left:477.01px;top:249.35px" class="cls_006"><span class="cls_006">37</span></div>
<div style="position:absolute;left:148.36px;top:263.90px" class="cls_006"><span class="cls_006">4.3.2.4</span></div>
<div style="position:absolute;left:193.10px;top:263.90px" class="cls_006"><span class="cls_006">Store the data in the disk</span></div>
<div style="position:absolute;left:477.01px;top:263.90px" class="cls_006"><span class="cls_006">38</span></div>
<div style="position:absolute;left:88.36px;top:278.44px" class="cls_006"><span class="cls_006">4.4</span></div>
<div style="position:absolute;left:113.45px;top:278.44px" class="cls_006"><span class="cls_006">Building the model</span></div>
<div style="position:absolute;left:477.00px;top:278.44px" class="cls_006"><span class="cls_006">39</span></div>
<div style="position:absolute;left:113.46px;top:292.99px" class="cls_006"><span class="cls_006">4.4.1</span></div>
<div style="position:absolute;left:148.36px;top:292.99px" class="cls_006"><span class="cls_006">Negative sampling</span></div>
<div style="position:absolute;left:477.00px;top:292.99px" class="cls_006"><span class="cls_006">39</span></div>
<div style="position:absolute;left:113.46px;top:307.53px" class="cls_006"><span class="cls_006">4.4.2</span></div>
<div style="position:absolute;left:148.36px;top:307.53px" class="cls_006"><span class="cls_006">Creating the training dataset</span></div>
<div style="position:absolute;left:477.01px;top:307.53px" class="cls_006"><span class="cls_006">39</span></div>
<div style="position:absolute;left:113.46px;top:322.08px" class="cls_006"><span class="cls_006">4.4.3</span></div>
<div style="position:absolute;left:148.36px;top:322.08px" class="cls_006"><span class="cls_006">The model architecture</span></div>
<div style="position:absolute;left:477.00px;top:322.08px" class="cls_006"><span class="cls_006">41</span></div>
<div style="position:absolute;left:113.46px;top:336.62px" class="cls_006"><span class="cls_006">4.4.4</span></div>
<div style="position:absolute;left:148.36px;top:336.62px" class="cls_006"><span class="cls_006">Learned embeddings results</span></div>
<div style="position:absolute;left:477.01px;top:336.62px" class="cls_006"><span class="cls_006">42</span></div>
<div style="position:absolute;left:88.36px;top:351.17px" class="cls_006"><span class="cls_006">4.5</span></div>
<div style="position:absolute;left:113.45px;top:351.17px" class="cls_006"><span class="cls_006">Evaluating the model</span></div>
<div style="position:absolute;left:477.01px;top:351.17px" class="cls_006"><span class="cls_006">42</span></div>
<div style="position:absolute;left:113.46px;top:365.72px" class="cls_006"><span class="cls_006">4.5.1</span></div>
<div style="position:absolute;left:148.36px;top:365.72px" class="cls_006"><span class="cls_006">Evaluating similarity between the source text and target text  . .</span></div>
<div style="position:absolute;left:477.01px;top:365.72px" class="cls_006"><span class="cls_006">42</span></div>
<div style="position:absolute;left:113.46px;top:380.26px" class="cls_006"><span class="cls_006">4.5.2</span></div>
<div style="position:absolute;left:148.36px;top:380.26px" class="cls_006"><span class="cls_006">Evaluating similarity between the source link text and target tex-</span></div>
<div style="position:absolute;left:148.36px;top:393.81px" class="cls_006"><span class="cls_006">t/source text</span></div>
<div style="position:absolute;left:477.01px;top:393.81px" class="cls_006"><span class="cls_006">44</span></div>
<div style="position:absolute;left:72.00px;top:419.26px" class="cls_006"><span class="cls_006">5</span></div>
<div style="position:absolute;left:88.36px;top:419.26px" class="cls_006"><span class="cls_006">Conclusion</span></div>
<div style="position:absolute;left:475.37px;top:419.26px" class="cls_006"><span class="cls_006">45</span></div>
<div style="position:absolute;left:88.36px;top:433.81px" class="cls_006"><span class="cls_006">5.1</span></div>
<div style="position:absolute;left:113.45px;top:433.81px" class="cls_006"><span class="cls_006">Reflection</span></div>
<div style="position:absolute;left:477.01px;top:433.81px" class="cls_006"><span class="cls_006">45</span></div>
<div style="position:absolute;left:88.36px;top:448.36px" class="cls_006"><span class="cls_006">5.2</span></div>
<div style="position:absolute;left:113.45px;top:448.36px" class="cls_006"><span class="cls_006">Further Research</span></div>
<div style="position:absolute;left:477.00px;top:448.36px" class="cls_006"><span class="cls_006">46</span></div>
<div style="position:absolute;left:72.00px;top:473.81px" class="cls_006"><span class="cls_006">Bibliography</span></div>
<div style="position:absolute;left:475.37px;top:473.81px" class="cls_006"><span class="cls_006">47</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:5112px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background11.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:147.25px" class="cls_017"><span class="cls_017">List of Figures</span></div>
<div style="position:absolute;left:124.36px;top:215.53px" class="cls_006"><span class="cls_006">1.1</span></div>
<div style="position:absolute;left:149.45px;top:215.53px" class="cls_006"><span class="cls_006">Performance of Deep Learning models with data (55)</span></div>
<div style="position:absolute;left:518.46px;top:215.53px" class="cls_006"><span class="cls_006">1</span></div>
<div style="position:absolute;left:124.36px;top:230.07px" class="cls_006"><span class="cls_006">1.2</span></div>
<div style="position:absolute;left:149.45px;top:230.07px" class="cls_006"><span class="cls_006">Risk Analysis</span></div>
<div style="position:absolute;left:518.45px;top:230.07px" class="cls_006"><span class="cls_006">4</span></div>
<div style="position:absolute;left:124.36px;top:254.58px" class="cls_006"><span class="cls_006">2.1</span></div>
<div style="position:absolute;left:149.45px;top:254.58px" class="cls_006"><span class="cls_006">Basic CNN architecture</span></div>
<div style="position:absolute;left:518.46px;top:254.58px" class="cls_006"><span class="cls_006">6</span></div>
<div style="position:absolute;left:124.36px;top:269.13px" class="cls_006"><span class="cls_006">2.2</span></div>
<div style="position:absolute;left:149.45px;top:269.13px" class="cls_006"><span class="cls_006">Proposed solution - overview</span></div>
<div style="position:absolute;left:518.46px;top:269.13px" class="cls_006"><span class="cls_006">8</span></div>
<div style="position:absolute;left:124.36px;top:283.67px" class="cls_006"><span class="cls_006">2.3</span></div>
<div style="position:absolute;left:149.45px;top:283.67px" class="cls_006"><span class="cls_006">Basic architecture of LSTM(41)</span></div>
<div style="position:absolute;left:518.46px;top:283.67px" class="cls_006"><span class="cls_006">8</span></div>
<div style="position:absolute;left:124.36px;top:308.18px" class="cls_006"><span class="cls_006">3.1</span></div>
<div style="position:absolute;left:149.45px;top:308.18px" class="cls_006"><span class="cls_006">Performance adavantage of transfer learning</span></div>
<div style="position:absolute;left:513.00px;top:308.18px" class="cls_006"><span class="cls_006">13</span></div>
<div style="position:absolute;left:124.36px;top:322.73px" class="cls_006"><span class="cls_006">3.2</span></div>
<div style="position:absolute;left:149.45px;top:322.73px" class="cls_006"><span class="cls_006">VGG 16 architecture; last layer removed</span></div>
<div style="position:absolute;left:513.00px;top:322.73px" class="cls_006"><span class="cls_006">15</span></div>
<div style="position:absolute;left:124.36px;top:337.27px" class="cls_006"><span class="cls_006">3.3</span></div>
<div style="position:absolute;left:149.45px;top:337.27px" class="cls_006"><span class="cls_006">Process to create training data</span></div>
<div style="position:absolute;left:513.01px;top:337.27px" class="cls_006"><span class="cls_006">17</span></div>
<div style="position:absolute;left:124.36px;top:351.82px" class="cls_006"><span class="cls_006">3.4</span></div>
<div style="position:absolute;left:149.45px;top:351.82px" class="cls_006"><span class="cls_006">Modified architecture</span></div>
<div style="position:absolute;left:513.00px;top:351.82px" class="cls_006"><span class="cls_006">18</span></div>
<div style="position:absolute;left:124.36px;top:366.36px" class="cls_006"><span class="cls_006">3.5</span></div>
<div style="position:absolute;left:149.45px;top:366.36px" class="cls_006"><span class="cls_006">Final model</span></div>
<div style="position:absolute;left:513.00px;top:366.36px" class="cls_006"><span class="cls_006">20</span></div>
<div style="position:absolute;left:124.36px;top:380.91px" class="cls_006"><span class="cls_006">3.6</span></div>
<div style="position:absolute;left:149.45px;top:380.91px" class="cls_006"><span class="cls_006">Few captions generated by the model</span></div>
<div style="position:absolute;left:513.00px;top:380.91px" class="cls_006"><span class="cls_006">22</span></div>
<div style="position:absolute;left:124.36px;top:395.45px" class="cls_006"><span class="cls_006">3.7</span></div>
<div style="position:absolute;left:149.45px;top:395.45px" class="cls_006"><span class="cls_006">Working of a basic word embedding model (52)</span></div>
<div style="position:absolute;left:513.00px;top:395.45px" class="cls_006"><span class="cls_006">23</span></div>
<div style="position:absolute;left:124.36px;top:410.00px" class="cls_006"><span class="cls_006">3.8</span></div>
<div style="position:absolute;left:149.45px;top:410.00px" class="cls_006"><span class="cls_006">Optimizing word embeddings</span></div>
<div style="position:absolute;left:513.00px;top:410.00px" class="cls_006"><span class="cls_006">26</span></div>
<div style="position:absolute;left:124.36px;top:424.54px" class="cls_006"><span class="cls_006">3.9</span></div>
<div style="position:absolute;left:149.45px;top:424.54px" class="cls_006"><span class="cls_006">Object Detection App - User interaction</span></div>
<div style="position:absolute;left:513.00px;top:424.54px" class="cls_006"><span class="cls_006">30</span></div>
<div style="position:absolute;left:124.36px;top:439.09px" class="cls_006"><span class="cls_006">3.10</span></div>
<div style="position:absolute;left:149.45px;top:439.09px" class="cls_006"><span class="cls_006">Visualizing the algorithm for Relevance of image in a web page</span></div>
<div style="position:absolute;left:513.01px;top:439.09px" class="cls_006"><span class="cls_006">31</span></div>
<div style="position:absolute;left:124.36px;top:463.60px" class="cls_006"><span class="cls_006">4.1</span></div>
<div style="position:absolute;left:149.45px;top:463.60px" class="cls_006"><span class="cls_006">Links to Wikipedia as entity labels (47)</span></div>
<div style="position:absolute;left:513.01px;top:463.60px" class="cls_006"><span class="cls_006">34</span></div>
<div style="position:absolute;left:124.36px;top:478.14px" class="cls_006"><span class="cls_006">4.2</span></div>
<div style="position:absolute;left:149.45px;top:478.14px" class="cls_006"><span class="cls_006">Process to extend the dataset</span></div>
<div style="position:absolute;left:513.01px;top:478.14px" class="cls_006"><span class="cls_006">36</span></div>
<div style="position:absolute;left:124.36px;top:492.69px" class="cls_006"><span class="cls_006">4.3</span></div>
<div style="position:absolute;left:149.45px;top:492.69px" class="cls_006"><span class="cls_006">Words within and out of context example</span></div>
<div style="position:absolute;left:513.00px;top:492.69px" class="cls_006"><span class="cls_006">39</span></div>
<div style="position:absolute;left:124.36px;top:507.23px" class="cls_006"><span class="cls_006">4.4</span></div>
<div style="position:absolute;left:149.45px;top:507.23px" class="cls_006"><span class="cls_006">Words within and out of context example with tokens</span></div>
<div style="position:absolute;left:513.00px;top:507.23px" class="cls_006"><span class="cls_006">40</span></div>
<div style="position:absolute;left:124.36px;top:521.78px" class="cls_006"><span class="cls_006">4.5</span></div>
<div style="position:absolute;left:149.45px;top:521.78px" class="cls_006"><span class="cls_006">Negative Sampling Model Architecture</span></div>
<div style="position:absolute;left:513.00px;top:521.78px" class="cls_006"><span class="cls_006">41</span></div>
<div style="position:absolute;left:311.56px;top:773.49px" class="cls_006"><span class="cls_006">xi</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:5964px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background12.jpg" width=595 height=842></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:5964px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background13.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:147.25px" class="cls_017"><span class="cls_017">List of Tables</span></div>
<div style="position:absolute;left:124.36px;top:215.53px" class="cls_006"><span class="cls_006">3.1</span></div>
<div style="position:absolute;left:149.45px;top:215.53px" class="cls_006"><span class="cls_006">Dataset statistics</span></div>
<div style="position:absolute;left:513.01px;top:215.53px" class="cls_006"><span class="cls_006">17</span></div>
<div style="position:absolute;left:124.36px;top:230.07px" class="cls_006"><span class="cls_006">3.2</span></div>
<div style="position:absolute;left:149.45px;top:230.07px" class="cls_006"><span class="cls_006">Model parameters</span></div>
<div style="position:absolute;left:513.01px;top:230.07px" class="cls_006"><span class="cls_006">19</span></div>
<div style="position:absolute;left:124.36px;top:244.62px" class="cls_006"><span class="cls_006">3.3</span></div>
<div style="position:absolute;left:149.45px;top:244.62px" class="cls_006"><span class="cls_006">Model parameters</span></div>
<div style="position:absolute;left:513.01px;top:244.62px" class="cls_006"><span class="cls_006">22</span></div>
<div style="position:absolute;left:124.36px;top:259.16px" class="cls_006"><span class="cls_006">3.4</span></div>
<div style="position:absolute;left:149.45px;top:259.16px" class="cls_006"><span class="cls_006">WMD and BLEU scores for sample captions</span></div>
<div style="position:absolute;left:513.01px;top:259.16px" class="cls_006"><span class="cls_006">25</span></div>
<div style="position:absolute;left:124.36px;top:273.71px" class="cls_006"><span class="cls_006">3.5</span></div>
<div style="position:absolute;left:149.45px;top:273.71px" class="cls_006"><span class="cls_006">File management</span></div>
<div style="position:absolute;left:513.01px;top:273.71px" class="cls_006"><span class="cls_006">28</span></div>
<div style="position:absolute;left:124.36px;top:298.22px" class="cls_006"><span class="cls_006">4.1</span></div>
<div style="position:absolute;left:149.45px;top:298.22px" class="cls_006"><span class="cls_006">Statistics of the extended dataset</span></div>
<div style="position:absolute;left:513.01px;top:298.22px" class="cls_006"><span class="cls_006">38</span></div>
<div style="position:absolute;left:124.36px;top:312.76px" class="cls_006"><span class="cls_006">4.2</span></div>
<div style="position:absolute;left:149.45px;top:312.76px" class="cls_006"><span class="cls_006">Sample word pairs</span></div>
<div style="position:absolute;left:513.00px;top:312.76px" class="cls_006"><span class="cls_006">41</span></div>
<div style="position:absolute;left:124.36px;top:327.31px" class="cls_006"><span class="cls_006">4.3</span></div>
<div style="position:absolute;left:149.45px;top:327.31px" class="cls_006"><span class="cls_006">Some results of the learned embeddings</span></div>
<div style="position:absolute;left:513.00px;top:327.31px" class="cls_006"><span class="cls_006">42</span></div>
<div style="position:absolute;left:124.36px;top:341.85px" class="cls_006"><span class="cls_006">4.4</span></div>
<div style="position:absolute;left:149.45px;top:341.85px" class="cls_006"><span class="cls_006">Results for similarity between source text and target text</span></div>
<div style="position:absolute;left:513.00px;top:341.85px" class="cls_006"><span class="cls_006">43</span></div>
<div style="position:absolute;left:124.36px;top:356.40px" class="cls_006"><span class="cls_006">4.5</span></div>
<div style="position:absolute;left:149.45px;top:356.40px" class="cls_006"><span class="cls_006">Results for similarity between source link text and source text</span></div>
<div style="position:absolute;left:513.00px;top:356.40px" class="cls_006"><span class="cls_006">44</span></div>
<div style="position:absolute;left:308.53px;top:773.49px" class="cls_006"><span class="cls_006">xiii</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:6816px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background14.jpg" width=595 height=842></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:6816px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background15.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:147.25px" class="cls_017"><span class="cls_017">Listings</span></div>
<div style="position:absolute;left:124.36px;top:215.53px" class="cls_006"><span class="cls_006">3.1</span></div>
<div style="position:absolute;left:149.45px;top:215.53px" class="cls_006"><span class="cls_006">Optimization for photo features</span></div>
<div style="position:absolute;left:513.00px;top:215.53px" class="cls_006"><span class="cls_006">14</span></div>
<div style="position:absolute;left:124.36px;top:230.07px" class="cls_006"><span class="cls_006">3.2</span></div>
<div style="position:absolute;left:149.45px;top:230.07px" class="cls_006"><span class="cls_006">Data schema for each image id</span></div>
<div style="position:absolute;left:513.01px;top:230.07px" class="cls_006"><span class="cls_006">16</span></div>
<div style="position:absolute;left:124.36px;top:244.62px" class="cls_006"><span class="cls_006">3.3</span></div>
<div style="position:absolute;left:149.45px;top:244.62px" class="cls_006"><span class="cls_006">Alogorithm to create training dataset</span></div>
<div style="position:absolute;left:513.00px;top:244.62px" class="cls_006"><span class="cls_006">16</span></div>
<div style="position:absolute;left:124.36px;top:259.16px" class="cls_006"><span class="cls_006">3.4</span></div>
<div style="position:absolute;left:149.45px;top:259.16px" class="cls_006"><span class="cls_006">Generating the captions</span></div>
<div style="position:absolute;left:513.00px;top:259.16px" class="cls_006"><span class="cls_006">20</span></div>
<div style="position:absolute;left:124.36px;top:273.71px" class="cls_006"><span class="cls_006">3.5</span></div>
<div style="position:absolute;left:149.45px;top:273.71px" class="cls_006"><span class="cls_006">WMD implementation</span></div>
<div style="position:absolute;left:513.01px;top:273.71px" class="cls_006"><span class="cls_006">25</span></div>
<div style="position:absolute;left:124.36px;top:288.26px" class="cls_006"><span class="cls_006">3.6</span></div>
<div style="position:absolute;left:149.45px;top:288.26px" class="cls_006"><span class="cls_006">Model checkpoint implementation</span></div>
<div style="position:absolute;left:513.01px;top:288.26px" class="cls_006"><span class="cls_006">27</span></div>
<div style="position:absolute;left:124.36px;top:302.80px" class="cls_006"><span class="cls_006">3.7</span></div>
<div style="position:absolute;left:149.45px;top:302.80px" class="cls_006"><span class="cls_006">Sample response from Azure Vision API</span></div>
<div style="position:absolute;left:513.00px;top:302.80px" class="cls_006"><span class="cls_006">29</span></div>
<div style="position:absolute;left:124.36px;top:317.35px" class="cls_006"><span class="cls_006">3.8</span></div>
<div style="position:absolute;left:149.45px;top:317.35px" class="cls_006"><span class="cls_006">Algorithm for Relevance of image in a web page</span></div>
<div style="position:absolute;left:513.01px;top:317.35px" class="cls_006"><span class="cls_006">32</span></div>
<div style="position:absolute;left:124.36px;top:331.89px" class="cls_006"><span class="cls_006">4.1</span></div>
<div style="position:absolute;left:149.45px;top:331.89px" class="cls_006"><span class="cls_006">Data schema</span></div>
<div style="position:absolute;left:513.00px;top:331.89px" class="cls_006"><span class="cls_006">35</span></div>
<div style="position:absolute;left:124.36px;top:346.44px" class="cls_006"><span class="cls_006">4.2</span></div>
<div style="position:absolute;left:149.45px;top:346.44px" class="cls_006"><span class="cls_006">Red the file for valid URLs</span></div>
<div style="position:absolute;left:513.00px;top:346.44px" class="cls_006"><span class="cls_006">36</span></div>
<div style="position:absolute;left:124.36px;top:360.98px" class="cls_006"><span class="cls_006">4.3</span></div>
<div style="position:absolute;left:149.45px;top:360.98px" class="cls_006"><span class="cls_006">List of link texts, source links and target links</span></div>
<div style="position:absolute;left:513.00px;top:360.98px" class="cls_006"><span class="cls_006">36</span></div>
<div style="position:absolute;left:124.36px;top:375.53px" class="cls_006"><span class="cls_006">4.4</span></div>
<div style="position:absolute;left:149.45px;top:375.53px" class="cls_006"><span class="cls_006">Process to organize source and target data</span></div>
<div style="position:absolute;left:513.00px;top:375.53px" class="cls_006"><span class="cls_006">37</span></div>
<div style="position:absolute;left:124.36px;top:390.07px" class="cls_006"><span class="cls_006">4.5</span></div>
<div style="position:absolute;left:149.45px;top:390.07px" class="cls_006"><span class="cls_006">Algorithm to extend the dataset</span></div>
<div style="position:absolute;left:513.00px;top:390.07px" class="cls_006"><span class="cls_006">37</span></div>
<div style="position:absolute;left:124.36px;top:404.62px" class="cls_006"><span class="cls_006">4.6</span></div>
<div style="position:absolute;left:149.45px;top:404.62px" class="cls_006"><span class="cls_006">Final corpus data schema</span></div>
<div style="position:absolute;left:513.01px;top:404.62px" class="cls_006"><span class="cls_006">38</span></div>
<div style="position:absolute;left:124.36px;top:419.16px" class="cls_006"><span class="cls_006">4.7</span></div>
<div style="position:absolute;left:149.45px;top:419.16px" class="cls_006"><span class="cls_006">Final corpus data schema</span></div>
<div style="position:absolute;left:513.01px;top:419.16px" class="cls_006"><span class="cls_006">39</span></div>
<div style="position:absolute;left:124.36px;top:433.71px" class="cls_006"><span class="cls_006">4.8</span></div>
<div style="position:absolute;left:149.45px;top:433.71px" class="cls_006"><span class="cls_006">Final corpus data schema simplified</span></div>
<div style="position:absolute;left:513.00px;top:433.71px" class="cls_006"><span class="cls_006">39</span></div>
<div style="position:absolute;left:124.36px;top:448.26px" class="cls_006"><span class="cls_006">4.9</span></div>
<div style="position:absolute;left:149.45px;top:448.26px" class="cls_006"><span class="cls_006">Evaluating similarity between the source text and target text</span></div>
<div style="position:absolute;left:513.01px;top:448.26px" class="cls_006"><span class="cls_006">42</span></div>
<div style="position:absolute;left:310.20px;top:773.49px" class="cls_006"><span class="cls_006">xv</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:7668px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background16.jpg" width=595 height=842></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:7668px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background17.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:160.10px" class="cls_003"><span class="cls_003">Chapter 1</span></div>
<div style="position:absolute;left:108.00px;top:221.55px" class="cls_017"><span class="cls_017">Introduction and motivation</span></div>
<div style="position:absolute;left:108.00px;top:301.11px" class="cls_006"><span class="cls_006">The impact of Artificial Intelligence (AI) has already taken huge proportions and is af-</span></div>
<div style="position:absolute;left:108.00px;top:317.54px" class="cls_006"><span class="cls_006">fecting billions of human lives. AI holds enormous potential and can assist humans and</span></div>
<div style="position:absolute;left:108.00px;top:333.98px" class="cls_006"><span class="cls_006">complement traditional applications in a wide variety of tasks. This deviation from tra-</span></div>
<div style="position:absolute;left:108.00px;top:350.41px" class="cls_006"><span class="cls_006">ditional programming in which programmers specified complex rules and logic to enable</span></div>
<div style="position:absolute;left:108.00px;top:366.85px" class="cls_006"><span class="cls_006">a computer to solve a problem towards a more data-driven deep learning-based approach</span></div>
<div style="position:absolute;left:108.00px;top:383.28px" class="cls_006"><span class="cls_006">in which we train the computer with lots of examples and it figures out an estimate of the</span></div>
<div style="position:absolute;left:108.00px;top:399.72px" class="cls_006"><span class="cls_006">underlying function, much like how young children process information and learn from</span></div>
<div style="position:absolute;left:108.00px;top:416.15px" class="cls_006"><span class="cls_006">them has been possible due to the explosion of big data. As the availability of relevant</span></div>
<div style="position:absolute;left:108.00px;top:432.59px" class="cls_006"><span class="cls_006">data increases the performance of these models also increases as shown the figure 1.1.</span></div>
<div style="position:absolute;left:108.00px;top:458.42px" class="cls_006"><span class="cls_006">Also, AI has been used to assist humans in basic sensory functions like captioning speech</span></div>
<div style="position:absolute;left:108.00px;top:474.85px" class="cls_006"><span class="cls_006">from videos, natural language understanding enabled bots that can interact and converse</span></div>
<div style="position:absolute;left:108.00px;top:491.29px" class="cls_006"><span class="cls_006">with humans, and computer vision applications that can view and translate the world</span></div>
<div style="position:absolute;left:108.00px;top:507.72px" class="cls_006"><span class="cls_006">around us. In less than 25 years, computers went from manipulating 0 and 1 digits to</span></div>
<div style="position:absolute;left:108.00px;top:524.16px" class="cls_006"><span class="cls_006">utilizing neural network and deep learning technology to enable rapid progress in fields</span></div>
<div style="position:absolute;left:108.00px;top:540.59px" class="cls_006"><span class="cls_006">like computer vision and natural language understanding (29).</span></div>
<div style="position:absolute;left:169.72px;top:745.81px" class="cls_018"><span class="cls_018">Figure 1.1: Performance of Deep Learning models with data (55)</span></div>
<div style="position:absolute;left:313.23px;top:773.49px" class="cls_006"><span class="cls_006">1</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:8520px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background18.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">2</span></div>
<div style="position:absolute;left:300.58px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 1 Introduction and motivation</span></div>
<div style="position:absolute;left:72.00px;top:80.43px" class="cls_006"><span class="cls_006">Web accessibility is an important concern with the rapidly increasing number of internet</span></div>
<div style="position:absolute;left:72.00px;top:96.87px" class="cls_006"><span class="cls_006">users and online content in this modern digital age. Internet usage penetration rate has</span></div>
<div style="position:absolute;left:72.00px;top:113.31px" class="cls_006"><span class="cls_006">grown over 1000 % in the past decade and currently, over 58% of the world, i.e.  over</span></div>
<div style="position:absolute;left:72.00px;top:129.74px" class="cls_006"><span class="cls_006">four and a half billion people are connected to the internet (20).  The World Health</span></div>
<div style="position:absolute;left:72.00px;top:146.18px" class="cls_006"><span class="cls_006">Organization (WHO) estimates that about 15% of the worlds population lives with</span></div>
<div style="position:absolute;left:72.00px;top:162.61px" class="cls_006"><span class="cls_006">some form of disability (35), and this accounts for over 600 million internet users.  It</span></div>
<div style="position:absolute;left:72.00px;top:179.05px" class="cls_006"><span class="cls_006">is a challenge for them to consume internet content at the same speed and efficiency</span></div>
<div style="position:absolute;left:72.00px;top:195.48px" class="cls_006"><span class="cls_006">as any other person.  In this scenario, technology can be used to assist them and aid</span></div>
<div style="position:absolute;left:72.00px;top:211.92px" class="cls_006"><span class="cls_006">them and improve their online experience. Recently Equadex partnered with Microsoft</span></div>
<div style="position:absolute;left:72.00px;top:228.35px" class="cls_006"><span class="cls_006">to create an application that uses AI technologies like computer vision, speech to text,</span></div>
<div style="position:absolute;left:72.00px;top:244.79px" class="cls_006"><span class="cls_006">text translation, and natural language understanding to help autistic children better</span></div>
<div style="position:absolute;left:72.00px;top:261.22px" class="cls_006"><span class="cls_006">interact and communicate with their environment (43). This is an example in which AI</span></div>
<div style="position:absolute;left:72.00px;top:277.66px" class="cls_006"><span class="cls_006">technology can not only assist disabled people but enable them with new ways to interact</span></div>
<div style="position:absolute;left:72.00px;top:294.09px" class="cls_006"><span class="cls_006">with the world that was not possible a few years back.  Recently, Microsoft launched</span></div>
<div style="position:absolute;left:72.00px;top:310.53px" class="cls_006"><span class="cls_006">its app, Seeing AI which leverages the camera of a smartphone and uses deep learning</span></div>
<div style="position:absolute;left:72.00px;top:326.96px" class="cls_006"><span class="cls_006">and computer vision algorithms to look at objects in the surrounding of a person and</span></div>
<div style="position:absolute;left:72.00px;top:343.40px" class="cls_006"><span class="cls_006">describes them (38).</span></div>
<div style="position:absolute;left:72.00px;top:369.23px" class="cls_006"><span class="cls_006">However, the application of this technology in the field of web accessibility, tools that</span></div>
<div style="position:absolute;left:72.00px;top:385.66px" class="cls_006"><span class="cls_006">leverage the scope of AI to improve web accessibility for the blind has been limited.</span></div>
<div style="position:absolute;left:72.00px;top:402.10px" class="cls_006"><span class="cls_006">Most popular screen readers like NVDA, ORCA, Apple VoiceOver, and ChromeVox use</span></div>
<div style="position:absolute;left:72.00px;top:418.53px" class="cls_006"><span class="cls_006">a simple text-to-speech mechanism to read out the website for a blind person and help</span></div>
<div style="position:absolute;left:72.00px;top:434.97px" class="cls_006"><span class="cls_006">navigate them (33). The idea of this project will be to explore how AI technologies can</span></div>
<div style="position:absolute;left:72.00px;top:451.40px" class="cls_006"><span class="cls_006">augment these tools and improve the experience of disabled people.  This has been a</span></div>
<div style="position:absolute;left:72.00px;top:467.84px" class="cls_006"><span class="cls_006">largely unexplored field and the scope of research in this field is huge, and if correctly</span></div>
<div style="position:absolute;left:72.00px;top:484.27px" class="cls_006"><span class="cls_006">applied it can help millions of people in their experience on the internet.</span></div>
<div style="position:absolute;left:72.00px;top:528.42px" class="cls_005"><span class="cls_005">1.1</span></div>
<div style="position:absolute;left:108.76px;top:528.42px" class="cls_005"><span class="cls_005">Project aim</span></div>
<div style="position:absolute;left:72.00px;top:568.49px" class="cls_006"><span class="cls_006">The primary aim of this project is to explore how Artificial Intelligence technologies can</span></div>
<div style="position:absolute;left:72.00px;top:584.92px" class="cls_006"><span class="cls_006">be applied in a manner to help make Web content more accessible, primarily for people</span></div>
<div style="position:absolute;left:72.00px;top:601.36px" class="cls_006"><span class="cls_006">with disabilities and apply them in this regard.</span></div>
<div style="position:absolute;left:72.00px;top:627.19px" class="cls_006"><span class="cls_006">The secondary aim of this project is to optimize and evaluate the algorithms created,</span></div>
<div style="position:absolute;left:72.00px;top:643.62px" class="cls_006"><span class="cls_006">build upon them and explore how they can be extended to other tasks in this field.</span></div>
<div style="position:absolute;left:72.00px;top:687.77px" class="cls_005"><span class="cls_005">1.2</span></div>
<div style="position:absolute;left:108.76px;top:687.77px" class="cls_005"><span class="cls_005">Objectives and methodology</span></div>
<div style="position:absolute;left:72.00px;top:727.84px" class="cls_006"><span class="cls_006">The first objective in this project would be to explore how AI technologies can be</span></div>
<div style="position:absolute;left:72.00px;top:744.28px" class="cls_006"><span class="cls_006">applied to the task of automatically captioning of images with a motivation to help</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:9372px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background19.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 1 Introduction and motivation</span></div>
<div style="position:absolute;left:518.46px;top:47.41px" class="cls_006"><span class="cls_006">3</span></div>
<div style="position:absolute;left:108.00px;top:80.43px" class="cls_006"><span class="cls_006">visually impaired people better understand what the image is trying to convey.  This</span></div>
<div style="position:absolute;left:108.00px;top:96.87px" class="cls_006"><span class="cls_006">task will involve:</span></div>
<div style="position:absolute;left:121.33px;top:134.65px" class="cls_006"><span class="cls_006">1. Research and experiment on appropriate datasets suitable for this task</span></div>
<div style="position:absolute;left:121.33px;top:160.06px" class="cls_006"><span class="cls_006">2. Research and experiment on different model architectures and optimize the chosen</span></div>
<div style="position:absolute;left:135.27px;top:176.49px" class="cls_006"><span class="cls_006">model</span></div>
<div style="position:absolute;left:121.33px;top:201.89px" class="cls_006"><span class="cls_006">3. Evaluate the model using standard metrics for related tasks</span></div>
<div style="position:absolute;left:121.33px;top:227.29px" class="cls_006"><span class="cls_006">4. Apply Natural Language Processing (NLP) techniques to better quantify model</span></div>
<div style="position:absolute;left:135.27px;top:243.73px" class="cls_006"><span class="cls_006">performance</span></div>
<div style="position:absolute;left:121.33px;top:269.13px" class="cls_006"><span class="cls_006">5. Explore how automatic captioning can be extended to other related tasks that</span></div>
<div style="position:absolute;left:135.27px;top:285.57px" class="cls_006"><span class="cls_006">would improve the web experience of visually impaired people</span></div>
<div style="position:absolute;left:108.00px;top:323.35px" class="cls_006"><span class="cls_006">The next objective would be to detect if hyperlink texts are contextual to with the source</span></div>
<div style="position:absolute;left:108.00px;top:339.79px" class="cls_006"><span class="cls_006">page text and target page text. Often, on many websites, we find hyperlinks occurring</span></div>
<div style="position:absolute;left:108.00px;top:356.22px" class="cls_006"><span class="cls_006">randomly, unrelated to the current topic being discussed. My aim in this project is to</span></div>
<div style="position:absolute;left:108.00px;top:372.66px" class="cls_006"><span class="cls_006">find out if the hyperlink text, the text surrounding the hyperlink and the text (and title)</span></div>
<div style="position:absolute;left:108.00px;top:389.09px" class="cls_006"><span class="cls_006">of the link it points to are in close context or not. This will help detect if the hyperlink</span></div>
<div style="position:absolute;left:108.00px;top:405.53px" class="cls_006"><span class="cls_006">is contextual and relevant to the topic being discussed. This task will involve:</span></div>
<div style="position:absolute;left:121.33px;top:443.31px" class="cls_006"><span class="cls_006">1. Research and experiment on appropriate datasets suitable for this task</span></div>
<div style="position:absolute;left:121.33px;top:468.71px" class="cls_006"><span class="cls_006">2. Extend available datasets, if needed, for this task</span></div>
<div style="position:absolute;left:121.33px;top:494.11px" class="cls_006"><span class="cls_006">3. Build word embedding based networks to learn word associations and develop</span></div>
<div style="position:absolute;left:135.27px;top:510.55px" class="cls_006"><span class="cls_006">algorithms for detecting the similarity between hyperlink text, the source text,</span></div>
<div style="position:absolute;left:135.27px;top:526.98px" class="cls_006"><span class="cls_006">and the target link text.</span></div>
<div style="position:absolute;left:121.33px;top:552.38px" class="cls_006"><span class="cls_006">4. Apply and extend the NLP techniques in the previous task for this problem</span></div>
<div style="position:absolute;left:121.33px;top:577.79px" class="cls_006"><span class="cls_006">5. Compare results with pre-trained models on publicly available datasets and un-</span></div>
<div style="position:absolute;left:135.27px;top:594.22px" class="cls_006"><span class="cls_006">derstand the scope of creating separate text corpus for such scenarios</span></div>
<div style="position:absolute;left:108.00px;top:632.01px" class="cls_006"><span class="cls_006">While developing the project and making design choices, the scope of deploying the</span></div>
<div style="position:absolute;left:108.00px;top:648.44px" class="cls_006"><span class="cls_006">proposed solutions to the web has been largely taken into account. The algorithms have</span></div>
<div style="position:absolute;left:108.00px;top:664.88px" class="cls_006"><span class="cls_006">been optimized to train and provide results faster, and both time and space complexities</span></div>
<div style="position:absolute;left:108.00px;top:681.31px" class="cls_006"><span class="cls_006">have also been considered while writing the code.</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:10224px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background20.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">4</span></div>
<div style="position:absolute;left:300.58px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 1 Introduction and motivation</span></div>
<div style="position:absolute;left:220.46px;top:176.01px" class="cls_018"><span class="cls_018">Figure 1.2: Risk Analysis</span></div>
<div style="position:absolute;left:72.00px;top:204.45px" class="cls_005"><span class="cls_005">1.3</span></div>
<div style="position:absolute;left:108.76px;top:204.45px" class="cls_005"><span class="cls_005">Risk Analysis</span></div>
<div style="position:absolute;left:72.00px;top:244.52px" class="cls_006"><span class="cls_006">Deep Learning based applications and algorithms are computationally expensive. Also</span></div>
<div style="position:absolute;left:72.00px;top:260.95px" class="cls_006"><span class="cls_006">the models have to be trained and optimized experimentally, which is a time-consuming</span></div>
<div style="position:absolute;left:72.00px;top:277.39px" class="cls_006"><span class="cls_006">process. So slight errors can cause a big loss in time, and possible risks should be kept</span></div>
<div style="position:absolute;left:72.00px;top:293.82px" class="cls_006"><span class="cls_006">in mind.</span></div>
<div style="position:absolute;left:72.00px;top:319.65px" class="cls_006"><span class="cls_006">In developing these applications, errors are bound to occur and they will take time to</span></div>
<div style="position:absolute;left:72.00px;top:336.09px" class="cls_006"><span class="cls_006">fix. However, it does help to think about some possible risks and possible contingency</span></div>
<div style="position:absolute;left:72.00px;top:352.52px" class="cls_006"><span class="cls_006">plans to overcome the situation quickly and effectively.  Keeping this is mind, a risk</span></div>
<div style="position:absolute;left:72.00px;top:368.96px" class="cls_006"><span class="cls_006">analysis plan was developed.</span></div>
<div style="position:absolute;left:72.00px;top:394.79px" class="cls_006"><span class="cls_006">For every risk, we assign a corresponding probability or likelihood (P) of it occurring.</span></div>
<div style="position:absolute;left:72.00px;top:411.22px" class="cls_006"><span class="cls_006">Then, we also assign the impact the days will have. Since this project involves no cost,</span></div>
<div style="position:absolute;left:72.00px;top:427.66px" class="cls_006"><span class="cls_006">the impact (I) is the time (in days) approximately needed to fix the risk.  The Risk</span></div>
<div style="position:absolute;left:72.00px;top:444.09px" class="cls_006"><span class="cls_006">measure (R) can be calculated as R = I × P (9). This has been shown in Figure 1.2</span></div>
<div style="position:absolute;left:72.00px;top:469.92px" class="cls_006"><span class="cls_006">Based on the severity of the Risk Measure value (green means Risk Measure is low</span></div>
<div style="position:absolute;left:72.00px;top:486.36px" class="cls_006"><span class="cls_006">and red means high) and the type of risk, we can assign appropriate contingency plans.</span></div>
<div style="position:absolute;left:72.00px;top:502.79px" class="cls_006"><span class="cls_006">Because risk measure incorporates both probability and impact, we should handle risks</span></div>
<div style="position:absolute;left:72.00px;top:519.23px" class="cls_006"><span class="cls_006">with a high value of Risk Measure with utmost caution.  This is why for these risks</span></div>
<div style="position:absolute;left:72.00px;top:535.66px" class="cls_006"><span class="cls_006">associated with high-risk measure, asking for assistance from the supervisor is a good</span></div>
<div style="position:absolute;left:72.00px;top:552.10px" class="cls_006"><span class="cls_006">contingency plan.</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:11076px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background21.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:160.01px" class="cls_003"><span class="cls_003">Chapter 2</span></div>
<div style="position:absolute;left:108.00px;top:221.37px" class="cls_017"><span class="cls_017">Preliminaries and research</span></div>
<div style="position:absolute;left:108.00px;top:300.83px" class="cls_006"><span class="cls_006">As discussed previously in section 1.2, the primary aim for this project will be to de-</span></div>
<div style="position:absolute;left:108.00px;top:317.27px" class="cls_006"><span class="cls_006">velop an automatic image captioning system. A lot of work has been done in this field</span></div>
<div style="position:absolute;left:108.00px;top:333.70px" class="cls_006"><span class="cls_006">and this project is about understanding and exploring how these deep neural network</span></div>
<div style="position:absolute;left:108.00px;top:350.14px" class="cls_006"><span class="cls_006">architectures can be applied to the problem.  This chapter discusses some of the key</span></div>
<div style="position:absolute;left:108.00px;top:366.57px" class="cls_006"><span class="cls_006">technologies that will be used for this project and also provides an overview of how past</span></div>
<div style="position:absolute;left:108.00px;top:383.01px" class="cls_006"><span class="cls_006">contributions and research can be leveraged for the development.</span></div>
<div style="position:absolute;left:108.00px;top:426.97px" class="cls_005"><span class="cls_005">2.1</span></div>
<div style="position:absolute;left:144.76px;top:426.97px" class="cls_005"><span class="cls_005">The role of Convolutional Neural Networks (CNNs)</span></div>
<div style="position:absolute;left:108.00px;top:466.95px" class="cls_006"><span class="cls_006">The ImageNet competition (42) comprises a large database (25) containing around 14</span></div>
<div style="position:absolute;left:108.00px;top:483.39px" class="cls_006"><span class="cls_006">million images. These images are grouped into classes resembling the semantic hierarchy</span></div>
<div style="position:absolute;left:108.00px;top:499.82px" class="cls_006"><span class="cls_006">of WordNet (34).  The competition promotes developing computer vision algorithms</span></div>
<div style="position:absolute;left:108.00px;top:516.26px" class="cls_006"><span class="cls_006">primarily for the tasks of image classification and object detection.</span></div>
<div style="position:absolute;left:108.00px;top:541.99px" class="cls_006"><span class="cls_006">CNNs have gained huge popularity ever since AlexNet was developed. AlexNet (22) used</span></div>
<div style="position:absolute;left:108.00px;top:558.43px" class="cls_006"><span class="cls_006">a comparatively small deep neural network (five convolution layers) and 3 fully connected</span></div>
<div style="position:absolute;left:108.00px;top:574.86px" class="cls_006"><span class="cls_006">layers to get state-of-the-art results in the ImageNet Large Scale Visual Recognition</span></div>
<div style="position:absolute;left:108.00px;top:591.30px" class="cls_006"><span class="cls_006">Challenge competition in 2010.</span></div>
<div style="position:absolute;left:108.00px;top:631.65px" class="cls_002"><span class="cls_002">2.1.1</span></div>
<div style="position:absolute;left:149.10px;top:631.65px" class="cls_002"><span class="cls_002">The working of a basic CNN</span></div>
<div style="position:absolute;left:108.00px;top:665.48px" class="cls_006"><span class="cls_006">The working of a basic CNN can be explained through the diagram shown in figure 2.1.</span></div>
<div style="position:absolute;left:108.00px;top:681.91px" class="cls_006"><span class="cls_006">The basic steps involved in such a neural network are described below:</span></div>
<div style="position:absolute;left:121.33px;top:719.02px" class="cls_006"><span class="cls_006">1. Each convolution layer comprises of a number of image filters (kernels) that span</span></div>
<div style="position:absolute;left:135.27px;top:735.45px" class="cls_006"><span class="cls_006">over the image multiplying the corresponding image pixel values to the filter</span></div>
<div style="position:absolute;left:135.27px;top:751.89px" class="cls_006"><span class="cls_006">weights. This operation is called convolution.</span></div>
<div style="position:absolute;left:313.23px;top:773.49px" class="cls_006"><span class="cls_006">5</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:11928px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background22.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">6</span></div>
<div style="position:absolute;left:310.22px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 2 Preliminaries and research</span></div>
<div style="position:absolute;left:194.07px;top:81.56px" class="cls_019"><span class="cls_019">Multiple  such convolution an pooling layers</span></div>
<div style="position:absolute;left:315.58px;top:87.45px" class="cls_019"><span class="cls_019">Images are flatenned</span></div>
<div style="position:absolute;left:201.99px;top:103.64px" class="cls_019"><span class="cls_019">ReLU</span></div>
<div style="position:absolute;left:203.46px;top:128.36px" class="cls_019"><span class="cls_019">ReLU</span></div>
<div style="position:absolute;left:389.95px;top:127.18px" class="cls_019"><span class="cls_019">Output prediction</span></div>
<div style="position:absolute;left:203.46px;top:150.73px" class="cls_019"><span class="cls_019">ReLU</span></div>
<div style="position:absolute;left:173.84px;top:165.74px" class="cls_020"><span class="cls_020">Convolution layer</span></div>
<div style="position:absolute;left:339.53px;top:163.68px" class="cls_020"><span class="cls_020">Fully connected layer</span></div>
<div style="position:absolute;left:173.84px;top:170.15px" class="cls_021"><span class="cls_021">The filters act on the images followed by ReLU</span></div>
<div style="position:absolute;left:251.24px;top:167.80px" class="cls_020"><span class="cls_020">Pooling layer</span></div>
<div style="position:absolute;left:339.53px;top:168.09px" class="cls_021"><span class="cls_021">The feature maps are flattened and fed into a</span></div>
<div style="position:absolute;left:173.84px;top:174.27px" class="cls_021"><span class="cls_021">to produce a stack of feature maps</span></div>
<div style="position:absolute;left:251.24px;top:172.21px" class="cls_021"><span class="cls_021">Reduces the dimensionality of the feature maps</span></div>
<div style="position:absolute;left:339.53px;top:172.21px" class="cls_021"><span class="cls_021">fully connected layer. Multiple such layers can</span></div>
<div style="position:absolute;left:339.53px;top:176.33px" class="cls_021"><span class="cls_021">exists</span></div>
<div style="position:absolute;left:198.21px;top:193.34px" class="cls_018"><span class="cls_018">Figure 2.1: Basic CNN architecture</span></div>
<div style="position:absolute;left:85.33px;top:223.28px" class="cls_006"><span class="cls_006">2.</span></div>
<div style="position:absolute;left:99.27px;top:223.28px" class="cls_006"><span class="cls_006">By setting the filter weights appropriately the network learns patterns in the image</span></div>
<div style="position:absolute;left:99.27px;top:239.71px" class="cls_006"><span class="cls_006">like edges, borders, shapes, colors, texture, etc.</span></div>
<div style="position:absolute;left:85.33px;top:265.12px" class="cls_006"><span class="cls_006">3.</span></div>
<div style="position:absolute;left:99.27px;top:265.12px" class="cls_006"><span class="cls_006">The convolution is generally followed by an activation function that helps achieve</span></div>
<div style="position:absolute;left:99.27px;top:281.55px" class="cls_006"><span class="cls_006">non-linearity, popular ones being Rectified Linear Unit (ReLU) or hyperbolic tan-</span></div>
<div style="position:absolute;left:99.27px;top:297.99px" class="cls_006"><span class="cls_006">gent (tanh) (22)</span></div>
<div style="position:absolute;left:85.33px;top:323.39px" class="cls_006"><span class="cls_006">4.</span></div>
<div style="position:absolute;left:99.27px;top:323.39px" class="cls_006"><span class="cls_006">This convolving operation of the filters results in a set of new images called acti-</span></div>
<div style="position:absolute;left:99.27px;top:339.82px" class="cls_006"><span class="cls_006">vation or feature maps. Thus, every image, after a convolution operation becomes</span></div>
<div style="position:absolute;left:99.27px;top:356.26px" class="cls_006"><span class="cls_006">a stack of feature maps.</span></div>
<div style="position:absolute;left:85.33px;top:381.66px" class="cls_006"><span class="cls_006">5.</span></div>
<div style="position:absolute;left:99.27px;top:381.66px" class="cls_006"><span class="cls_006">The next step is usually to apply a Pooling operation to reduce the dimensionality</span></div>
<div style="position:absolute;left:99.27px;top:398.10px" class="cls_006"><span class="cls_006">of the feature maps for the remaining part of the network, while still preserving</span></div>
<div style="position:absolute;left:99.27px;top:414.53px" class="cls_006"><span class="cls_006">the pattern.  For example, AlexNet (22) uses a Max Pooling operation, which</span></div>
<div style="position:absolute;left:99.27px;top:430.97px" class="cls_006"><span class="cls_006">effectively reduces the dimensionality of the feature maps by half.</span></div>
<div style="position:absolute;left:85.33px;top:456.37px" class="cls_006"><span class="cls_006">6.</span></div>
<div style="position:absolute;left:99.27px;top:456.37px" class="cls_006"><span class="cls_006">To prevent overfitting, AlexNet and many popular CNN architectures use a tech-</span></div>
<div style="position:absolute;left:99.27px;top:472.80px" class="cls_006"><span class="cls_006">nique called Dropout (15), which basically ignores the contribution of each hidden</span></div>
<div style="position:absolute;left:99.27px;top:489.24px" class="cls_006"><span class="cls_006">neuron with a certain pre-set probability.</span></div>
<div style="position:absolute;left:85.33px;top:514.64px" class="cls_006"><span class="cls_006">7.</span></div>
<div style="position:absolute;left:99.27px;top:514.64px" class="cls_006"><span class="cls_006">Multiple such Convolution and Pooling layers are stacked together. The output of</span></div>
<div style="position:absolute;left:99.27px;top:531.07px" class="cls_006"><span class="cls_006">the last layer is flattened and fed into a fully connected (dense) layer.</span></div>
<div style="position:absolute;left:72.00px;top:568.86px" class="cls_006"><span class="cls_006">There may be multiple such layers.  The final output from this layer is generally a K</span></div>
<div style="position:absolute;left:72.00px;top:585.29px" class="cls_006"><span class="cls_006">dimensional vector, K being the number of classes to predict. These K output variables</span></div>
<div style="position:absolute;left:72.00px;top:601.73px" class="cls_006"><span class="cls_006">represent the corresponding probability of the image falling in that particular category</span></div>
<div style="position:absolute;left:72.00px;top:618.16px" class="cls_006"><span class="cls_006">and are generally accompanied by a softmax activation function.  The network learns</span></div>
<div style="position:absolute;left:72.00px;top:634.60px" class="cls_006"><span class="cls_006">through the backpropagation algorithm (21) - after every iteration, the weights of the</span></div>
<div style="position:absolute;left:72.00px;top:651.03px" class="cls_006"><span class="cls_006">filters, as well as the fully connected layers, are updated to reduce the loss of the network.</span></div>
<div style="position:absolute;left:72.00px;top:691.57px" class="cls_002"><span class="cls_002">2.1.2</span></div>
<div style="position:absolute;left:113.10px;top:691.57px" class="cls_002"><span class="cls_002">Transfer Learning</span></div>
<div style="position:absolute;left:72.00px;top:725.49px" class="cls_006"><span class="cls_006">Transfer Learning can be used for the task of captioning images.  In transfer learning</span></div>
<div style="position:absolute;left:72.00px;top:741.92px" class="cls_006"><span class="cls_006">(66), we use a network, or part of a network that has been trained on a related task</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:12780px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background23.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 2 Preliminaries and research</span></div>
<div style="position:absolute;left:518.46px;top:47.41px" class="cls_006"><span class="cls_006">7</span></div>
<div style="position:absolute;left:108.00px;top:80.43px" class="cls_006"><span class="cls_006">(like image classification) and use it for a similar task. By doing this we hope that the</span></div>
<div style="position:absolute;left:108.00px;top:96.87px" class="cls_006"><span class="cls_006">network has learned some general features about the data it was originally trained on,</span></div>
<div style="position:absolute;left:108.00px;top:113.31px" class="cls_006"><span class="cls_006">and these learned features can be applied to the task we are aiming to solve.</span></div>
<div style="position:absolute;left:108.00px;top:139.03px" class="cls_006"><span class="cls_006">Oxford Visual Geometry Group or VGG model won the ImageNet competition described</span></div>
<div style="position:absolute;left:108.00px;top:155.47px" class="cls_006"><span class="cls_006">above in 2014 (46). Since then it has been used as a transfer learning CNN model for a</span></div>
<div style="position:absolute;left:108.00px;top:171.90px" class="cls_006"><span class="cls_006">wide variety of applications. What makes it a good model for our use case is:</span></div>
<div style="position:absolute;left:121.33px;top:208.95px" class="cls_006"><span class="cls_006">1. The network is fairly simple to understand and implement</span></div>
<div style="position:absolute;left:121.33px;top:234.14px" class="cls_006"><span class="cls_006">2. Since the network is only 16 or 19 layers deep, the number of trainable parameters</span></div>
<div style="position:absolute;left:135.27px;top:250.58px" class="cls_006"><span class="cls_006">is quite manageable</span></div>
<div style="position:absolute;left:121.33px;top:275.76px" class="cls_006"><span class="cls_006">3. The network relies on minimal pre-processing. The images are normalized by mean</span></div>
<div style="position:absolute;left:135.27px;top:292.20px" class="cls_006"><span class="cls_006">centering each pixel value. Other than that, no pre-processing is necessary</span></div>
<div style="position:absolute;left:121.33px;top:317.39px" class="cls_006"><span class="cls_006">4. The network is sophisticated enough to perform classification with high accuracy</span></div>
<div style="position:absolute;left:135.27px;top:333.82px" class="cls_006"><span class="cls_006">and also simple enough to generalize well to other datasets and problems</span></div>
<div style="position:absolute;left:108.00px;top:370.87px" class="cls_006"><span class="cls_006">The role of the CNN model used in the Image Captioning task is to basically understand</span></div>
<div style="position:absolute;left:108.00px;top:387.31px" class="cls_006"><span class="cls_006">the features that distinguish each object. The final layer of the VGG model is a fully-</span></div>
<div style="position:absolute;left:108.00px;top:403.74px" class="cls_006"><span class="cls_006">connected layer with 1000 classes.  We remove this layer as for the purpose of our</span></div>
<div style="position:absolute;left:108.00px;top:420.18px" class="cls_006"><span class="cls_006">problem, we are not interested in the final classification. We look one layer back. This</span></div>
<div style="position:absolute;left:108.00px;top:436.61px" class="cls_006"><span class="cls_006">layer is a fully-connected layer with 4096 neurons.  This is the layer that extracts the</span></div>
<div style="position:absolute;left:108.00px;top:453.05px" class="cls_006"><span class="cls_006">high-level features from the image.  We use the output of this layer for the captioning</span></div>
<div style="position:absolute;left:108.00px;top:469.48px" class="cls_006"><span class="cls_006">model</span></div>
<div style="position:absolute;left:108.00px;top:509.81px" class="cls_002"><span class="cls_002">2.1.3</span></div>
<div style="position:absolute;left:149.10px;top:509.81px" class="cls_002"><span class="cls_002">Transfer Learning in Word Embeddings</span></div>
<div style="position:absolute;left:108.00px;top:543.63px" class="cls_006"><span class="cls_006">Neural network architectures have become popular in Natural Language Processing</span></div>
<div style="position:absolute;left:108.00px;top:560.07px" class="cls_006"><span class="cls_006">tasks.</span></div>
<div style="position:absolute;left:143.34px;top:560.07px" class="cls_006"><span class="cls_006">(31) uses a method to learn efficient word embeddings in vector space.  Each</span></div>
<div style="position:absolute;left:108.00px;top:576.50px" class="cls_006"><span class="cls_006">word is represented in vector space by a dense vector of a pre-fixed dimensionality. Also</span></div>
<div style="position:absolute;left:108.00px;top:592.94px" class="cls_006"><span class="cls_006">the model is trained in a manner that similar words occur in close proximity in vector</span></div>
<div style="position:absolute;left:108.00px;top:609.37px" class="cls_006"><span class="cls_006">space.</span></div>
<div style="position:absolute;left:108.00px;top:635.10px" class="cls_006"><span class="cls_006">Pre-trained architectures and word embeddings exist (12) in this regard, which can be</span></div>
<div style="position:absolute;left:108.00px;top:651.54px" class="cls_006"><span class="cls_006">used for the contextual hyperlink detection task as discussed in section 1.2.</span></div>
<div style="position:absolute;left:108.00px;top:695.49px" class="cls_005"><span class="cls_005">2.2</span></div>
<div style="position:absolute;left:144.76px;top:695.49px" class="cls_005"><span class="cls_005">The role of Recurrent Neural Networks (RNN)</span></div>
<div style="position:absolute;left:108.00px;top:735.45px" class="cls_006"><span class="cls_006">Recurrent Neural Networks (RNNs) have achieved outstanding results in sequence to</span></div>
<div style="position:absolute;left:108.00px;top:751.89px" class="cls_006"><span class="cls_006">sequence-based modeling tasks (45). The task of captioning an image can also be viewed</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:13632px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background24.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">8</span></div>
<div style="position:absolute;left:310.22px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 2 Preliminaries and research</span></div>
<div style="position:absolute;left:296.94px;top:94.44px" class="cls_023"><span class="cls_023">Text</span></div>
<div style="position:absolute;left:356.28px;top:94.44px" class="cls_024"><span class="cls_024">Encoding layer</span></div>
<div style="position:absolute;left:206.36px;top:163.56px" class="cls_024"><span class="cls_024">CNN Network</span></div>
<div style="position:absolute;left:358.63px;top:163.56px" class="cls_024"><span class="cls_024">RNN network</span></div>
<div style="position:absolute;left:358.05px;top:213.34px" class="cls_023"><span class="cls_023">Predicted text</span></div>
<div style="position:absolute;left:267.25px;top:225.05px" class="cls_024"><span class="cls_024">Extracted features</span></div>
<div style="position:absolute;left:187.26px;top:248.74px" class="cls_018"><span class="cls_018">Figure 2.2: Proposed solution - overview</span></div>
<div style="position:absolute;left:180.71px;top:465.16px" class="cls_018"><span class="cls_018">Figure 2.3: Basic architecture of LSTM(41)</span></div>
<div style="position:absolute;left:72.00px;top:497.59px" class="cls_006"><span class="cls_006">as such.  Given a set of features from an image we want to translate (caption) these</span></div>
<div style="position:absolute;left:72.00px;top:514.02px" class="cls_006"><span class="cls_006">features to text that would represent the corresponding captions. We discussed already</span></div>
<div style="position:absolute;left:72.00px;top:530.46px" class="cls_006"><span class="cls_006">that the output of our CNN model will be a list of features that characterize the image.</span></div>
<div style="position:absolute;left:72.00px;top:546.89px" class="cls_006"><span class="cls_006">The idea is to feed these features along with part of the training caption into the RNN</span></div>
<div style="position:absolute;left:72.00px;top:563.33px" class="cls_006"><span class="cls_006">model and have it predict the next word in the sequence. This is described in the figure</span></div>
<div style="position:absolute;left:72.00px;top:579.76px" class="cls_006"><span class="cls_006">2.2:</span></div>
<div style="position:absolute;left:72.00px;top:605.59px" class="cls_006"><span class="cls_006">The problem with vanilla recurrent neural networks is that they look back only limited</span></div>
<div style="position:absolute;left:72.00px;top:622.03px" class="cls_006"><span class="cls_006">time steps.  They doe not have a feature to retain items in memory.  Basically the</span></div>
<div style="position:absolute;left:72.00px;top:638.46px" class="cls_006"><span class="cls_006">problem is that it has very short term memory (1). The solution is to use a new type of</span></div>
<div style="position:absolute;left:72.00px;top:654.90px" class="cls_006"><span class="cls_006">RNN called Long short-term memory (LSTM) (16).</span></div>
<div style="position:absolute;left:72.00px;top:680.73px" class="cls_006"><span class="cls_006">The key part of LSTM is the addition of a memory branch as shown in 2.3. This enables</span></div>
<div style="position:absolute;left:72.00px;top:697.16px" class="cls_006"><span class="cls_006">the network to store information that is potentially useful.  The network learns what</span></div>
<div style="position:absolute;left:72.00px;top:713.60px" class="cls_006"><span class="cls_006">information to store and what to forget through another neural network that learns this</span></div>
<div style="position:absolute;left:72.00px;top:730.03px" class="cls_006"><span class="cls_006">through the training data.  The selection part is like a filter that filters the relevant</span></div>
<div style="position:absolute;left:72.00px;top:746.47px" class="cls_006"><span class="cls_006">predictions from the collected possibilities. The last part is like an attention mechanism</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:14484px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background25.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 2 Preliminaries and research</span></div>
<div style="position:absolute;left:518.46px;top:47.41px" class="cls_006"><span class="cls_006">9</span></div>
<div style="position:absolute;left:108.00px;top:80.43px" class="cls_006"><span class="cls_006">that filters the possible possibilities so that the memory is not overloaded (41).  This</span></div>
<div style="position:absolute;left:108.00px;top:96.87px" class="cls_006"><span class="cls_006">entire system can sufficiently overcome the problems of RNN discussed earlier and will</span></div>
<div style="position:absolute;left:108.00px;top:113.31px" class="cls_006"><span class="cls_006">be an essential part of this project.</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:15336px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background26.jpg" width=595 height=842></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:15336px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background27.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:160.10px" class="cls_003"><span class="cls_003">Chapter 3</span></div>
<div style="position:absolute;left:108.00px;top:221.55px" class="cls_017"><span class="cls_017">Automatic Image Captioning</span></div>
<div style="position:absolute;left:108.00px;top:257.80px" class="cls_017"><span class="cls_017">System</span></div>
<div style="position:absolute;left:108.00px;top:339.24px" class="cls_005"><span class="cls_005">3.1</span></div>
<div style="position:absolute;left:144.76px;top:339.24px" class="cls_005"><span class="cls_005">The Problem</span></div>
<div style="position:absolute;left:108.00px;top:379.31px" class="cls_006"><span class="cls_006">The World Wide Web Consortium (W3C) is an organization responsible for developing</span></div>
<div style="position:absolute;left:108.00px;top:395.74px" class="cls_006"><span class="cls_006">and maintaining web standards such as HTML, CSS, etc.</span></div>
<div style="position:absolute;left:403.79px;top:395.74px" class="cls_006"><span class="cls_006">(57).  The Web Content</span></div>
<div style="position:absolute;left:108.00px;top:412.18px" class="cls_006"><span class="cls_006">Accessibility Guidelines (WCAG) is developed through the W3C process and it aims</span></div>
<div style="position:absolute;left:108.00px;top:428.62px" class="cls_006"><span class="cls_006">to create and maintain a single set of guidelines and recommendations for individuals,</span></div>
<div style="position:absolute;left:108.00px;top:445.05px" class="cls_006"><span class="cls_006">organizations, and governments internationally to follow to make web content more</span></div>
<div style="position:absolute;left:108.00px;top:461.49px" class="cls_006"><span class="cls_006">accessible and inclusive, especially for people with disabilities (58; 64).</span></div>
<div style="position:absolute;left:108.00px;top:487.31px" class="cls_006"><span class="cls_006">Guideline H37 (56) of the WCAG focuses on the proper use of alternative (alt) texts for</span></div>
<div style="position:absolute;left:108.00px;top:503.75px" class="cls_006"><span class="cls_006">images to help visually impaired people understand the message the image is trying to</span></div>
<div style="position:absolute;left:108.00px;top:520.18px" class="cls_006"><span class="cls_006">convey. Often developers fail to provide the above-mentioned alt texts and even if they</span></div>
<div style="position:absolute;left:108.00px;top:536.62px" class="cls_006"><span class="cls_006">do, the text does not really convey the message of the image.</span></div>
<div style="position:absolute;left:108.00px;top:562.45px" class="cls_006"><span class="cls_006">Automatic Image captioning is a challenging task because it combines the workings of</span></div>
<div style="position:absolute;left:108.00px;top:578.88px" class="cls_006"><span class="cls_006">both CNNs and RNNs together. The CNN must understand the high-level features of</span></div>
<div style="position:absolute;left:108.00px;top:595.32px" class="cls_006"><span class="cls_006">the image and the RNN must translate these features into relevant captions as discussed</span></div>
<div style="position:absolute;left:108.00px;top:611.75px" class="cls_006"><span class="cls_006">in subsection 2.1.2.</span></div>
<div style="position:absolute;left:108.00px;top:655.90px" class="cls_005"><span class="cls_005">3.2</span></div>
<div style="position:absolute;left:144.76px;top:655.90px" class="cls_005"><span class="cls_005">The Dataset</span></div>
<div style="position:absolute;left:108.00px;top:695.97px" class="cls_006"><span class="cls_006">There are several options for a dataset of images accompanied by their corresponding</span></div>
<div style="position:absolute;left:108.00px;top:712.41px" class="cls_006"><span class="cls_006">captions. Some of these are Flickr8k (17), Microsoft COCO: Common objects in context</span></div>
<div style="position:absolute;left:108.00px;top:728.84px" class="cls_006"><span class="cls_006">(MSCOCO) (27), Conceptual captions dataset by Google (44). I have used the Flickr8k</span></div>
<div style="position:absolute;left:108.00px;top:745.28px" class="cls_006"><span class="cls_006">dataset for this task because of the following reasons:</span></div>
<div style="position:absolute;left:310.50px;top:773.49px" class="cls_006"><span class="cls_006">11</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:16188px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background28.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">12</span></div>
<div style="position:absolute;left:255.40px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:85.33px;top:80.43px" class="cls_006"><span class="cls_006">1. The dataset is relatively small so that the model can be trained using reasonable</span></div>
<div style="position:absolute;left:99.27px;top:96.87px" class="cls_006"><span class="cls_006">RAM (less than 30GB). As mentioned in section 1.3, there are risks associated</span></div>
<div style="position:absolute;left:99.27px;top:113.31px" class="cls_006"><span class="cls_006">with RAM over-utilization and RAM demand should be taken into account</span></div>
<div style="position:absolute;left:85.33px;top:137.74px" class="cls_006"><span class="cls_006">2. The dataset is representative and contains a wide variety of images</span></div>
<div style="position:absolute;left:85.33px;top:162.17px" class="cls_006"><span class="cls_006">3. Each image has five caption texts that were collected via crowdsourcing</span></div>
<div style="position:absolute;left:85.33px;top:186.60px" class="cls_006"><span class="cls_006">4. The dataset contains over eight thousand images split into training (six thousand)</span></div>
<div style="position:absolute;left:99.27px;top:203.04px" class="cls_006"><span class="cls_006">validation (one thousand) and testing (one thousand) parts</span></div>
<div style="position:absolute;left:85.33px;top:227.47px" class="cls_006"><span class="cls_006">5. There exists multiple copies of the dataset, available for downloading, so we have</span></div>
<div style="position:absolute;left:99.27px;top:243.91px" class="cls_006"><span class="cls_006">options in case the primary link does not work. As dicussed in section 1.3, this is</span></div>
<div style="position:absolute;left:99.27px;top:260.34px" class="cls_006"><span class="cls_006">a good option to have</span></div>
<div style="position:absolute;left:72.00px;top:303.58px" class="cls_005"><span class="cls_005">3.3</span></div>
<div style="position:absolute;left:108.76px;top:303.58px" class="cls_005"><span class="cls_005">Data Cleaning and pre-processing</span></div>
<div style="position:absolute;left:72.00px;top:343.19px" class="cls_006"><span class="cls_006">In this task, we are dealing with both image data as well as textual data, which has</span></div>
<div style="position:absolute;left:72.00px;top:359.62px" class="cls_006"><span class="cls_006">been crowdsourced (17).  Data cleaning and preprocessing is very important for the</span></div>
<div style="position:absolute;left:72.00px;top:376.06px" class="cls_006"><span class="cls_006">performance of the deep learning model.</span></div>
<div style="position:absolute;left:72.00px;top:401.43px" class="cls_006"><span class="cls_006">The pre-processing steps vary for images and text.  The pre-processing for the images</span></div>
<div style="position:absolute;left:72.00px;top:417.87px" class="cls_006"><span class="cls_006">involves:</span></div>
<div style="position:absolute;left:85.33px;top:452.29px" class="cls_006"><span class="cls_006">1. Resize the images to dimensions: 224x224x3 - 224 is the image height and width</span></div>
<div style="position:absolute;left:99.27px;top:468.72px" class="cls_006"><span class="cls_006">(in pixels). 3 denotes the number of color channels (RGB)</span></div>
<div style="position:absolute;left:85.33px;top:493.15px" class="cls_006"><span class="cls_006">2. Normalize the images by mean centering them.  The mean RGB value was sub-</span></div>
<div style="position:absolute;left:99.27px;top:509.59px" class="cls_006"><span class="cls_006">tracted from each pixel value of the image</span></div>
<div style="position:absolute;left:72.00px;top:544.01px" class="cls_006"><span class="cls_006">For natural language processing based tasks it is a good practice to clean text data and</span></div>
<div style="position:absolute;left:72.00px;top:560.45px" class="cls_006"><span class="cls_006">create a text cleaning pipeline using tools like python nltk (2; 6). The steps in the text</span></div>
<div style="position:absolute;left:72.00px;top:576.88px" class="cls_006"><span class="cls_006">cleaning pipeline suitable for our task include:</span></div>
<div style="position:absolute;left:85.33px;top:611.30px" class="cls_006"><span class="cls_006">1. Tokenize the captions into separate words</span></div>
<div style="position:absolute;left:85.33px;top:635.73px" class="cls_006"><span class="cls_006">2. Case Normalization - Convert all words to lowercase</span></div>
<div style="position:absolute;left:85.33px;top:660.17px" class="cls_006"><span class="cls_006">3. Remove punctuations from the words</span></div>
<div style="position:absolute;left:85.33px;top:684.60px" class="cls_006"><span class="cls_006">4. Remove non-alphanumeric characters from the words</span></div>
<div style="position:absolute;left:72.00px;top:719.02px" class="cls_006"><span class="cls_006">For more traditional machine learning tasks additional steps like stemming and lemma-</span></div>
<div style="position:absolute;left:72.00px;top:735.45px" class="cls_006"><span class="cls_006">tization need to be carried out, but because our model is going to have an embedding</span></div>
<div style="position:absolute;left:72.00px;top:751.89px" class="cls_006"><span class="cls_006">layer, it does not make sense to perform additional preprocessing (6).</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:17040px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background29.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:513.00px;top:47.41px" class="cls_006"><span class="cls_006">13</span></div>
<div style="position:absolute;left:189.76px;top:232.27px" class="cls_018"><span class="cls_018">Figure 3.1: Performance adavantage of transfer learning</span></div>
<div style="position:absolute;left:108.00px;top:260.71px" class="cls_005"><span class="cls_005">3.4</span></div>
<div style="position:absolute;left:144.76px;top:260.71px" class="cls_005"><span class="cls_005">Model for Image Classification</span></div>
<div style="position:absolute;left:108.00px;top:300.78px" class="cls_006"><span class="cls_006">As previously mentioned in section 2.1, we use CNN generally for image classification.</span></div>
<div style="position:absolute;left:108.00px;top:317.21px" class="cls_006"><span class="cls_006">In this task, we strip away the last layer of the CNN model.  This is because we are</span></div>
<div style="position:absolute;left:108.00px;top:333.65px" class="cls_006"><span class="cls_006">only interested in the high-level features that the CNN learns from the image, and not</span></div>
<div style="position:absolute;left:108.00px;top:350.08px" class="cls_006"><span class="cls_006">on the final classification.  These features can be fed into the RNN along with part of</span></div>
<div style="position:absolute;left:108.00px;top:366.52px" class="cls_006"><span class="cls_006">the corresponding text of the caption.  The features and the text together are used to</span></div>
<div style="position:absolute;left:108.00px;top:382.95px" class="cls_006"><span class="cls_006">predict the next text in the caption as shown in Figure 2.2.</span></div>
<div style="position:absolute;left:108.00px;top:408.78px" class="cls_006"><span class="cls_006">For the purpose of this task, I initially tried training my own CNN models, but the</span></div>
<div style="position:absolute;left:108.00px;top:425.22px" class="cls_006"><span class="cls_006">results were not good.  Then, I used Transfer learning, where we re-use a model that</span></div>
<div style="position:absolute;left:108.00px;top:441.65px" class="cls_006"><span class="cls_006">has already been developed for a certain task for a related but unidentical task (5). We</span></div>
<div style="position:absolute;left:108.00px;top:458.09px" class="cls_006"><span class="cls_006">can use a pre-trained network for recognizing and classifying images and use it to get</span></div>
<div style="position:absolute;left:108.00px;top:474.52px" class="cls_006"><span class="cls_006">the high-level features of the images. Transfer learning helps reduce running time as the</span></div>
<div style="position:absolute;left:108.00px;top:490.96px" class="cls_006"><span class="cls_006">model does not have to be trained from scratch. Figure 3.1 also suggests how transfer</span></div>
<div style="position:absolute;left:108.00px;top:507.39px" class="cls_006"><span class="cls_006">learning can lead to better performance, as the model has a better start as it does not</span></div>
<div style="position:absolute;left:108.00px;top:523.83px" class="cls_006"><span class="cls_006">need to be trained from scratch.  The rate of increase in performance (or decrease in</span></div>
<div style="position:absolute;left:108.00px;top:540.26px" class="cls_006"><span class="cls_006">loss) is also higher and these two factors help the model reach a better final performance</span></div>
<div style="position:absolute;left:108.00px;top:556.70px" class="cls_006"><span class="cls_006">score (5).</span></div>
<div style="position:absolute;left:108.00px;top:582.53px" class="cls_006"><span class="cls_006">Various options, like AlexNet (22), VGG networks(46), Resnet (13) and GoogleNet</span></div>
<div style="position:absolute;left:108.00px;top:598.96px" class="cls_006"><span class="cls_006">(49) exist for using Transfer Learning for Image Classification. For this project, I have</span></div>
<div style="position:absolute;left:108.00px;top:615.40px" class="cls_006"><span class="cls_006">experimented with the following architectures:</span></div>
<div style="position:absolute;left:124.36px;top:653.18px" class="cls_006"><span class="cls_006">• AlexNet</span></div>
<div style="position:absolute;left:124.36px;top:678.58px" class="cls_006"><span class="cls_006">• VGG 16 - a variation of VGG network with 16 layers</span></div>
<div style="position:absolute;left:124.36px;top:703.98px" class="cls_006"><span class="cls_006">• VGG 19 - a variation of VGG network with 19 layers</span></div>
<div style="position:absolute;left:108.00px;top:741.77px" class="cls_006"><span class="cls_006">Some observations after running the experiments:</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:17892px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background30.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">14</span></div>
<div style="position:absolute;left:255.40px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:85.33px;top:80.43px" class="cls_006"><span class="cls_006">1. The performance was highest for VGG 19, then VGG 16 and least AlexNet, which</span></div>
<div style="position:absolute;left:99.27px;top:96.87px" class="cls_006"><span class="cls_006">is to be expected, considering the fact that deeper the network, better the perfor-</span></div>
<div style="position:absolute;left:99.27px;top:113.31px" class="cls_006"><span class="cls_006">mance, in general</span></div>
<div style="position:absolute;left:85.33px;top:137.20px" class="cls_006"><span class="cls_006">2. The performance improvement for VGG 19 over VGG 16 was 0.7% on the test</span></div>
<div style="position:absolute;left:99.27px;top:153.63px" class="cls_006"><span class="cls_006">dataset.  But the training time was considerably larger (23 minutes longer on 25</span></div>
<div style="position:absolute;left:99.27px;top:170.07px" class="cls_006"><span class="cls_006">GB RAM)</span></div>
<div style="position:absolute;left:72.00px;top:202.62px" class="cls_006"><span class="cls_006">Thus, VGG 16 was selected as the final model and it was used it to extract the photo</span></div>
<div style="position:absolute;left:72.00px;top:219.05px" class="cls_006"><span class="cls_006">features. Each feature has a dimensionality of 4096.</span></div>
<div style="position:absolute;left:72.00px;top:258.16px" class="cls_002"><span class="cls_002">3.4.1</span></div>
<div style="position:absolute;left:113.10px;top:258.16px" class="cls_002"><span class="cls_002">Optimization of the VGG-16 model</span></div>
<div style="position:absolute;left:72.00px;top:291.37px" class="cls_006"><span class="cls_006">Running each image through the entire VGG network takes a long time and had scope</span></div>
<div style="position:absolute;left:72.00px;top:307.81px" class="cls_006"><span class="cls_006">for optimization. The process was:</span></div>
<div style="position:absolute;left:72.96px;top:327.91px" class="cls_025"><span class="cls_025">Assign a unique  id  to  each image  in  the  dataset</span></div>
<div style="position:absolute;left:72.93px;top:341.20px" class="cls_025"><span class="cls_025">Create a  dictionary  of  form</span></div>
<div style="position:absolute;left:227.28px;top:341.20px" class="cls_025"><span class="cls_025">{</span></div>
<div style="position:absolute;left:248.60px;top:341.20px" class="cls_025"><span class="cls_025">i d</span></div>
<div style="position:absolute;left:278.06px;top:341.20px" class="cls_025"><span class="cls_025">:</span></div>
<div style="position:absolute;left:289.76px;top:341.20px" class="cls_025"><span class="cls_025">[...</span></div>
<div style="position:absolute;left:310.61px;top:341.20px" class="cls_025"><span class="cls_025">,  features ,</span></div>
<div style="position:absolute;left:394.00px;top:341.20px" class="cls_025"><span class="cls_025">]}</span></div>
<div style="position:absolute;left:72.59px;top:354.49px" class="cls_025"><span class="cls_025">For each image</span></div>
<div style="position:absolute;left:94.01px;top:367.79px" class="cls_025"><span class="cls_025">Run  t h e  image  t h r o u g h  t h e  VGG  network  e x c e p t  f o r  t h e  l a s t  l a y e r</span></div>
<div style="position:absolute;left:95.13px;top:381.08px" class="cls_025"><span class="cls_025">Extract  the  features  for  that image</span></div>
<div style="position:absolute;left:95.22px;top:394.37px" class="cls_025"><span class="cls_025">Store  the image  id  as  the key and the  list  of  features  as  the  value  in</span></div>
<div style="position:absolute;left:92.87px;top:407.67px" class="cls_025"><span class="cls_025">the  dictionary</span></div>
<div style="position:absolute;left:72.69px;top:420.96px" class="cls_025"><span class="cls_025">Return the formed  dictionary</span></div>
<div style="position:absolute;left:180.04px;top:440.69px" class="cls_018"><span class="cls_018">Listing 3.1: Optimization for photo features</span></div>
<div style="position:absolute;left:72.00px;top:471.56px" class="cls_006"><span class="cls_006">Once the dictionary is formed, we can easily look up the corresponding features for an</span></div>
<div style="position:absolute;left:72.00px;top:488.00px" class="cls_006"><span class="cls_006">image using the id of the image</span></div>
<div style="position:absolute;left:72.00px;top:513.12px" class="cls_006"><span class="cls_006">Python supports pickle files for easily storing python objects, however, they do not</span></div>
<div style="position:absolute;left:72.00px;top:529.55px" class="cls_006"><span class="cls_006">scale well for big data (beyond 1GB) and are often filled with garbage values.  So,</span></div>
<div style="position:absolute;left:72.00px;top:545.99px" class="cls_006"><span class="cls_006">Hierarchical Data Format (HDF5) file (61) was used for storing the computed features.</span></div>
<div style="position:absolute;left:72.00px;top:562.42px" class="cls_006"><span class="cls_006">This optimization process reduced the training time of the final network from 50 minutes</span></div>
<div style="position:absolute;left:72.00px;top:578.86px" class="cls_006"><span class="cls_006">to almost 20 minutes.</span></div>
<div style="position:absolute;left:72.00px;top:603.97px" class="cls_006"><span class="cls_006">At the end of this step, we have computed the high-level features of an image. An</span></div>
<div style="position:absolute;left:72.00px;top:620.41px" class="cls_006"><span class="cls_006">important point to note here is that the choice of stripping away exactly one layer from</span></div>
<div style="position:absolute;left:72.00px;top:636.84px" class="cls_006"><span class="cls_006">the model is experimental. As discussed in the prev section 2.1, as we go deeper into the</span></div>
<div style="position:absolute;left:72.00px;top:653.28px" class="cls_006"><span class="cls_006">CNN, it learns more complicated features. One can argue that it might be a good idea</span></div>
<div style="position:absolute;left:72.00px;top:669.71px" class="cls_006"><span class="cls_006">to explore the results after stripping away the last two layers so that slightly less specific</span></div>
<div style="position:absolute;left:72.00px;top:686.15px" class="cls_006"><span class="cls_006">features are taken into account.  I tried this process, however, the performance of the</span></div>
<div style="position:absolute;left:72.00px;top:702.58px" class="cls_006"><span class="cls_006">model by stripping away 2 layers reduced dramatically and the network became under</span></div>
<div style="position:absolute;left:72.00px;top:719.02px" class="cls_006"><span class="cls_006">fitted. That means by stripping away 2 layers the network could not understand</span></div>
<div style="position:absolute;left:72.00px;top:735.45px" class="cls_006"><span class="cls_006">the complexities of the image well enough to associate the features with the</span></div>
<div style="position:absolute;left:72.00px;top:751.89px" class="cls_006"><span class="cls_006">captions.</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:18744px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background31.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:513.00px;top:47.41px" class="cls_006"><span class="cls_006">15</span></div>
<div style="position:absolute;left:197.80px;top:800.98px" class="cls_018"><span class="cls_018">Figure 3.2: VGG 16 architecture; last layer removed</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:19596px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background32.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">16</span></div>
<div style="position:absolute;left:255.40px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:72.00px;top:77.00px" class="cls_005"><span class="cls_005">3.5</span></div>
<div style="position:absolute;left:108.76px;top:77.00px" class="cls_005"><span class="cls_005">Creating the training set</span></div>
<div style="position:absolute;left:72.00px;top:117.07px" class="cls_006"><span class="cls_006">The CNN model has already identified the high-level features of the image.  The next</span></div>
<div style="position:absolute;left:72.00px;top:133.50px" class="cls_006"><span class="cls_006">step is to associate these features with the caption. As discussed in section 3.3 we have</span></div>
<div style="position:absolute;left:72.00px;top:149.94px" class="cls_006"><span class="cls_006">already run the text preprocessing pipeline on the captions.  Also, we have assigned a</span></div>
<div style="position:absolute;left:72.00px;top:166.37px" class="cls_006"><span class="cls_006">unique id to each image. So the data for some an image with id</span><span class="cls_026"><sub>1</sub></span><span class="cls_006"> is of the form:</span></div>
<div style="position:absolute;left:86.43px;top:187.98px" class="cls_025"><span class="cls_025">i d</span></div>
<div style="position:absolute;left:108.25px;top:187.98px" class="cls_025"><span class="cls_025">1</span></div>
<div style="position:absolute;left:128.78px;top:187.98px" class="cls_025"><span class="cls_025">:</span></div>
<div style="position:absolute;left:139.84px;top:187.98px" class="cls_025"><span class="cls_025">[caption  text</span></div>
<div style="position:absolute;left:216.73px;top:187.98px" class="cls_025"><span class="cls_025">1,  caption  text</span></div>
<div style="position:absolute;left:305.20px;top:187.98px" class="cls_025"><span class="cls_025">2,  caption  text</span></div>
<div style="position:absolute;left:393.67px;top:187.98px" class="cls_025"><span class="cls_025">3,  caption  text</span></div>
<div style="position:absolute;left:92.90px;top:201.27px" class="cls_025"><span class="cls_025">4,  caption  text</span></div>
<div style="position:absolute;left:181.69px;top:201.27px" class="cls_025"><span class="cls_025">5]</span></div>
<div style="position:absolute;left:86.43px;top:214.57px" class="cls_025"><span class="cls_025">i d</span></div>
<div style="position:absolute;left:108.25px;top:214.57px" class="cls_025"><span class="cls_025">2</span></div>
<div style="position:absolute;left:128.78px;top:214.57px" class="cls_025"><span class="cls_025">:</span></div>
<div style="position:absolute;left:140.82px;top:214.57px" class="cls_025"><span class="cls_025">[...]</span></div>
<div style="position:absolute;left:182.20px;top:248.34px" class="cls_018"><span class="cls_018">Listing 3.2: Data schema for each image id</span></div>
<div style="position:absolute;left:72.00px;top:282.19px" class="cls_006"><span class="cls_006">The next step is to create the dataset that will combine the image features and the cap-</span></div>
<div style="position:absolute;left:72.00px;top:298.63px" class="cls_006"><span class="cls_006">tions together which we can then feed into our RNN model. While generating captions</span></div>
<div style="position:absolute;left:72.00px;top:315.06px" class="cls_006"><span class="cls_006">we have to set a limit for the model to stop predicting the next word in the caption.</span></div>
<div style="position:absolute;left:72.00px;top:331.50px" class="cls_006"><span class="cls_006">We do this by appending two special tokens startseq and endseq to the beginning and</span></div>
<div style="position:absolute;left:72.00px;top:347.93px" class="cls_006"><span class="cls_006">end of each caption respectively. These tokens tell the system when to start and when</span></div>
<div style="position:absolute;left:72.00px;top:364.37px" class="cls_006"><span class="cls_006">to stop predicting the sequence of words.  This process is visualized in Figure 3.3 and</span></div>
<div style="position:absolute;left:72.00px;top:380.80px" class="cls_006"><span class="cls_006">outlined as:</span></div>
<div style="position:absolute;left:73.84px;top:402.41px" class="cls_025"><span class="cls_025">I n i t i a l i z e  l i s t s  X1, X2</span></div>
<div style="position:absolute;left:205.14px;top:402.41px" class="cls_025"><span class="cls_025">and  y</span></div>
<div style="position:absolute;left:72.59px;top:415.70px" class="cls_025"><span class="cls_025">For each image</span></div>
<div style="position:absolute;left:94.56px;top:429.00px" class="cls_025"><span class="cls_025">Lookup  the  image  features  learned  by  the VGG model  using  the  image  id</span></div>
<div style="position:absolute;left:94.56px;top:442.29px" class="cls_025"><span class="cls_025">Lookup  the  image  captions   using  the  image  id</span></div>
<div style="position:absolute;left:94.71px;top:455.58px" class="cls_025"><span class="cls_025">For index  i  to  length  of  words  in  the  caption</span></div>
<div style="position:absolute;left:116.47px;top:468.88px" class="cls_025"><span class="cls_025">Append  the  photo  features  in  the  l i s t  X1</span></div>
<div style="position:absolute;left:116.47px;top:482.17px" class="cls_025"><span class="cls_025">Append  the  f i r s t  i  words  in  the  caption  in  the  l i s t  X2</span></div>
<div style="position:absolute;left:116.47px;top:495.46px" class="cls_025"><span class="cls_025">Append</span></div>
<div style="position:absolute;left:155.91px;top:495.46px" class="cls_025"><span class="cls_025">(i+1)the word in  the  caption  in  the  list  y</span></div>
<div style="position:absolute;left:116.93px;top:508.76px" class="cls_025"><span class="cls_025">Repeat  until we append the</span></div>
<div style="position:absolute;left:277.65px;top:508.76px" class="cls_025"><span class="cls_025">e n d s e q</span></div>
<div style="position:absolute;left:338.30px;top:508.76px" class="cls_025"><span class="cls_025">token  in y</span></div>
<div style="position:absolute;left:72.69px;top:522.05px" class="cls_025"><span class="cls_025">Return X1, X2 and y</span></div>
<div style="position:absolute;left:167.44px;top:542.53px" class="cls_018"><span class="cls_018">Listing 3.3: Alogorithm to create training dataset</span></div>
<div style="position:absolute;left:72.00px;top:576.38px" class="cls_006"><span class="cls_006">X1, X2 and y are now our training lists. The model should receive the pair [X1, X2]</span></div>
<div style="position:absolute;left:72.00px;top:592.82px" class="cls_006"><span class="cls_006">and predict y.</span></div>
<div style="position:absolute;left:151.48px;top:592.82px" class="cls_006"><span class="cls_006">[X1, X2] are like the photo features and the corresponding tokens from</span></div>
<div style="position:absolute;left:72.00px;top:609.25px" class="cls_006"><span class="cls_006">the caption combined and y is the next token that the model should learn to predict.The</span></div>
<div style="position:absolute;left:72.00px;top:625.69px" class="cls_006"><span class="cls_006">same algorithm is used to create the corresponding test sequences.</span></div>
<div style="position:absolute;left:72.00px;top:651.52px" class="cls_006"><span class="cls_006">Figure 3.3 represents X2 and y as words for readability and understanding purpose.</span></div>
<div style="position:absolute;left:72.00px;top:667.95px" class="cls_006"><span class="cls_006">However neural networks cannot understand text features.  We have to encode these</span></div>
<div style="position:absolute;left:72.00px;top:684.39px" class="cls_006"><span class="cls_006">numbers in some form. The paper (31) discusses the benefits of using word embeddings.</span></div>
<div style="position:absolute;left:72.00px;top:700.82px" class="cls_006"><span class="cls_006">So we convert the input text into a one-hot vector and then feed them into an Embedding</span></div>
<div style="position:absolute;left:72.00px;top:717.26px" class="cls_006"><span class="cls_006">layer, which is built into Keras (8).  This layer basically converts these sparse one-hot</span></div>
<div style="position:absolute;left:72.00px;top:733.69px" class="cls_006"><span class="cls_006">vectors into a dense vector representation by embedding them into a lower dimension.</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:20448px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background33.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:513.00px;top:47.41px" class="cls_006"><span class="cls_006">17</span></div>
<div style="position:absolute;left:218.81px;top:251.79px" class="cls_018"><span class="cls_018">Figure 3.3: Process to create training data</span></div>
<div style="position:absolute;left:113.98px;top:274.22px" class="cls_006"><span class="cls_006">Dataset statistics</span></div>
<div style="position:absolute;left:375.48px;top:274.22px" class="cls_006"><span class="cls_006">Value</span></div>
<div style="position:absolute;left:113.98px;top:288.17px" class="cls_006"><span class="cls_006">Total images in dataset</span></div>
<div style="position:absolute;left:375.48px;top:288.17px" class="cls_006"><span class="cls_006">8091</span></div>
<div style="position:absolute;left:113.98px;top:302.11px" class="cls_006"><span class="cls_006">Average words per caption</span></div>
<div style="position:absolute;left:375.48px;top:302.11px" class="cls_006"><span class="cls_006">9</span></div>
<div style="position:absolute;left:113.98px;top:316.06px" class="cls_006"><span class="cls_006">Length of training set</span></div>
<div style="position:absolute;left:375.48px;top:316.06px" class="cls_006"><span class="cls_006">6000</span></div>
<div style="position:absolute;left:113.98px;top:330.01px" class="cls_006"><span class="cls_006">Len of testing set</span></div>
<div style="position:absolute;left:375.48px;top:330.01px" class="cls_006"><span class="cls_006">1000</span></div>
<div style="position:absolute;left:113.98px;top:343.96px" class="cls_006"><span class="cls_006">Vocabulary size</span></div>
<div style="position:absolute;left:375.48px;top:343.96px" class="cls_006"><span class="cls_006">8763</span></div>
<div style="position:absolute;left:113.98px;top:357.90px" class="cls_006"><span class="cls_006">Maximum words in a caption</span></div>
<div style="position:absolute;left:375.48px;top:357.90px" class="cls_006"><span class="cls_006">34</span></div>
<div style="position:absolute;left:113.98px;top:371.85px" class="cls_006"><span class="cls_006">Minimum words in a caption</span></div>
<div style="position:absolute;left:375.49px;top:371.85px" class="cls_006"><span class="cls_006">3</span></div>
<div style="position:absolute;left:113.98px;top:385.80px" class="cls_006"><span class="cls_006">Length  of  training  set  after  creating  sequences</span></div>
<div style="position:absolute;left:375.48px;top:385.80px" class="cls_006"><span class="cls_006">306404</span></div>
<div style="position:absolute;left:113.98px;top:399.35px" class="cls_006"><span class="cls_006">(X1,X2,y)</span></div>
<div style="position:absolute;left:113.98px;top:413.30px" class="cls_006"><span class="cls_006">Length of test set after creating sequences (X1,X2,y)</span></div>
<div style="position:absolute;left:375.48px;top:413.30px" class="cls_006"><span class="cls_006">50903</span></div>
<div style="position:absolute;left:250.73px;top:436.47px" class="cls_018"><span class="cls_018">Table 3.1: Dataset statistics</span></div>
<div style="position:absolute;left:108.00px;top:466.41px" class="cls_006"><span class="cls_006">The choice of how many dimensions to use for embedding is arbitrary, and through</span></div>
<div style="position:absolute;left:108.00px;top:482.85px" class="cls_006"><span class="cls_006">experimentation it was found that 300 dimensions gave the best results.</span></div>
<div style="position:absolute;left:108.00px;top:508.67px" class="cls_006"><span class="cls_006">Table 3.1 displays some statistics about the dataset and also after creating the training</span></div>
<div style="position:absolute;left:108.00px;top:525.11px" class="cls_006"><span class="cls_006">sequences X1, X2 and y and the corresponding test sequences.</span></div>
<div style="position:absolute;left:108.00px;top:569.26px" class="cls_005"><span class="cls_005">3.6</span></div>
<div style="position:absolute;left:144.76px;top:569.26px" class="cls_005"><span class="cls_005">Model for Image Captioning</span></div>
<div style="position:absolute;left:108.00px;top:609.33px" class="cls_006"><span class="cls_006">(65) discussed the approach of using CNNs for extracting higher-level features from an</span></div>
<div style="position:absolute;left:108.00px;top:625.76px" class="cls_006"><span class="cls_006">image. Instead of extracting the features from the last fully connected layer, it extracts</span></div>
<div style="position:absolute;left:108.00px;top:642.20px" class="cls_006"><span class="cls_006">the features from the last convolutional layer. As mentioned in subsection 3.4.1, I also</span></div>
<div style="position:absolute;left:108.00px;top:658.63px" class="cls_006"><span class="cls_006">experimented with this approach of successively looking back into the VGG network so</span></div>
<div style="position:absolute;left:108.00px;top:675.07px" class="cls_006"><span class="cls_006">that the RNN network can better understand the features.  The network discussed in</span></div>
<div style="position:absolute;left:108.00px;top:691.50px" class="cls_006"><span class="cls_006">(65) follows an encoder-decoder based architecture. (50) also follows a similar approach</span></div>
<div style="position:absolute;left:108.00px;top:707.94px" class="cls_006"><span class="cls_006">but considers a variety of possible architectures in this space.</span></div>
<div style="position:absolute;left:108.00px;top:733.77px" class="cls_006"><span class="cls_006">As mentioned in (65), the task of automatically generating captions from an image is very</span></div>
<div style="position:absolute;left:108.00px;top:750.20px" class="cls_006"><span class="cls_006">similar to the task of machine translation.  Encoder-decoder based architectures have</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:21300px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background34.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">18</span></div>
<div style="position:absolute;left:255.40px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:257.51px;top:90.95px" class="cls_027"><span class="cls_027">Text</span></div>
<div style="position:absolute;left:293.50px;top:90.95px" class="cls_028"><span class="cls_028">Word embedding layer</span></div>
<div style="position:absolute;left:377.10px;top:138.49px" class="cls_028"><span class="cls_028">Decoder network</span></div>
<div style="position:absolute;left:189.43px;top:142.89px" class="cls_028"><span class="cls_028">CNN Network</span></div>
<div style="position:absolute;left:302.56px;top:142.89px" class="cls_028"><span class="cls_028">LSTM network</span></div>
<div style="position:absolute;left:303.44px;top:180.31px" class="cls_027"><span class="cls_027">Predicted text</span></div>
<div style="position:absolute;left:183.55px;top:196.16px" class="cls_028"><span class="cls_028">Encoder network</span></div>
<div style="position:absolute;left:240.48px;top:196.16px" class="cls_028"><span class="cls_028">Extracted features</span></div>
<div style="position:absolute;left:203.43px;top:216.66px" class="cls_018"><span class="cls_018">Figure 3.4: Modified architecture</span></div>
<div style="position:absolute;left:72.00px;top:246.60px" class="cls_006"><span class="cls_006">achieved excellent results in machine translation (7).  Encoder-decoder based models</span></div>
<div style="position:absolute;left:72.00px;top:263.03px" class="cls_006"><span class="cls_006">generally consist of an encoder RNN, which maps the input sequence to a vector of fixed</span></div>
<div style="position:absolute;left:72.00px;top:279.47px" class="cls_006"><span class="cls_006">length, and the decoder maps this representation to a target sequence. This architecture,</span></div>
<div style="position:absolute;left:72.00px;top:295.90px" class="cls_006"><span class="cls_006">when incorporated with attention-based networks like LSTM (16) achieve state-of-the-</span></div>
<div style="position:absolute;left:72.00px;top:312.34px" class="cls_006"><span class="cls_006">art results in machine translation tasks.  Also, the models are very interpretable and</span></div>
<div style="position:absolute;left:72.00px;top:328.77px" class="cls_006"><span class="cls_006">quite simple compared to other complex models for similar tasks.</span></div>
<div style="position:absolute;left:72.00px;top:354.60px" class="cls_006"><span class="cls_006">The choice of LSTM for the model was both inspired by the attention mechanism in-</span></div>
<div style="position:absolute;left:72.00px;top:371.04px" class="cls_006"><span class="cls_006">built into the network that proved important for the image captioning task (65). Also,</span></div>
<div style="position:absolute;left:72.00px;top:387.47px" class="cls_006"><span class="cls_006">LSTMs have proven to be very successful in machine translation tasks like English to</span></div>
<div style="position:absolute;left:72.00px;top:403.91px" class="cls_006"><span class="cls_006">French translation (48) and the model was found to be consistent even for long input</span></div>
<div style="position:absolute;left:72.00px;top:420.34px" class="cls_006"><span class="cls_006">sequences. This is to be expected as discussed in section 2.2, as the primary advantage</span></div>
<div style="position:absolute;left:72.00px;top:436.78px" class="cls_006"><span class="cls_006">of LSTMs over vanilla RNNs is the addition of the memory sub-network.</span></div>
<div style="position:absolute;left:72.00px;top:462.61px" class="cls_006"><span class="cls_006">Keeping all these factors in mind, we can modify the initial architecture as discussed in</span></div>
<div style="position:absolute;left:72.00px;top:479.04px" class="cls_006"><span class="cls_006">section 2.2 and Figure 2.2 to the one depicted in Figure 3.4:</span></div>
<div style="position:absolute;left:72.00px;top:504.87px" class="cls_006"><span class="cls_006">Now that we have fixed the type of architecture and the type of RNN to be used we can</span></div>
<div style="position:absolute;left:72.00px;top:521.31px" class="cls_006"><span class="cls_006">focus on how our model will understand the features of the image together with that of</span></div>
<div style="position:absolute;left:72.00px;top:537.74px" class="cls_006"><span class="cls_006">the text from the captions. The work of (50) discusses various architectures regarding</span></div>
<div style="position:absolute;left:72.00px;top:554.18px" class="cls_006"><span class="cls_006">how the image features are incorporated in the model. The primary difference between</span></div>
<div style="position:absolute;left:72.00px;top:570.61px" class="cls_006"><span class="cls_006">the architectures is the input to the LSTM network.  These are discussed in details</span></div>
<div style="position:absolute;left:72.00px;top:587.05px" class="cls_006"><span class="cls_006">below:</span></div>
<div style="position:absolute;left:85.33px;top:624.83px" class="cls_006"><span class="cls_006">1. Inject architecture: In this architecture, the image features and embedded vec-</span></div>
<div style="position:absolute;left:99.27px;top:641.27px" class="cls_006"><span class="cls_006">tor of the word sequence are provided as input to the LSTM network. Thus in this</span></div>
<div style="position:absolute;left:99.27px;top:657.70px" class="cls_006"><span class="cls_006">type of architecture, the LSTM learns the visual features as well as the textual</span></div>
<div style="position:absolute;left:99.27px;top:674.14px" class="cls_006"><span class="cls_006">features together (3).</span></div>
<div style="position:absolute;left:85.33px;top:699.54px" class="cls_006"><span class="cls_006">2. Merge architecture: In this, the RNN part of the network only deals with the</span></div>
<div style="position:absolute;left:99.27px;top:715.97px" class="cls_006"><span class="cls_006">text. The input to the LSTM is just the embedded vector of the word sequence.</span></div>
<div style="position:absolute;left:99.27px;top:732.41px" class="cls_006"><span class="cls_006">The features learned by the LSTM is then merged with the image features and</span></div>
<div style="position:absolute;left:99.27px;top:748.84px" class="cls_006"><span class="cls_006">inputted to a fully connected layer.</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:22152px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background35.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:513.00px;top:47.41px" class="cls_006"><span class="cls_006">19</span></div>
<div style="position:absolute;left:113.98px;top:78.96px" class="cls_006"><span class="cls_006">Parameter</span></div>
<div style="position:absolute;left:229.91px;top:78.96px" class="cls_006"><span class="cls_006">Experimental values</span></div>
<div style="position:absolute;left:366.65px;top:78.96px" class="cls_006"><span class="cls_006">Final</span></div>
<div style="position:absolute;left:420.19px;top:78.96px" class="cls_006"><span class="cls_006">Parameter</span></div>
<div style="position:absolute;left:366.64px;top:92.51px" class="cls_006"><span class="cls_006">value</span></div>
<div style="position:absolute;left:420.19px;top:92.51px" class="cls_006"><span class="cls_006">optimized</span></div>
<div style="position:absolute;left:366.64px;top:106.06px" class="cls_006"><span class="cls_006">chosen</span></div>
<div style="position:absolute;left:420.19px;top:106.06px" class="cls_006"><span class="cls_006">(Yes/No)</span></div>
<div style="position:absolute;left:113.98px;top:120.01px" class="cls_006"><span class="cls_006">LSTM layer neurons</span></div>
<div style="position:absolute;left:229.91px;top:120.01px" class="cls_006"><span class="cls_006">128,256,512,1024</span></div>
<div style="position:absolute;left:366.64px;top:120.01px" class="cls_006"><span class="cls_006">512</span></div>
<div style="position:absolute;left:420.20px;top:120.01px" class="cls_006"><span class="cls_006">Yes</span></div>
<div style="position:absolute;left:113.98px;top:133.95px" class="cls_006"><span class="cls_006">Dense layer neurons</span></div>
<div style="position:absolute;left:229.92px;top:133.95px" class="cls_006"><span class="cls_006">256,512,1024</span></div>
<div style="position:absolute;left:366.65px;top:133.95px" class="cls_006"><span class="cls_006">512</span></div>
<div style="position:absolute;left:390.66px;top:133.95px" class="cls_006"><span class="cls_006">and   Yes</span></div>
<div style="position:absolute;left:366.64px;top:147.50px" class="cls_006"><span class="cls_006">256</span></div>
<div style="position:absolute;left:113.98px;top:161.45px" class="cls_006"><span class="cls_006">Dropout rate</span></div>
<div style="position:absolute;left:229.92px;top:161.45px" class="cls_006"><span class="cls_006">0, 0.3, 0.5</span></div>
<div style="position:absolute;left:366.64px;top:161.45px" class="cls_006"><span class="cls_006">0.3</span></div>
<div style="position:absolute;left:420.19px;top:161.45px" class="cls_006"><span class="cls_006">Yes</span></div>
<div style="position:absolute;left:113.98px;top:175.40px" class="cls_006"><span class="cls_006">Embedding    dimen-</span></div>
<div style="position:absolute;left:229.91px;top:175.40px" class="cls_006"><span class="cls_006">100, 200, 300, 500</span></div>
<div style="position:absolute;left:366.64px;top:175.40px" class="cls_006"><span class="cls_006">300</span></div>
<div style="position:absolute;left:420.19px;top:175.40px" class="cls_006"><span class="cls_006">Yes</span></div>
<div style="position:absolute;left:113.98px;top:188.95px" class="cls_006"><span class="cls_006">sion</span></div>
<div style="position:absolute;left:113.98px;top:202.90px" class="cls_006"><span class="cls_006">Optimizer</span></div>
<div style="position:absolute;left:229.92px;top:202.90px" class="cls_006"><span class="cls_006">Stochastic  Gradient  De-   Adam</span></div>
<div style="position:absolute;left:420.19px;top:202.90px" class="cls_006"><span class="cls_006">Yes</span></div>
<div style="position:absolute;left:229.91px;top:216.44px" class="cls_006"><span class="cls_006">scent, Adam, RMSProp</span></div>
<div style="position:absolute;left:113.98px;top:230.39px" class="cls_006"><span class="cls_006">Learning rate</span></div>
<div style="position:absolute;left:229.91px;top:230.39px" class="cls_006"><span class="cls_006">0.01</span></div>
<div style="position:absolute;left:258.37px;top:230.39px" class="cls_006"><span class="cls_006">till</span></div>
<div style="position:absolute;left:280.76px;top:230.39px" class="cls_006"><span class="cls_006">0.5;   randomly</span></div>
<div style="position:absolute;left:366.64px;top:230.39px" class="cls_006"><span class="cls_006">0.03</span></div>
<div style="position:absolute;left:420.19px;top:230.39px" class="cls_006"><span class="cls_006">No</span></div>
<div style="position:absolute;left:443.37px;top:230.39px" class="cls_006"><span class="cls_006">(RAM  con-</span></div>
<div style="position:absolute;left:229.91px;top:243.94px" class="cls_006"><span class="cls_006">sampled</span></div>
<div style="position:absolute;left:420.19px;top:243.94px" class="cls_006"><span class="cls_006">straint)</span></div>
<div style="position:absolute;left:113.98px;top:257.89px" class="cls_006"><span class="cls_006">Number of epochs</span></div>
<div style="position:absolute;left:229.92px;top:257.89px" class="cls_006"><span class="cls_006">10, 20, 50, 75</span></div>
<div style="position:absolute;left:366.64px;top:257.89px" class="cls_006"><span class="cls_006">50</span></div>
<div style="position:absolute;left:420.19px;top:257.89px" class="cls_006"><span class="cls_006">No</span></div>
<div style="position:absolute;left:443.38px;top:257.89px" class="cls_006"><span class="cls_006">(RAM  con-</span></div>
<div style="position:absolute;left:420.19px;top:271.44px" class="cls_006"><span class="cls_006">straint)</span></div>
<div style="position:absolute;left:249.06px;top:294.73px" class="cls_018"><span class="cls_018">Table 3.2: Model parameters</span></div>
<div style="position:absolute;left:108.00px;top:326.60px" class="cls_006"><span class="cls_006">The experiments by (50) prove that the merge model is better in the following ways:</span></div>
<div style="position:absolute;left:121.33px;top:364.38px" class="cls_006"><span class="cls_006">1. Merge model needs almost half the number of neurons than the inject architecture.</span></div>
<div style="position:absolute;left:135.27px;top:380.82px" class="cls_006"><span class="cls_006">This is because the merge model uses RNN for training on only the words and not</span></div>
<div style="position:absolute;left:135.27px;top:397.26px" class="cls_006"><span class="cls_006">on the images. From this, we can conclude that the number of learnable parameters</span></div>
<div style="position:absolute;left:135.27px;top:413.69px" class="cls_006"><span class="cls_006">in the merge architecture will also be much lesser. Thus the training time will be</span></div>
<div style="position:absolute;left:135.27px;top:430.13px" class="cls_006"><span class="cls_006">much shorter. This is an essential factor for our project, as the training time needs</span></div>
<div style="position:absolute;left:135.27px;top:446.56px" class="cls_006"><span class="cls_006">to be taken into account for a model that is deployed on the web.  (50) suggests</span></div>
<div style="position:absolute;left:135.27px;top:463.00px" class="cls_006"><span class="cls_006">that the optimal number of neurons for the LSTM network in the merge model</span></div>
<div style="position:absolute;left:135.27px;top:479.43px" class="cls_006"><span class="cls_006">was 128 vs 512 for the inject model.</span></div>
<div style="position:absolute;left:121.33px;top:504.83px" class="cls_006"><span class="cls_006">2. Inject architecture, because of its need for larger LSTM networks needs much more</span></div>
<div style="position:absolute;left:135.27px;top:521.27px" class="cls_006"><span class="cls_006">memory to train.  For our project, we will be training on 6000 images and each</span></div>
<div style="position:absolute;left:135.27px;top:537.70px" class="cls_006"><span class="cls_006">image will have 5 captions. As shown in table 3.1, the length of the training set is</span></div>
<div style="position:absolute;left:135.27px;top:554.14px" class="cls_006"><span class="cls_006">306404. Google collaboratory is used for training the model which provides 25GB</span></div>
<div style="position:absolute;left:135.27px;top:570.57px" class="cls_006"><span class="cls_006">of RAM and the merge model is beneficial over the inject model for this purpose.</span></div>
<div style="position:absolute;left:135.27px;top:587.01px" class="cls_006"><span class="cls_006">section 1.3 also discusses possible issues regarding RAM and GPU over-utilization,</span></div>
<div style="position:absolute;left:135.27px;top:603.44px" class="cls_006"><span class="cls_006">and these risks have been considered</span></div>
<div style="position:absolute;left:108.00px;top:641.23px" class="cls_006"><span class="cls_006">Based on the architectures defined in the paper, experiments were conducted based on</span></div>
<div style="position:absolute;left:108.00px;top:657.66px" class="cls_006"><span class="cls_006">a number of parameters as shown in table 3.2.  Some parameters like the number of</span></div>
<div style="position:absolute;left:108.00px;top:674.10px" class="cls_006"><span class="cls_006">epochs and the learning rate of the optimizer could not be optimized because of the</span></div>
<div style="position:absolute;left:108.00px;top:690.53px" class="cls_006"><span class="cls_006">RAM constraint of Google Collaboratory.  The final model architecture is shown in</span></div>
<div style="position:absolute;left:108.00px;top:706.97px" class="cls_006"><span class="cls_006">Figure 3.5</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:23004px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background36.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">20</span></div>
<div style="position:absolute;left:255.40px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:224.01px;top:328.40px" class="cls_018"><span class="cls_018">Figure 3.5: Final model</span></div>
<div style="position:absolute;left:72.00px;top:354.90px" class="cls_005"><span class="cls_005">3.7</span></div>
<div style="position:absolute;left:108.76px;top:354.90px" class="cls_005"><span class="cls_005">Generating the captions</span></div>
<div style="position:absolute;left:72.00px;top:394.97px" class="cls_006"><span class="cls_006">Now, we have our final model which has been trained on 6000 images and their cor-</span></div>
<div style="position:absolute;left:72.00px;top:411.40px" class="cls_006"><span class="cls_006">responding captions.  We can generate the captions on the test set (1000 images) and</span></div>
<div style="position:absolute;left:72.00px;top:427.84px" class="cls_006"><span class="cls_006">evaluate the results. To generate the captions we use the following algorithm:</span></div>
<div style="position:absolute;left:72.59px;top:449.45px" class="cls_025"><span class="cls_025">For each image  in  the  test  set</span></div>
<div style="position:absolute;left:94.56px;top:462.74px" class="cls_025"><span class="cls_025">Lookup  the  image  features  learned  by  the VGG model  using  the  image  id</span></div>
<div style="position:absolute;left:95.06px;top:476.03px" class="cls_025"><span class="cls_025">Set  initial  input  sequence  as</span></div>
<div style="position:absolute;left:272.20px;top:476.03px" class="cls_025"><span class="cls_025">s t a r t s e q</span></div>
<div style="position:absolute;left:94.76px;top:489.33px" class="cls_025"><span class="cls_025">While  length  of  generated  sequence  is  less  than or  equal  to</span></div>
<div style="position:absolute;left:426.49px;top:489.33px" class="cls_025"><span class="cls_025">34</span></div>
<div style="position:absolute;left:443.44px;top:489.33px" class="cls_025"><span class="cls_025">( maximum</span></div>
<div style="position:absolute;left:98.61px;top:502.62px" class="cls_025"><span class="cls_025">length  of a caption)</span></div>
<div style="position:absolute;left:116.29px;top:515.91px" class="cls_025"><span class="cls_025">One  hot  encode  the  input  sequence</span></div>
<div style="position:absolute;left:117.03px;top:529.21px" class="cls_025"><span class="cls_025">Pass the image  features  and the  input  sequence  into  the model</span></div>
<div style="position:absolute;left:116.41px;top:542.50px" class="cls_025"><span class="cls_025">The  output  prediction  i s  a  vector  of  length  equal  to  the  vocabulary</span></div>
<div style="position:absolute;left:98.74px;top:555.79px" class="cls_025"><span class="cls_025">size ,  containing  the  probability  distribution  of  the words</span></div>
<div style="position:absolute;left:116.65px;top:569.09px" class="cls_025"><span class="cls_025">Get  the  index  of  the  maximum  element  of  the  vector</span></div>
<div style="position:absolute;left:116.65px;top:582.38px" class="cls_025"><span class="cls_025">Get  the  word  for  the  corresponding  index</span></div>
<div style="position:absolute;left:117.87px;top:595.67px" class="cls_025"><span class="cls_025">If  the word  is</span></div>
<div style="position:absolute;left:211.30px;top:595.67px" class="cls_025"><span class="cls_025">e n d s e q</span></div>
<div style="position:absolute;left:116.89px;top:608.97px" class="cls_025"><span class="cls_025">Break out  of  the  loop</span></div>
<div style="position:absolute;left:116.47px;top:622.26px" class="cls_025"><span class="cls_025">Append  the  word  to  the  input  sequence</span></div>
<div style="position:absolute;left:94.29px;top:635.55px" class="cls_025"><span class="cls_025">The  input  sequence  holds  the  caption  for  that  particular  image</span></div>
<div style="position:absolute;left:196.90px;top:656.03px" class="cls_018"><span class="cls_018">Listing 3.4: Generating the captions</span></div>
<div style="position:absolute;left:72.00px;top:689.88px" class="cls_006"><span class="cls_006">At this stage, we have the captions for all the 1000 images in the test set.</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:23856px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background37.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:513.00px;top:47.41px" class="cls_006"><span class="cls_006">21</span></div>
<div style="position:absolute;left:108.00px;top:77.00px" class="cls_005"><span class="cls_005">3.8</span></div>
<div style="position:absolute;left:144.76px;top:77.00px" class="cls_005"><span class="cls_005">Evaluating the model</span></div>
<div style="position:absolute;left:108.00px;top:117.07px" class="cls_006"><span class="cls_006">(50) mentions a variety of metrics to evaluate the quality of the generated captions.</span></div>
<div style="position:absolute;left:108.00px;top:133.50px" class="cls_006"><span class="cls_006">Initially, the metric Bilingual Evaluation Understudy Score (BLEU) (36) was used for</span></div>
<div style="position:absolute;left:108.00px;top:149.94px" class="cls_006"><span class="cls_006">evaluating the generated captions against the real captions in the test dataset.  As</span></div>
<div style="position:absolute;left:108.00px;top:166.37px" class="cls_006"><span class="cls_006">discussed in (4), BLEU offers some advantages that apply to this project like:</span></div>
<div style="position:absolute;left:121.33px;top:204.16px" class="cls_006"><span class="cls_006">1. Simple to understand</span></div>
<div style="position:absolute;left:121.33px;top:229.56px" class="cls_006"><span class="cls_006">2. Easy to implement - nltk (14) in python has an implementation of BLEU</span></div>
<div style="position:absolute;left:121.33px;top:254.96px" class="cls_006"><span class="cls_006">3. It can be used to compare the performance of our model against the model de-</span></div>
<div style="position:absolute;left:135.27px;top:271.39px" class="cls_006"><span class="cls_006">scribed in (65) and (50)</span></div>
<div style="position:absolute;left:121.33px;top:296.80px" class="cls_006"><span class="cls_006">4. It correlates highly with human evaluation</span></div>
<div style="position:absolute;left:121.33px;top:322.20px" class="cls_006"><span class="cls_006">5. The score is calculated irrespective of the order of the words</span></div>
<div style="position:absolute;left:121.33px;top:347.60px" class="cls_006"><span class="cls_006">6. A cumulative BLEU score can be calculated based on N-gram (62) matches</span></div>
<div style="position:absolute;left:108.00px;top:385.38px" class="cls_006"><span class="cls_006">The metric ranges from 0 to 1; 1 being a perfect match.  We can calculate cumulative</span></div>
<div style="position:absolute;left:108.00px;top:401.82px" class="cls_006"><span class="cls_006">BLEU score for N-grams. For example, while considering the BLEU-2 score we see the</span></div>
<div style="position:absolute;left:108.00px;top:418.25px" class="cls_006"><span class="cls_006">percentage of matching 2-grams in the real and generated caption.  Using this metric,</span></div>
<div style="position:absolute;left:108.00px;top:434.69px" class="cls_006"><span class="cls_006">our final model Figure 3.5 has the following scores:</span></div>
<div style="position:absolute;left:124.36px;top:472.47px" class="cls_006"><span class="cls_006">• BLEU-1: 0.535031</span></div>
<div style="position:absolute;left:124.36px;top:497.87px" class="cls_006"><span class="cls_006">• BLEU-2: 0.282928</span></div>
<div style="position:absolute;left:124.36px;top:523.28px" class="cls_006"><span class="cls_006">• BLEU-3: 0.196293</span></div>
<div style="position:absolute;left:124.36px;top:548.68px" class="cls_006"><span class="cls_006">• BLEU-4: 0.091624</span></div>
<div style="position:absolute;left:108.00px;top:589.21px" class="cls_002"><span class="cls_002">3.8.1</span></div>
<div style="position:absolute;left:149.10px;top:589.21px" class="cls_002"><span class="cls_002">Sample results</span></div>
<div style="position:absolute;left:108.00px;top:623.13px" class="cls_006"><span class="cls_006">Some examples of the captions generated by our model are shown in Figure 3.6. It can</span></div>
<div style="position:absolute;left:108.00px;top:639.56px" class="cls_006"><span class="cls_006">be seen that there are cases when the model gets the caption correct (green), partially</span></div>
<div style="position:absolute;left:108.00px;top:656.00px" class="cls_006"><span class="cls_006">correct (yellow) and completely incorrect (red).</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:24708px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background38.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">22</span></div>
<div style="position:absolute;left:255.40px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:147.15px;top:143.75px" class="cls_029"><span class="cls_029">Caption generated: two children are playing in the snow</span></div>
<div style="position:absolute;left:287.96px;top:143.75px" class="cls_029"><span class="cls_029">Caption generated: man in blue is surfing in the water</span></div>
<div style="position:absolute;left:143.18px;top:220.55px" class="cls_029"><span class="cls_029">Caption generated: man in red shirt is riding bike on the street</span></div>
<div style="position:absolute;left:267.90px;top:239.75px" class="cls_029"><span class="cls_029">Caption generated: person climbing down rocky cliffside with water below</span></div>
<div style="position:absolute;left:158.17px;top:305.04px" class="cls_029"><span class="cls_029">Caption generated: man is climbing rock face</span></div>
<div style="position:absolute;left:301.40px;top:305.04px" class="cls_029"><span class="cls_029">Caption generated: two dogs are racing on the track</span></div>
<div style="position:absolute;left:168.76px;top:324.45px" class="cls_018"><span class="cls_018">Figure 3.6: Few captions generated by the model</span></div>
<div style="position:absolute;left:77.98px;top:346.88px" class="cls_006"><span class="cls_006">Source caption   Generated cap-   BLEU-   BLEU-   BLEU-   BLEU-</span></div>
<div style="position:absolute;left:173.11px;top:360.42px" class="cls_006"><span class="cls_006">tion</span></div>
<div style="position:absolute;left:268.25px;top:360.42px" class="cls_006"><span class="cls_006">1</span></div>
<div style="position:absolute;left:321.80px;top:360.42px" class="cls_006"><span class="cls_006">2</span></div>
<div style="position:absolute;left:375.35px;top:360.42px" class="cls_006"><span class="cls_006">3</span></div>
<div style="position:absolute;left:428.90px;top:360.42px" class="cls_006"><span class="cls_006">4</span></div>
<div style="position:absolute;left:77.98px;top:374.37px" class="cls_006"><span class="cls_006">puppies  running   two dogs playing</span></div>
<div style="position:absolute;left:268.25px;top:374.37px" class="cls_006"><span class="cls_006">0.33</span></div>
<div style="position:absolute;left:321.80px;top:374.37px" class="cls_006"><span class="cls_006">0.2</span></div>
<div style="position:absolute;left:375.35px;top:374.37px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:428.89px;top:374.37px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:77.98px;top:387.92px" class="cls_006"><span class="cls_006">in the ground</span></div>
<div style="position:absolute;left:173.11px;top:387.92px" class="cls_006"><span class="cls_006">in the field</span></div>
<div style="position:absolute;left:77.98px;top:401.87px" class="cls_006"><span class="cls_006">man riding bicy-</span></div>
<div style="position:absolute;left:173.11px;top:401.87px" class="cls_006"><span class="cls_006">man in red shirt</span></div>
<div style="position:absolute;left:268.25px;top:401.87px" class="cls_006"><span class="cls_006">0.5</span></div>
<div style="position:absolute;left:321.80px;top:401.87px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:375.34px;top:401.87px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:428.90px;top:401.87px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:77.98px;top:415.42px" class="cls_006"><span class="cls_006">cle in maroon</span></div>
<div style="position:absolute;left:173.11px;top:415.42px" class="cls_006"><span class="cls_006">riding bike</span></div>
<div style="position:absolute;left:77.98px;top:429.37px" class="cls_006"><span class="cls_006">bird  sits  in  leaf-</span></div>
<div style="position:absolute;left:173.11px;top:429.37px" class="cls_006"><span class="cls_006">little bird sitting</span></div>
<div style="position:absolute;left:268.25px;top:429.37px" class="cls_006"><span class="cls_006">0.17</span></div>
<div style="position:absolute;left:321.80px;top:429.37px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:375.34px;top:429.37px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:428.90px;top:429.37px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:77.98px;top:442.91px" class="cls_006"><span class="cls_006">less tree</span></div>
<div style="position:absolute;left:173.11px;top:442.91px" class="cls_006"><span class="cls_006">on a branch</span></div>
<div style="position:absolute;left:77.98px;top:456.86px" class="cls_006"><span class="cls_006">child   in   black</span></div>
<div style="position:absolute;left:173.11px;top:456.86px" class="cls_006"><span class="cls_006">man in blue is in</span></div>
<div style="position:absolute;left:268.25px;top:456.86px" class="cls_006"><span class="cls_006">0.37</span></div>
<div style="position:absolute;left:321.80px;top:456.86px" class="cls_006"><span class="cls_006">0.22</span></div>
<div style="position:absolute;left:375.35px;top:456.86px" class="cls_006"><span class="cls_006">0.13</span></div>
<div style="position:absolute;left:428.90px;top:456.86px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:77.98px;top:470.41px" class="cls_006"><span class="cls_006">wetsuit   is   in</span></div>
<div style="position:absolute;left:173.11px;top:470.41px" class="cls_006"><span class="cls_006">the water</span></div>
<div style="position:absolute;left:77.98px;top:483.96px" class="cls_006"><span class="cls_006">the   waves   on</span></div>
<div style="position:absolute;left:77.98px;top:497.51px" class="cls_006"><span class="cls_006">surfboard</span></div>
<div style="position:absolute;left:213.06px;top:520.80px" class="cls_018"><span class="cls_018">Table 3.3: Model parameters</span></div>
<div style="position:absolute;left:72.00px;top:551.63px" class="cls_002"><span class="cls_002">3.8.2</span></div>
<div style="position:absolute;left:113.10px;top:551.63px" class="cls_002"><span class="cls_002">BLEU metrics - The problem</span></div>
<div style="position:absolute;left:72.00px;top:585.55px" class="cls_006"><span class="cls_006">Even though the BLEU metrics for our model look promising a closer inspection revealed</span></div>
<div style="position:absolute;left:72.00px;top:601.98px" class="cls_006"><span class="cls_006">a problem.</span></div>
<div style="position:absolute;left:72.00px;top:627.81px" class="cls_006"><span class="cls_006">The BLEU scores between the real and generated captions as shown in table 3.3 are</span></div>
<div style="position:absolute;left:72.00px;top:644.25px" class="cls_006"><span class="cls_006">very low although the captions are quite close to the real ones. The reason why BLEU</span></div>
<div style="position:absolute;left:72.00px;top:660.68px" class="cls_006"><span class="cls_006">score fails to capture this is because it tries to match every word exactly considering</span></div>
<div style="position:absolute;left:72.00px;top:677.12px" class="cls_006"><span class="cls_006">various N-grams. It fails to understand the words in their context and synonyms. Other</span></div>
<div style="position:absolute;left:72.00px;top:693.55px" class="cls_006"><span class="cls_006">metrics discussed in (65) and (50) like CIDEr (54), METEOR (24), and ROUGE-L (26)</span></div>
<div style="position:absolute;left:72.00px;top:709.99px" class="cls_006"><span class="cls_006">also fail to address this issue.</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:25560px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background39.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:513.00px;top:47.41px" class="cls_006"><span class="cls_006">23</span></div>
<div style="position:absolute;left:182.53px;top:270.55px" class="cls_018"><span class="cls_018">Figure 3.7: Working of a basic word embedding model (52)</span></div>
<div style="position:absolute;left:108.00px;top:301.05px" class="cls_002"><span class="cls_002">3.8.3</span></div>
<div style="position:absolute;left:149.10px;top:301.05px" class="cls_002"><span class="cls_002">A Proposed Solution</span></div>
<div style="position:absolute;left:108.00px;top:334.76px" class="cls_006"><span class="cls_006">It is clear from the above discussion that we would require a more representative method</span></div>
<div style="position:absolute;left:108.00px;top:351.19px" class="cls_006"><span class="cls_006">of evaluating the generated captions. One possible solution is to use a word embedding</span></div>
<div style="position:absolute;left:108.00px;top:367.63px" class="cls_006"><span class="cls_006">technique like word2vec (31).  The motivation behind using word embeddings for this</span></div>
<div style="position:absolute;left:108.00px;top:384.06px" class="cls_006"><span class="cls_006">project is:</span></div>
<div style="position:absolute;left:121.33px;top:420.31px" class="cls_006"><span class="cls_006">1. When words are trained by deep learning algorithms, they should be represented</span></div>
<div style="position:absolute;left:135.27px;top:436.74px" class="cls_006"><span class="cls_006">in a manner which can capture the context in some manner</span></div>
<div style="position:absolute;left:121.33px;top:461.70px" class="cls_006"><span class="cls_006">2. Detecting phrases which have close context is essential not only for evaluating the</span></div>
<div style="position:absolute;left:135.27px;top:478.13px" class="cls_006"><span class="cls_006">current model but also for developing the second part of this project as discussed</span></div>
<div style="position:absolute;left:135.27px;top:494.57px" class="cls_006"><span class="cls_006">in section 1.2</span></div>
<div style="position:absolute;left:121.33px;top:519.53px" class="cls_006"><span class="cls_006">3. By embedding the words in vector space we can apply metrics like Cosine similarity</span></div>
<div style="position:absolute;left:135.27px;top:535.96px" class="cls_006"><span class="cls_006">between them which can give us a better idea about the similarity than BLEU</span></div>
<div style="position:absolute;left:135.27px;top:552.40px" class="cls_006"><span class="cls_006">scores or Euclidean similarity. Cosine similarity is insensitive to the relative sizes</span></div>
<div style="position:absolute;left:135.27px;top:568.83px" class="cls_006"><span class="cls_006">of the documents (28).</span></div>
<div style="position:absolute;left:108.00px;top:619.57px" class="cls_002"><span class="cls_002">3.8.4</span></div>
<div style="position:absolute;left:149.10px;top:619.57px" class="cls_002"><span class="cls_002">Working of a word embedding model - word2vec</span></div>
<div style="position:absolute;left:108.00px;top:653.28px" class="cls_006"><span class="cls_006">For this project, we use the famous word embedding technique - word2vec (31). Word2vec</span></div>
<div style="position:absolute;left:108.00px;top:669.71px" class="cls_006"><span class="cls_006">is a shallow neural network that learns the association between the words (52).  From</span></div>
<div style="position:absolute;left:108.00px;top:686.15px" class="cls_006"><span class="cls_006">Figure 3.7, we can see that the model takes as input the one-hot encoded form of the</span></div>
<div style="position:absolute;left:108.00px;top:702.58px" class="cls_006"><span class="cls_006">words. The size of this input vector is the same as our vocabulary size. Then there is a</span></div>
<div style="position:absolute;left:108.00px;top:719.02px" class="cls_006"><span class="cls_006">hidden layer, the size of which determines the vector size in which all the words will be</span></div>
<div style="position:absolute;left:108.00px;top:735.45px" class="cls_006"><span class="cls_006">embedded. The final output layer again is the same size as the vocabulary, but usually</span></div>
<div style="position:absolute;left:108.00px;top:751.89px" class="cls_006"><span class="cls_006">has a softmax (63) activation function, which outputs a probability distribution of all</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:26412px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background40.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">24</span></div>
<div style="position:absolute;left:255.40px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:72.00px;top:80.43px" class="cls_006"><span class="cls_006">possible words in the vocabulary. By training this network on a large corpus of words,</span></div>
<div style="position:absolute;left:72.00px;top:96.87px" class="cls_006"><span class="cls_006">it gradually learns to maximize the probability of words which are in close proximity to</span></div>
<div style="position:absolute;left:72.00px;top:113.31px" class="cls_006"><span class="cls_006">it. An important assumption that this model makes is that words in close proximity are</span></div>
<div style="position:absolute;left:72.00px;top:129.74px" class="cls_006"><span class="cls_006">often similar or contextual. Finally, after the network has learned the associations, we</span></div>
<div style="position:absolute;left:72.00px;top:146.18px" class="cls_006"><span class="cls_006">can extract the hidden (embedding) dimension, and words which have similar context</span></div>
<div style="position:absolute;left:72.00px;top:162.61px" class="cls_006"><span class="cls_006">will represent similar vector space embeddings.</span></div>
<div style="position:absolute;left:72.00px;top:203.14px" class="cls_002"><span class="cls_002">3.8.5</span></div>
<div style="position:absolute;left:113.10px;top:203.14px" class="cls_002"><span class="cls_002">Word Movers Distance</span></div>
<div style="position:absolute;left:72.00px;top:237.06px" class="cls_006"><span class="cls_006">Now that we have a brief idea about how word2vec works, we can go back to our</span></div>
<div style="position:absolute;left:72.00px;top:253.50px" class="cls_006"><span class="cls_006">problem of understanding if the generated captions are similar to the original captions</span></div>
<div style="position:absolute;left:72.00px;top:269.93px" class="cls_006"><span class="cls_006">or not. Basically, we have to compare two sentences, and in the field of natural language</span></div>
<div style="position:absolute;left:72.00px;top:286.37px" class="cls_006"><span class="cls_006">processing, sentences are often referred to as documents. So we need a metric that can</span></div>
<div style="position:absolute;left:72.00px;top:302.80px" class="cls_006"><span class="cls_006">give us a similarity score based on the distances between the documents, incorporating</span></div>
<div style="position:absolute;left:72.00px;top:319.24px" class="cls_006"><span class="cls_006">the features of word2vec.</span></div>
<div style="position:absolute;left:72.00px;top:345.07px" class="cls_006"><span class="cls_006">Word Movers Distance (WMD) (23) is such a metric. WMD utilized word2vec embed-</span></div>
<div style="position:absolute;left:72.00px;top:361.50px" class="cls_006"><span class="cls_006">dings. The words and hence, the documents can be represented in vector space. WMD</span></div>
<div style="position:absolute;left:72.00px;top:377.94px" class="cls_006"><span class="cls_006">computes the cumulative distance between two documents in vector space as the mini-</span></div>
<div style="position:absolute;left:72.00px;top:394.37px" class="cls_006"><span class="cls_006">mum distance the cloud of points for one document will need to travel to merge with the</span></div>
<div style="position:absolute;left:72.00px;top:410.81px" class="cls_006"><span class="cls_006">cloud of points for the above document. Because it uses word2vec embeddings, similar</span></div>
<div style="position:absolute;left:72.00px;top:427.24px" class="cls_006"><span class="cls_006">words will be close together in vector space, and, as a result, similar documents will have</span></div>
<div style="position:absolute;left:72.00px;top:443.68px" class="cls_006"><span class="cls_006">less WMD between them. The motivation for using WMD for our use case of identifying</span></div>
<div style="position:absolute;left:72.00px;top:460.11px" class="cls_006"><span class="cls_006">the similarity between captions are:</span></div>
<div style="position:absolute;left:85.33px;top:497.90px" class="cls_006"><span class="cls_006">1. Can leverage pre-trained embeddings of the word2vec model</span></div>
<div style="position:absolute;left:85.33px;top:523.30px" class="cls_006"><span class="cls_006">2. Can capture the semantic similarity between documents that other metrics like</span></div>
<div style="position:absolute;left:99.27px;top:539.73px" class="cls_006"><span class="cls_006">TF-IDF (23; 37) fail to do</span></div>
<div style="position:absolute;left:85.33px;top:565.14px" class="cls_006"><span class="cls_006">3. The algorithm is hyperparameter free - so a lot of time and memory for hyperpa-</span></div>
<div style="position:absolute;left:99.27px;top:581.57px" class="cls_006"><span class="cls_006">rameter optimization can be saved. As discussed in section 1.3, RAM management</span></div>
<div style="position:absolute;left:99.27px;top:598.01px" class="cls_006"><span class="cls_006">is a crucial factor</span></div>
<div style="position:absolute;left:85.33px;top:623.41px" class="cls_006"><span class="cls_006">4. The algorithm can be easily implemented using the gensim package in python (39)</span></div>
<div style="position:absolute;left:72.00px;top:663.94px" class="cls_002"><span class="cls_002">3.8.6</span></div>
<div style="position:absolute;left:113.10px;top:663.94px" class="cls_002"><span class="cls_002">Implementing the proposed solution</span></div>
<div style="position:absolute;left:72.00px;top:697.86px" class="cls_006"><span class="cls_006">To implement WMD, we use pre-trained word2vec embeddings from the Google News</span></div>
<div style="position:absolute;left:72.00px;top:714.29px" class="cls_006"><span class="cls_006">dataset (12).  These are a set of pre-trained vectors, each representing a unique word,</span></div>
<div style="position:absolute;left:72.00px;top:730.73px" class="cls_006"><span class="cls_006">which have been extracted from Google news data. There are about 100 billion unique</span></div>
<div style="position:absolute;left:72.00px;top:747.16px" class="cls_006"><span class="cls_006">words, and the vector (embedding) size is 300. It is a safe assumption to consider that</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:27264px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background41.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:513.00px;top:47.41px" class="cls_006"><span class="cls_006">25</span></div>
<div style="position:absolute;left:113.98px;top:78.96px" class="cls_006"><span class="cls_006">Source caption   Generated cap-   WMD    BLEU-   BLEU-   BLEU-   BLEU-</span></div>
<div style="position:absolute;left:209.11px;top:92.51px" class="cls_006"><span class="cls_006">tion</span></div>
<div style="position:absolute;left:357.80px;top:92.51px" class="cls_006"><span class="cls_006">1</span></div>
<div style="position:absolute;left:411.35px;top:92.51px" class="cls_006"><span class="cls_006">2</span></div>
<div style="position:absolute;left:464.90px;top:92.51px" class="cls_006"><span class="cls_006">3</span></div>
<div style="position:absolute;left:518.45px;top:92.51px" class="cls_006"><span class="cls_006">4</span></div>
<div style="position:absolute;left:113.98px;top:106.46px" class="cls_006"><span class="cls_006">puppies  running   two dogs playing</span></div>
<div style="position:absolute;left:304.25px;top:106.46px" class="cls_006"><span class="cls_006">0.98</span></div>
<div style="position:absolute;left:357.80px;top:106.46px" class="cls_006"><span class="cls_006">0.33</span></div>
<div style="position:absolute;left:411.35px;top:106.46px" class="cls_006"><span class="cls_006">0.2</span></div>
<div style="position:absolute;left:464.90px;top:106.46px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:518.44px;top:106.46px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:113.98px;top:120.01px" class="cls_006"><span class="cls_006">in the ground</span></div>
<div style="position:absolute;left:209.11px;top:120.01px" class="cls_006"><span class="cls_006">in the field</span></div>
<div style="position:absolute;left:113.98px;top:133.95px" class="cls_006"><span class="cls_006">man riding bicy-</span></div>
<div style="position:absolute;left:209.11px;top:133.95px" class="cls_006"><span class="cls_006">man in red shirt</span></div>
<div style="position:absolute;left:304.25px;top:133.95px" class="cls_006"><span class="cls_006">1.04</span></div>
<div style="position:absolute;left:357.80px;top:133.95px" class="cls_006"><span class="cls_006">0.5</span></div>
<div style="position:absolute;left:411.35px;top:133.95px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:464.89px;top:133.95px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:518.45px;top:133.95px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:113.98px;top:147.50px" class="cls_006"><span class="cls_006">cle in maroon</span></div>
<div style="position:absolute;left:209.11px;top:147.50px" class="cls_006"><span class="cls_006">riding bike</span></div>
<div style="position:absolute;left:113.98px;top:161.45px" class="cls_006"><span class="cls_006">bird  sits in  leaf-</span></div>
<div style="position:absolute;left:209.11px;top:161.45px" class="cls_006"><span class="cls_006">little bird sitting</span></div>
<div style="position:absolute;left:304.25px;top:161.45px" class="cls_006"><span class="cls_006">1.06</span></div>
<div style="position:absolute;left:357.80px;top:161.45px" class="cls_006"><span class="cls_006">0.17</span></div>
<div style="position:absolute;left:411.35px;top:161.45px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:464.89px;top:161.45px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:518.45px;top:161.45px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:113.98px;top:175.00px" class="cls_006"><span class="cls_006">less tree</span></div>
<div style="position:absolute;left:209.11px;top:175.00px" class="cls_006"><span class="cls_006">on a branch</span></div>
<div style="position:absolute;left:113.98px;top:188.95px" class="cls_006"><span class="cls_006">child   in   black</span></div>
<div style="position:absolute;left:209.11px;top:188.95px" class="cls_006"><span class="cls_006">man in blue is in</span></div>
<div style="position:absolute;left:304.25px;top:188.95px" class="cls_006"><span class="cls_006">0.88</span></div>
<div style="position:absolute;left:357.80px;top:188.95px" class="cls_006"><span class="cls_006">0.37</span></div>
<div style="position:absolute;left:411.35px;top:188.95px" class="cls_006"><span class="cls_006">0.22</span></div>
<div style="position:absolute;left:464.90px;top:188.95px" class="cls_006"><span class="cls_006">0.13</span></div>
<div style="position:absolute;left:518.44px;top:188.95px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:113.98px;top:202.50px" class="cls_006"><span class="cls_006">wetsuit   is   in</span></div>
<div style="position:absolute;left:209.11px;top:202.50px" class="cls_006"><span class="cls_006">the water</span></div>
<div style="position:absolute;left:113.98px;top:216.05px" class="cls_006"><span class="cls_006">the   waves   on</span></div>
<div style="position:absolute;left:113.98px;top:229.60px" class="cls_006"><span class="cls_006">surfboard</span></div>
<div style="position:absolute;left:191.41px;top:252.88px" class="cls_018"><span class="cls_018">Table 3.4: WMD and BLEU scores for sample captions</span></div>
<div style="position:absolute;left:108.00px;top:284.76px" class="cls_006"><span class="cls_006">all of the words in our dataset is a part of this massive 100 billion words dataset, so we</span></div>
<div style="position:absolute;left:108.00px;top:301.19px" class="cls_006"><span class="cls_006">do not have to train our own word2vec model for this. Another important catch while</span></div>
<div style="position:absolute;left:108.00px;top:317.63px" class="cls_006"><span class="cls_006">computing WMD is it considers Euclidean distance (60), and not Cosine Similarity (28).</span></div>
<div style="position:absolute;left:108.00px;top:334.06px" class="cls_006"><span class="cls_006">So if two documents have different lengths the Euclidean distance will be large, so we</span></div>
<div style="position:absolute;left:108.00px;top:350.50px" class="cls_006"><span class="cls_006">normalize the embedded vectors so that they have the same lengths.</span></div>
<div style="position:absolute;left:108.00px;top:376.33px" class="cls_006"><span class="cls_006">The implementation of the WMD distance on our test dataset is as follows:</span></div>
<div style="position:absolute;left:108.90px;top:397.94px" class="cls_025"><span class="cls_025">Normalize  the word2vec embedded vectors</span></div>
<div style="position:absolute;left:108.59px;top:411.23px" class="cls_025"><span class="cls_025">For each  real  and generated  caption  in  the  test  dataset</span></div>
<div style="position:absolute;left:130.89px;top:424.52px" class="cls_025"><span class="cls_025">Convert  all  words to  lowercase</span></div>
<div style="position:absolute;left:130.27px;top:437.82px" class="cls_025"><span class="cls_025">Remove  a l l  stopwords</span></div>
<div style="position:absolute;left:130.35px;top:451.11px" class="cls_025"><span class="cls_025">Compute  the WMD s i m i l a r i t y</span></div>
<div style="position:absolute;left:131.22px;top:464.40px" class="cls_025"><span class="cls_025">Store  this  similarity  score  in a  list</span></div>
<div style="position:absolute;left:108.23px;top:477.70px" class="cls_025"><span class="cls_025">Compute  the  mean  of  the  s i m i l a r i t y  scores</span></div>
<div style="position:absolute;left:236.23px;top:498.18px" class="cls_018"><span class="cls_018">Listing 3.5: WMD implementation</span></div>
<div style="position:absolute;left:108.00px;top:532.03px" class="cls_006"><span class="cls_006">The average WMD Distance score is 1.17, which suggests that the generated captions</span></div>
<div style="position:absolute;left:108.00px;top:548.46px" class="cls_006"><span class="cls_006">are quite close to the original ones.  Through experimentation, we can see that if the</span></div>
<div style="position:absolute;left:108.00px;top:564.90px" class="cls_006"><span class="cls_006">captions are not similar the WMD score is generally over 1.25, and often over 1.5.</span></div>
<div style="position:absolute;left:108.00px;top:605.43px" class="cls_002"><span class="cls_002">3.8.7</span></div>
<div style="position:absolute;left:149.10px;top:605.43px" class="cls_002"><span class="cls_002">Results</span></div>
<div style="position:absolute;left:108.00px;top:639.35px" class="cls_006"><span class="cls_006">In table 3.3, we had observed few samples of real and generated captions for which</span></div>
<div style="position:absolute;left:108.00px;top:655.78px" class="cls_006"><span class="cls_006">the N-gram BLEU scores were very less, even though the captions were quite similar.</span></div>
<div style="position:absolute;left:108.00px;top:672.22px" class="cls_006"><span class="cls_006">We compute the WMD Distance metric between these captions using pre-trained Google</span></div>
<div style="position:absolute;left:108.00px;top:688.65px" class="cls_006"><span class="cls_006">News word embeddings (12), and the results are summarized in table 3.4. It is clear from</span></div>
<div style="position:absolute;left:108.00px;top:705.09px" class="cls_006"><span class="cls_006">the above results that applying WMD metric to the captions generate better results.</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:28116px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background42.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">26</span></div>
<div style="position:absolute;left:255.40px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:346.88px;top:82.71px" class="cls_030"><span class="cls_030">Prediction</span></div>
<div style="position:absolute;left:341.75px;top:117.55px" class="cls_030"><span class="cls_030">Is new words</span></div>
<div style="position:absolute;left:402.44px;top:116.39px" class="cls_030"><span class="cls_030">YES</span></div>
<div style="position:absolute;left:341.65px;top:125.68px" class="cls_030"><span class="cls_030">percentage ></span></div>
<div style="position:absolute;left:347.75px;top:133.81px" class="cls_030"><span class="cls_030">threshold</span></div>
<div style="position:absolute;left:270.22px;top:143.10px" class="cls_030"><span class="cls_030">unique words list</span></div>
<div style="position:absolute;left:371.96px;top:160.52px" class="cls_030"><span class="cls_030">NO</span></div>
<div style="position:absolute;left:138.53px;top:189.56px" class="cls_030"><span class="cls_030">Training</span></div>
<div style="position:absolute;left:210.02px;top:189.56px" class="cls_030"><span class="cls_030">Embedding matrix</span></div>
<div style="position:absolute;left:333.32px;top:189.56px" class="cls_030"><span class="cls_030">Lookup embedding</span></div>
<div style="position:absolute;left:177.70px;top:223.24px" class="cls_030"><span class="cls_030">pre-computed</span></div>
<div style="position:absolute;left:171.51px;top:231.37px" class="cls_030"><span class="cls_030">embedding matrix</span></div>
<div style="position:absolute;left:257.91px;top:265.05px" class="cls_030"><span class="cls_030">Re-train the embedding layer</span></div>
<div style="position:absolute;left:186.26px;top:288.44px" class="cls_018"><span class="cls_018">Figure 3.8: Optimizing word embeddings</span></div>
<div style="position:absolute;left:72.00px;top:316.87px" class="cls_005"><span class="cls_005">3.9</span></div>
<div style="position:absolute;left:108.76px;top:316.87px" class="cls_005"><span class="cls_005">Some optimizations for deploying</span></div>
<div style="position:absolute;left:72.00px;top:356.94px" class="cls_006"><span class="cls_006">As discussed in subsection 3.4.1, we optimized the CNN part of the model by pre-</span></div>
<div style="position:absolute;left:72.00px;top:373.38px" class="cls_006"><span class="cls_006">computing and storing the image features.  When we deploy the model to the web,</span></div>
<div style="position:absolute;left:72.00px;top:389.81px" class="cls_006"><span class="cls_006">we need to consider the running time of the model.  For that, we should store all</span></div>
<div style="position:absolute;left:72.00px;top:406.25px" class="cls_006"><span class="cls_006">the variables, data structures and weights of the models which have been trained and</span></div>
<div style="position:absolute;left:72.00px;top:422.68px" class="cls_006"><span class="cls_006">optimized on disk so that while predicting, the application can just read from these files</span></div>
<div style="position:absolute;left:72.00px;top:439.12px" class="cls_006"><span class="cls_006">and run the data through the model to get the predictions.  We should not have to</span></div>
<div style="position:absolute;left:72.00px;top:455.55px" class="cls_006"><span class="cls_006">re-create the datasets or re-train the model every time. Most importantly, the creation</span></div>
<div style="position:absolute;left:72.00px;top:471.99px" class="cls_006"><span class="cls_006">of the training dataset, as discussed in section 3.5 takes a long time (30 mins on 25GB</span></div>
<div style="position:absolute;left:72.00px;top:488.42px" class="cls_006"><span class="cls_006">RAM) and should be stored on disk.</span></div>
<div style="position:absolute;left:72.00px;top:528.95px" class="cls_002"><span class="cls_002">3.9.1</span></div>
<div style="position:absolute;left:113.10px;top:528.95px" class="cls_002"><span class="cls_002">Optimizing word embeddings</span></div>
<div style="position:absolute;left:72.00px;top:562.87px" class="cls_006"><span class="cls_006">As discussed in section 3.6, our model has an embedding layer of fixed dimensionality</span></div>
<div style="position:absolute;left:72.00px;top:579.31px" class="cls_006"><span class="cls_006">which learns dense vector space embeddings of words. Once trained on the whole dataset,</span></div>
<div style="position:absolute;left:72.00px;top:595.74px" class="cls_006"><span class="cls_006">8763 words are learned by the model. This is a significant number and it does not make</span></div>
<div style="position:absolute;left:72.00px;top:612.18px" class="cls_006"><span class="cls_006">sense to precompute the embeddings every time. So, the embedding matrix once learned</span></div>
<div style="position:absolute;left:72.00px;top:628.62px" class="cls_006"><span class="cls_006">can be stored as a numpy array of dimensionality (vocabulary size, embedding size)</span></div>
<div style="position:absolute;left:72.00px;top:645.05px" class="cls_006"><span class="cls_006">and then be stored on disk as a pickle, which the model can refer to while training. This</span></div>
<div style="position:absolute;left:72.00px;top:661.49px" class="cls_006"><span class="cls_006">significantly reduced the training time of the model (by almost 10 minutes on a batch</span></div>
<div style="position:absolute;left:72.00px;top:677.92px" class="cls_006"><span class="cls_006">of 1000 new images for 20 epochs).  Also, to make this approach work, we keep a list</span></div>
<div style="position:absolute;left:72.00px;top:694.36px" class="cls_006"><span class="cls_006">of words in our vocabulary.  If newer data comes in and the percentage of words that</span></div>
<div style="position:absolute;left:72.00px;top:710.79px" class="cls_006"><span class="cls_006">are out-of-vocabulary is beyond a particular threshold, we re-train the embedding. This</span></div>
<div style="position:absolute;left:72.00px;top:727.23px" class="cls_006"><span class="cls_006">process is summarized in Figure 3.8.</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:28968px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background43.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:513.00px;top:47.41px" class="cls_006"><span class="cls_006">27</span></div>
<div style="position:absolute;left:108.00px;top:79.39px" class="cls_002"><span class="cls_002">3.9.2</span></div>
<div style="position:absolute;left:149.10px;top:79.39px" class="cls_002"><span class="cls_002">Using model checkpoints</span></div>
<div style="position:absolute;left:108.00px;top:113.09px" class="cls_006"><span class="cls_006">Also, because the model will be deployed on the cloud, we may need to re-train the</span></div>
<div style="position:absolute;left:108.00px;top:129.53px" class="cls_006"><span class="cls_006">model as we receive new data. So we should always ensure that we are using the best</span></div>
<div style="position:absolute;left:108.00px;top:145.96px" class="cls_006"><span class="cls_006">model for the predictions. An easy way to do this is to monitor the loss via callbacks.</span></div>
<div style="position:absolute;left:108.00px;top:162.40px" class="cls_006"><span class="cls_006">Callbacks allow us to monitor important statistics of a model like loss and accuracy</span></div>
<div style="position:absolute;left:108.00px;top:178.83px" class="cls_006"><span class="cls_006">while training [29]. Using callbacks we can create checkpoints of the model The way we</span></div>
<div style="position:absolute;left:108.00px;top:195.27px" class="cls_006"><span class="cls_006">do this is:</span></div>
<div style="position:absolute;left:108.95px;top:216.42px" class="cls_025"><span class="cls_025">Set  the  format  of  the model  f i l e  as :  model−ep{epoch :03d}−loss { loss :.3 f}−</span></div>
<div style="position:absolute;left:129.53px;top:229.71px" class="cls_025"><span class="cls_025">val loss{val loss :.3 f}.h5</span></div>
<div style="position:absolute;left:108.93px;top:243.00px" class="cls_025"><span class="cls_025">Create a new ModelCheckpoint  instance</span></div>
<div style="position:absolute;left:108.95px;top:256.29px" class="cls_025"><span class="cls_025">Set  the  appropriate  parameters:</span></div>
<div style="position:absolute;left:130.99px;top:269.59px" class="cls_025"><span class="cls_025">monitor =</span></div>
<div style="position:absolute;left:198.11px;top:269.59px" class="cls_025"><span class="cls_025">v a l  l o s s</span></div>
<div style="position:absolute;left:131.45px;top:282.88px" class="cls_025"><span class="cls_025">save best only</span></div>
<div style="position:absolute;left:212.24px;top:282.88px" class="cls_025"><span class="cls_025">= True :  Only  save  the  model  with  the  lowest  validation</span></div>
<div style="position:absolute;left:129.19px;top:296.17px" class="cls_025"><span class="cls_025">score</span></div>
<div style="position:absolute;left:130.24px;top:309.47px" class="cls_025"><span class="cls_025">mode =    m i n</span></div>
<div style="position:absolute;left:220.07px;top:309.47px" class="cls_025"><span class="cls_025">:  overwrite  the  current  file  if  the model has the</span></div>
<div style="position:absolute;left:127.96px;top:322.76px" class="cls_025"><span class="cls_025">minimum  validation  l o s s</span></div>
<div style="position:absolute;left:108.95px;top:336.05px" class="cls_025"><span class="cls_025">Set  the  checkpoint  as a callback  while  training  the model</span></div>
<div style="position:absolute;left:211.39px;top:356.31px" class="cls_018"><span class="cls_018">Listing 3.6: Model checkpoint implementation</span></div>
<div style="position:absolute;left:108.00px;top:389.24px" class="cls_006"><span class="cls_006">Thus, only the model with the lowest validation loss will be stored on disk. If the current</span></div>
<div style="position:absolute;left:108.00px;top:405.68px" class="cls_006"><span class="cls_006">model has a lower validation loss, the model on disk will be replaced by the current one.</span></div>
<div style="position:absolute;left:108.00px;top:445.78px" class="cls_002"><span class="cls_002">3.9.3</span></div>
<div style="position:absolute;left:149.10px;top:445.78px" class="cls_002"><span class="cls_002">File management</span></div>
<div style="position:absolute;left:108.00px;top:479.48px" class="cls_006"><span class="cls_006">Table 3.5 shows a possible configuration of the files that have to be maintained on disk,</span></div>
<div style="position:absolute;left:108.00px;top:495.91px" class="cls_006"><span class="cls_006">their types and refresh rates. Generally files of sizes less than 1GB can be maintained</span></div>
<div style="position:absolute;left:108.00px;top:512.35px" class="cls_006"><span class="cls_006">as pickle files. Files larger than that have to be stored in HDF (HD5) format.</span></div>
<div style="position:absolute;left:108.00px;top:556.06px" class="cls_005"><span class="cls_005">3.10</span></div>
<div style="position:absolute;left:152.83px;top:556.06px" class="cls_005"><span class="cls_005">Extensions</span></div>
<div style="position:absolute;left:108.00px;top:595.91px" class="cls_006"><span class="cls_006">Now that we have developed a system for automatically captioning images and we have</span></div>
<div style="position:absolute;left:108.00px;top:612.35px" class="cls_006"><span class="cls_006">also applied metrics to test the quality of the generated captions keeping in mind the</span></div>
<div style="position:absolute;left:108.00px;top:628.79px" class="cls_006"><span class="cls_006">complexities of natural language.  Additionally, we explore some additional extended</span></div>
<div style="position:absolute;left:108.00px;top:645.22px" class="cls_006"><span class="cls_006">features that we can provide users to improve their experiences on the web.</span></div>
<div style="position:absolute;left:108.00px;top:685.32px" class="cls_002"><span class="cls_002">3.10.1</span></div>
<div style="position:absolute;left:155.82px;top:685.32px" class="cls_002"><span class="cls_002">Object detection</span></div>
<div style="position:absolute;left:108.00px;top:719.02px" class="cls_006"><span class="cls_006">Image captions provide a visually impaired user with an overview of what the image is</span></div>
<div style="position:absolute;left:108.00px;top:735.45px" class="cls_006"><span class="cls_006">trying to convey. However, what we noticed was that the captions generated were often</span></div>
<div style="position:absolute;left:108.00px;top:751.89px" class="cls_006"><span class="cls_006">vague, like children playing in the park instead of three children playing with football</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:29820px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background44.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">28</span></div>
<div style="position:absolute;left:255.40px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:77.98px;top:78.96px" class="cls_006"><span class="cls_006">Filename</span></div>
<div style="position:absolute;left:173.11px;top:78.96px" class="cls_006"><span class="cls_006">File type</span></div>
<div style="position:absolute;left:247.46px;top:78.96px" class="cls_006"><span class="cls_006">File description</span></div>
<div style="position:absolute;left:384.19px;top:78.96px" class="cls_006"><span class="cls_006">Refresh</span></div>
<div style="position:absolute;left:77.98px;top:92.91px" class="cls_006"><span class="cls_006">X1</span></div>
<div style="position:absolute;left:173.12px;top:92.91px" class="cls_006"><span class="cls_006">Hierarchical</span></div>
<div style="position:absolute;left:247.45px;top:92.91px" class="cls_006"><span class="cls_006">Training data - photo fea-</span></div>
<div style="position:absolute;left:384.18px;top:92.91px" class="cls_006"><span class="cls_006">When  the  number  of  train-</span></div>
<div style="position:absolute;left:173.11px;top:106.46px" class="cls_006"><span class="cls_006">Data Format</span></div>
<div style="position:absolute;left:247.45px;top:106.46px" class="cls_006"><span class="cls_006">tures</span></div>
<div style="position:absolute;left:384.18px;top:106.46px" class="cls_006"><span class="cls_006">ing  data  points  increase  sig-</span></div>
<div style="position:absolute;left:173.11px;top:120.01px" class="cls_006"><span class="cls_006">(HD5)</span></div>
<div style="position:absolute;left:384.18px;top:120.01px" class="cls_006"><span class="cls_006">nificantly and the newer data</span></div>
<div style="position:absolute;left:384.18px;top:133.56px" class="cls_006"><span class="cls_006">points  improve  the  perfor-</span></div>
<div style="position:absolute;left:384.18px;top:147.10px" class="cls_006"><span class="cls_006">mance of the model</span></div>
<div style="position:absolute;left:77.98px;top:161.05px" class="cls_006"><span class="cls_006">X2</span></div>
<div style="position:absolute;left:173.12px;top:161.05px" class="cls_006"><span class="cls_006">Hierarchical</span></div>
<div style="position:absolute;left:247.45px;top:161.05px" class="cls_006"><span class="cls_006">Training  data</span></div>
<div style="position:absolute;left:324.82px;top:161.05px" class="cls_006"><span class="cls_006">- the  se-</span></div>
<div style="position:absolute;left:384.18px;top:161.05px" class="cls_006"><span class="cls_006">When  the  number  of  train-</span></div>
<div style="position:absolute;left:173.11px;top:174.60px" class="cls_006"><span class="cls_006">Data Format</span></div>
<div style="position:absolute;left:247.45px;top:174.60px" class="cls_006"><span class="cls_006">quence of captions to be</span></div>
<div style="position:absolute;left:384.18px;top:174.60px" class="cls_006"><span class="cls_006">ing  data  points  increase  sig-</span></div>
<div style="position:absolute;left:173.11px;top:188.15px" class="cls_006"><span class="cls_006">(HD5)</span></div>
<div style="position:absolute;left:247.45px;top:188.15px" class="cls_006"><span class="cls_006">fed into the model</span></div>
<div style="position:absolute;left:384.18px;top:188.15px" class="cls_006"><span class="cls_006">nificantly and the newer data</span></div>
<div style="position:absolute;left:384.18px;top:201.70px" class="cls_006"><span class="cls_006">points  improve  the  perfor-</span></div>
<div style="position:absolute;left:384.18px;top:215.25px" class="cls_006"><span class="cls_006">mance of the model</span></div>
<div style="position:absolute;left:77.98px;top:229.20px" class="cls_006"><span class="cls_006">y</span></div>
<div style="position:absolute;left:173.11px;top:229.20px" class="cls_006"><span class="cls_006">Hierarchical</span></div>
<div style="position:absolute;left:247.45px;top:229.20px" class="cls_006"><span class="cls_006">Training data - the token</span></div>
<div style="position:absolute;left:384.18px;top:229.20px" class="cls_006"><span class="cls_006">When  the  number  of  train-</span></div>
<div style="position:absolute;left:173.11px;top:242.75px" class="cls_006"><span class="cls_006">Data    For-</span></div>
<div style="position:absolute;left:247.45px;top:242.75px" class="cls_006"><span class="cls_006">that the model should pre-</span></div>
<div style="position:absolute;left:384.18px;top:242.75px" class="cls_006"><span class="cls_006">ing  data  points  increase  sig-</span></div>
<div style="position:absolute;left:173.11px;top:256.30px" class="cls_006"><span class="cls_006">mat</span></div>
<div style="position:absolute;left:205.05px;top:256.30px" class="cls_006"><span class="cls_006">(HD5)</span></div>
<div style="position:absolute;left:247.45px;top:256.30px" class="cls_006"><span class="cls_006">dict</span></div>
<div style="position:absolute;left:384.18px;top:256.30px" class="cls_006"><span class="cls_006">nificantly and the newer data</span></div>
<div style="position:absolute;left:173.11px;top:269.84px" class="cls_006"><span class="cls_006">or</span></div>
<div style="position:absolute;left:200.50px;top:269.84px" class="cls_006"><span class="cls_006">Python</span></div>
<div style="position:absolute;left:384.18px;top:269.84px" class="cls_006"><span class="cls_006">points  improve  the  perfor-</span></div>
<div style="position:absolute;left:173.11px;top:283.39px" class="cls_006"><span class="cls_006">pickle</span></div>
<div style="position:absolute;left:384.18px;top:283.39px" class="cls_006"><span class="cls_006">mance of the model</span></div>
<div style="position:absolute;left:77.98px;top:297.34px" class="cls_006"><span class="cls_006">features</span></div>
<div style="position:absolute;left:173.12px;top:297.34px" class="cls_006"><span class="cls_006">Hierarchical</span></div>
<div style="position:absolute;left:247.45px;top:297.34px" class="cls_006"><span class="cls_006">Features for the images</span></div>
<div style="position:absolute;left:384.18px;top:297.34px" class="cls_006"><span class="cls_006">When  the  number  of  train-</span></div>
<div style="position:absolute;left:173.11px;top:310.89px" class="cls_006"><span class="cls_006">Data    For-</span></div>
<div style="position:absolute;left:384.18px;top:310.89px" class="cls_006"><span class="cls_006">ing  data  points  increase  sig-</span></div>
<div style="position:absolute;left:173.11px;top:324.44px" class="cls_006"><span class="cls_006">mat</span></div>
<div style="position:absolute;left:205.05px;top:324.44px" class="cls_006"><span class="cls_006">(HD5)</span></div>
<div style="position:absolute;left:384.18px;top:324.44px" class="cls_006"><span class="cls_006">nificantly and the newer data</span></div>
<div style="position:absolute;left:173.11px;top:337.99px" class="cls_006"><span class="cls_006">or</span></div>
<div style="position:absolute;left:200.50px;top:337.99px" class="cls_006"><span class="cls_006">Python</span></div>
<div style="position:absolute;left:384.18px;top:337.99px" class="cls_006"><span class="cls_006">points  improve  the  perfor-</span></div>
<div style="position:absolute;left:173.11px;top:351.54px" class="cls_006"><span class="cls_006">pickle</span></div>
<div style="position:absolute;left:384.18px;top:351.54px" class="cls_006"><span class="cls_006">mance of the model</span></div>
<div style="position:absolute;left:77.98px;top:365.49px" class="cls_006"><span class="cls_006">embedding matrix</span></div>
<div style="position:absolute;left:173.11px;top:365.49px" class="cls_006"><span class="cls_006">Hierarchical</span></div>
<div style="position:absolute;left:247.45px;top:365.49px" class="cls_006"><span class="cls_006">Embedding   vectors   for</span></div>
<div style="position:absolute;left:384.18px;top:365.49px" class="cls_006"><span class="cls_006">Whenever the number of words</span></div>
<div style="position:absolute;left:173.11px;top:379.04px" class="cls_006"><span class="cls_006">Data    For-</span></div>
<div style="position:absolute;left:247.45px;top:379.04px" class="cls_006"><span class="cls_006">each word in the vocabu-</span></div>
<div style="position:absolute;left:384.18px;top:379.04px" class="cls_006"><span class="cls_006">out  of  vocabulary  crosses</span></div>
<div style="position:absolute;left:524.31px;top:379.04px" class="cls_006"><span class="cls_006">a</span></div>
<div style="position:absolute;left:173.11px;top:392.58px" class="cls_006"><span class="cls_006">mat</span></div>
<div style="position:absolute;left:205.05px;top:392.58px" class="cls_006"><span class="cls_006">(HD5)</span></div>
<div style="position:absolute;left:247.45px;top:392.58px" class="cls_006"><span class="cls_006">lary</span></div>
<div style="position:absolute;left:384.18px;top:392.58px" class="cls_006"><span class="cls_006">particular threshold</span></div>
<div style="position:absolute;left:173.11px;top:406.13px" class="cls_006"><span class="cls_006">or</span></div>
<div style="position:absolute;left:200.50px;top:406.13px" class="cls_006"><span class="cls_006">Python</span></div>
<div style="position:absolute;left:173.11px;top:419.68px" class="cls_006"><span class="cls_006">pickle</span></div>
<div style="position:absolute;left:77.98px;top:433.63px" class="cls_006"><span class="cls_006">best model</span></div>
<div style="position:absolute;left:173.12px;top:433.63px" class="cls_006"><span class="cls_006">Hierarchical</span></div>
<div style="position:absolute;left:247.45px;top:433.63px" class="cls_006"><span class="cls_006">The model with the lowest</span></div>
<div style="position:absolute;left:384.18px;top:433.63px" class="cls_006"><span class="cls_006">Whenever a model achieves a</span></div>
<div style="position:absolute;left:173.11px;top:447.18px" class="cls_006"><span class="cls_006">Data Format</span></div>
<div style="position:absolute;left:247.45px;top:447.18px" class="cls_006"><span class="cls_006">validation loss</span></div>
<div style="position:absolute;left:384.18px;top:447.18px" class="cls_006"><span class="cls_006">lesser validation loss</span></div>
<div style="position:absolute;left:173.11px;top:460.73px" class="cls_006"><span class="cls_006">(HD5)</span></div>
<div style="position:absolute;left:214.98px;top:484.02px" class="cls_018"><span class="cls_018">Table</span></div>
<div style="position:absolute;left:248.26px;top:484.02px" class="cls_018"><span class="cls_018">3.5: File management</span></div>
<div style="position:absolute;left:72.00px;top:515.89px" class="cls_006"><span class="cls_006">in the park.</span></div>
<div style="position:absolute;left:134.26px;top:515.89px" class="cls_006"><span class="cls_006">One way to do this would be to incorporate an attention mechanism (65)</span></div>
<div style="position:absolute;left:72.00px;top:532.33px" class="cls_006"><span class="cls_006">in our model. However, this would not solve the problem perfectly as the results in (65)</span></div>
<div style="position:absolute;left:72.00px;top:548.76px" class="cls_006"><span class="cls_006">and (50) are quite comparable.</span></div>
<div style="position:absolute;left:72.00px;top:574.59px" class="cls_006"><span class="cls_006">A solution is to use object detection. As mentioned, image captioning provides a general</span></div>
<div style="position:absolute;left:72.00px;top:591.03px" class="cls_006"><span class="cls_006">overview of what the image is trying to convey. The motivation behind this is that to</span></div>
<div style="position:absolute;left:72.00px;top:607.46px" class="cls_006"><span class="cls_006">get a detailed understanding of what is going on in the image, users can toggle through</span></div>
<div style="position:absolute;left:72.00px;top:623.90px" class="cls_006"><span class="cls_006">the objects in the image and the system will read out what that object is.</span></div>
<div style="position:absolute;left:72.00px;top:649.72px" class="cls_006"><span class="cls_006">There are many object detection libraries and APIs available, but we use Microsoft</span></div>
<div style="position:absolute;left:72.00px;top:666.16px" class="cls_006"><span class="cls_006">Azure Vision API (30) for this purpose, due to the following reasons:</span></div>
<div style="position:absolute;left:85.33px;top:703.94px" class="cls_006"><span class="cls_006">1. The API returns the objects detected as well as the coordinates of the bounding</span></div>
<div style="position:absolute;left:99.27px;top:720.38px" class="cls_006"><span class="cls_006">boxes</span></div>
<div style="position:absolute;left:85.33px;top:745.78px" class="cls_006"><span class="cls_006">2. The API returns a confidence level of the detection</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:30672px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background45.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:513.00px;top:47.41px" class="cls_006"><span class="cls_006">29</span></div>
<div style="position:absolute;left:121.33px;top:80.43px" class="cls_006"><span class="cls_006">3. The API returns parent objects for any detected object. For e.g bicycle helmet -</span></div>
<div style="position:absolute;left:135.27px;top:96.87px" class="cls_006"><span class="cls_006">helmet - headwear. bicycle helmet is the detected object here.</span></div>
<div style="position:absolute;left:121.33px;top:121.48px" class="cls_006"><span class="cls_006">4. The API returns the data in JSON (JavaScript Object Notation) format which is</span></div>
<div style="position:absolute;left:135.27px;top:137.91px" class="cls_006"><span class="cls_006">easy to interpret using JavaScript and Python</span></div>
<div style="position:absolute;left:108.00px;top:172.94px" class="cls_006"><span class="cls_006">A sample response from the API, when the model has detected the object dog is shown</span></div>
<div style="position:absolute;left:108.00px;top:189.38px" class="cls_006"><span class="cls_006">below:</span></div>
<div style="position:absolute;left:108.46px;top:210.20px" class="cls_025"><span class="cls_025">{</span></div>
<div style="position:absolute;left:130.58px;top:223.49px" class="cls_025"><span class="cls_025">”url”:  url  for  the image,</span></div>
<div style="position:absolute;left:130.58px;top:236.78px" class="cls_025"><span class="cls_025">”response”:</span></div>
<div style="position:absolute;left:196.93px;top:236.78px" class="cls_025"><span class="cls_025">{</span></div>
<div style="position:absolute;left:152.70px;top:250.07px" class="cls_025"><span class="cls_025">”objects”:</span></div>
<div style="position:absolute;left:214.35px;top:250.07px" class="cls_025"><span class="cls_025">[{</span></div>
<div style="position:absolute;left:163.75px;top:263.37px" class="cls_025"><span class="cls_025">”rectangle”:</span></div>
<div style="position:absolute;left:235.63px;top:263.37px" class="cls_025"><span class="cls_025">{</span></div>
<div style="position:absolute;left:174.81px;top:276.66px" class="cls_025"><span class="cls_025">”x”:</span></div>
<div style="position:absolute;left:202.95px;top:276.66px" class="cls_025"><span class="cls_025">154,</span></div>
<div style="position:absolute;left:174.81px;top:289.95px" class="cls_025"><span class="cls_025">”y”:</span></div>
<div style="position:absolute;left:202.96px;top:289.95px" class="cls_025"><span class="cls_025">23,</span></div>
<div style="position:absolute;left:174.81px;top:303.25px" class="cls_025"><span class="cls_025">”w” :</span></div>
<div style="position:absolute;left:202.95px;top:303.25px" class="cls_025"><span class="cls_025">102,</span></div>
<div style="position:absolute;left:174.81px;top:316.54px" class="cls_025"><span class="cls_025">”h”:</span></div>
<div style="position:absolute;left:202.69px;top:316.54px" class="cls_025"><span class="cls_025">122</span></div>
<div style="position:absolute;left:164.27px;top:329.83px" class="cls_025"><span class="cls_025">},</span></div>
<div style="position:absolute;left:163.75px;top:343.13px" class="cls_025"><span class="cls_025">”object”:</span></div>
<div style="position:absolute;left:219.05px;top:343.13px" class="cls_025"><span class="cls_025">”dog”,</span></div>
<div style="position:absolute;left:163.75px;top:356.42px" class="cls_025"><span class="cls_025">”confidence”:</span></div>
<div style="position:absolute;left:241.90px;top:356.42px" class="cls_025"><span class="cls_025">0.845,</span></div>
<div style="position:absolute;left:163.75px;top:369.71px" class="cls_025"><span class="cls_025">”parent”:</span></div>
<div style="position:absolute;left:219.05px;top:369.71px" class="cls_025"><span class="cls_025">{</span></div>
<div style="position:absolute;left:174.81px;top:383.01px" class="cls_025"><span class="cls_025">”object”:</span></div>
<div style="position:absolute;left:230.11px;top:383.01px" class="cls_025"><span class="cls_025">”mammal” ,</span></div>
<div style="position:absolute;left:174.81px;top:396.30px" class="cls_025"><span class="cls_025">”confidence”:</span></div>
<div style="position:absolute;left:252.96px;top:396.30px" class="cls_025"><span class="cls_025">0.849,</span></div>
<div style="position:absolute;left:174.81px;top:409.59px" class="cls_025"><span class="cls_025">”parent”:</span></div>
<div style="position:absolute;left:230.10px;top:409.59px" class="cls_025"><span class="cls_025">{</span></div>
<div style="position:absolute;left:185.87px;top:422.88px" class="cls_025"><span class="cls_025">”object”:</span></div>
<div style="position:absolute;left:241.17px;top:422.88px" class="cls_025"><span class="cls_025">”animal”,</span></div>
<div style="position:absolute;left:185.87px;top:436.18px" class="cls_025"><span class="cls_025">”confidence”:</span></div>
<div style="position:absolute;left:263.93px;top:436.18px" class="cls_025"><span class="cls_025">0.906</span></div>
<div style="position:absolute;left:174.81px;top:449.47px" class="cls_025"><span class="cls_025">}</span></div>
<div style="position:absolute;left:163.75px;top:462.76px" class="cls_025"><span class="cls_025">}</span></div>
<div style="position:absolute;left:153.61px;top:476.06px" class="cls_025"><span class="cls_025">}],</span></div>
<div style="position:absolute;left:152.70px;top:489.35px" class="cls_025"><span class="cls_025">”requestId”:  unique  request</span></div>
<div style="position:absolute;left:307.90px;top:489.35px" class="cls_025"><span class="cls_025">id ,</span></div>
<div style="position:absolute;left:152.70px;top:502.64px" class="cls_025"><span class="cls_025">”metadata”:</span></div>
<div style="position:absolute;left:219.05px;top:502.64px" class="cls_025"><span class="cls_025">{</span></div>
<div style="position:absolute;left:163.75px;top:515.94px" class="cls_025"><span class="cls_025">”width”:</span></div>
<div style="position:absolute;left:214.01px;top:515.94px" class="cls_025"><span class="cls_025">356,</span></div>
<div style="position:absolute;left:163.75px;top:529.23px" class="cls_025"><span class="cls_025">”height”:</span></div>
<div style="position:absolute;left:219.54px;top:529.23px" class="cls_025"><span class="cls_025">277,</span></div>
<div style="position:absolute;left:163.75px;top:542.52px" class="cls_025"><span class="cls_025">”format”:</span></div>
<div style="position:absolute;left:219.04px;top:542.52px" class="cls_025"><span class="cls_025">”Png”</span></div>
<div style="position:absolute;left:152.70px;top:555.82px" class="cls_025"><span class="cls_025">}</span></div>
<div style="position:absolute;left:141.64px;top:569.11px" class="cls_025"><span class="cls_025">}</span></div>
<div style="position:absolute;left:108.97px;top:582.40px" class="cls_025"><span class="cls_025">},</span></div>
<div style="position:absolute;left:197.09px;top:602.49px" class="cls_018"><span class="cls_018">Listing 3.7: Sample response from Azure Vision API</span></div>
<div style="position:absolute;left:108.00px;top:634.77px" class="cls_006"><span class="cls_006">Some of the features that this application should have are the following:</span></div>
<div style="position:absolute;left:121.33px;top:669.80px" class="cls_006"><span class="cls_006">1. The image should have a generic caption which will be the output of the image</span></div>
<div style="position:absolute;left:135.27px;top:686.24px" class="cls_006"><span class="cls_006">captioning model</span></div>
<div style="position:absolute;left:121.33px;top:710.85px" class="cls_006"><span class="cls_006">2. If the user wants, they can explore more.  On the click of a button, the object</span></div>
<div style="position:absolute;left:135.27px;top:727.28px" class="cls_006"><span class="cls_006">detection API should be executed</span></div>
<div style="position:absolute;left:121.33px;top:751.89px" class="cls_006"><span class="cls_006">3. The bounding boxes for each object in the image should be drawn</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:31524px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background46.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">30</span></div>
<div style="position:absolute;left:255.40px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:291.71px;top:159.38px" class="cls_031"><span class="cls_031">User loads specific image from the dropdown</span></div>
<div style="position:absolute;left:147.42px;top:167.52px" class="cls_031"><span class="cls_031">Home screen - default image and the caption is loaded</span></div>
<div style="position:absolute;left:290.34px;top:270.55px" class="cls_031"><span class="cls_031">User has clicked the button. Object detection is applied on</span></div>
<div style="position:absolute;left:139.85px;top:277.88px" class="cls_031"><span class="cls_031">User presses 'space' key or hovers over the parts of the</span></div>
<div style="position:absolute;left:302.02px;top:276.25px" class="cls_031"><span class="cls_031">the image as can be seen by the green borders</span></div>
<div style="position:absolute;left:139.85px;top:283.58px" class="cls_031"><span class="cls_031">image to get feedback. The description is provided both by</span></div>
<div style="position:absolute;left:139.85px;top:289.28px" class="cls_031"><span class="cls_031">text and speech and accompanied with a confidence level</span></div>
<div style="position:absolute;left:143.11px;top:417.14px" class="cls_031"><span class="cls_031">As the user hovers over the objects or presses the 'space'</span></div>
<div style="position:absolute;left:143.11px;top:422.84px" class="cls_031"><span class="cls_031">key, the corresponding description also changes</span></div>
<div style="position:absolute;left:161.82px;top:443.70px" class="cls_018"><span class="cls_018">Figure 3.9: Object Detection App - User interaction</span></div>
<div style="position:absolute;left:85.33px;top:475.05px" class="cls_006"><span class="cls_006">4. User can hover over these objects and the information should be provided via text</span></div>
<div style="position:absolute;left:99.27px;top:491.48px" class="cls_006"><span class="cls_006">and speech</span></div>
<div style="position:absolute;left:85.33px;top:516.62px" class="cls_006"><span class="cls_006">5. User can also toggle through the objects by pressing a specific key (completely</span></div>
<div style="position:absolute;left:99.27px;top:533.06px" class="cls_006"><span class="cls_006">blind users will not know where to hover on the image)</span></div>
<div style="position:absolute;left:72.00px;top:569.93px" class="cls_006"><span class="cls_006">Keeping these features in mind, a demonstration application was built. The link to this</span></div>
<div style="position:absolute;left:72.00px;top:586.36px" class="cls_006"><span class="cls_006">application is: </span><A HREF="https://codepen.io/shaunak1105/full/dybZEXa">https://codepen.io/shaunak1105/full/dybZEXa</A> </div>
<div style="position:absolute;left:72.00px;top:612.07px" class="cls_006"><span class="cls_006">Figure 3.9 shows how users can interact with the app. The information about the objects</span></div>
<div style="position:absolute;left:72.00px;top:628.50px" class="cls_006"><span class="cls_006">detected is provided both by text and speech. The bounding boxes are also overlaid on</span></div>
<div style="position:absolute;left:72.00px;top:644.94px" class="cls_006"><span class="cls_006">the image. Users can choose to hover over the objects or toggle through them by pressing</span></div>
<div style="position:absolute;left:72.00px;top:661.37px" class="cls_006"><span class="cls_006">the space key.</span></div>
<div style="position:absolute;left:72.00px;top:701.66px" class="cls_002"><span class="cls_002">3.10.2</span></div>
<div style="position:absolute;left:119.82px;top:701.66px" class="cls_002"><span class="cls_002">Relevance of image on a web page</span></div>
<div style="position:absolute;left:72.00px;top:735.45px" class="cls_006"><span class="cls_006">Often, we come across web sites which are cluttered with images.  These images may</span></div>
<div style="position:absolute;left:72.00px;top:751.89px" class="cls_006"><span class="cls_006">be present for the purpose of advertisements and they do not convey any real meaning</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:32376px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background47.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:513.00px;top:47.41px" class="cls_006"><span class="cls_006">31</span></div>
<div style="position:absolute;left:270.72px;top:81.19px" class="cls_032"><span class="cls_032">Caption: man in blue is surfing in the water</span></div>
<div style="position:absolute;left:262.81px;top:89.01px" class="cls_033"><span class="cls_033">Pre-processed text</span></div>
<div style="position:absolute;left:281.50px;top:96.83px" class="cls_032"><span class="cls_032">man blue surfing water</span></div>
<div style="position:absolute;left:244.63px;top:102.85px" class="cls_033"><span class="cls_033">Surfing  is  a  surface</span></div>
<div style="position:absolute;left:244.63px;top:105.66px" class="cls_033"><span class="cls_033">water  sport  in  which</span></div>
<div style="position:absolute;left:244.63px;top:108.47px" class="cls_033"><span class="cls_033">the</span></div>
<div style="position:absolute;left:252.56px;top:108.47px" class="cls_033"><span class="cls_033">wave    rider,</span></div>
<div style="position:absolute;left:244.63px;top:111.27px" class="cls_033"><span class="cls_033">referred   to   as   a</span></div>
<div style="position:absolute;left:244.63px;top:114.08px" class="cls_033"><span class="cls_033">surfer,  rides  on  the</span></div>
<div style="position:absolute;left:244.63px;top:116.89px" class="cls_033"><span class="cls_033">forward  or  face  of  a</span></div>
<div style="position:absolute;left:333.75px;top:116.69px" class="cls_033"><span class="cls_033">Run the algorithm to get WMDscores by N-grams</span></div>
<div style="position:absolute;left:244.63px;top:119.70px" class="cls_033"><span class="cls_033">moving  wave,  whichusually   carries   the</span></div>
<div style="position:absolute;left:175.49px;top:125.32px" class="cls_033"><span class="cls_033">1.34 (not similar)</span></div>
<div style="position:absolute;left:203.04px;top:125.32px" class="cls_033"><span class="cls_033">Compute WMD Score</span></div>
<div style="position:absolute;left:244.63px;top:125.32px" class="cls_033"><span class="cls_033">surfer   towards   the</span></div>
<div style="position:absolute;left:244.63px;top:128.12px" class="cls_033"><span class="cls_033">shore. Waves suitable</span></div>
<div style="position:absolute;left:244.63px;top:130.93px" class="cls_033"><span class="cls_033">for</span></div>
<div style="position:absolute;left:252.42px;top:130.93px" class="cls_033"><span class="cls_033">surfing</span></div>
<div style="position:absolute;left:264.63px;top:130.93px" class="cls_033"><span class="cls_033">are</span></div>
<div style="position:absolute;left:244.63px;top:133.74px" class="cls_033"><span class="cls_033">primarily found in theocean,  but  can  also</span></div>
<div style="position:absolute;left:362.14px;top:136.35px" class="cls_033"><span class="cls_033">N-grams matched:  ('man', 'blue', 'surfing') ('surfing', 'surface', 'water') Score: 0.88</span></div>
<div style="position:absolute;left:244.63px;top:139.36px" class="cls_033"><span class="cls_033">be found in lakes or</span></div>
<div style="position:absolute;left:270.30px;top:138.55px" class="cls_033"><span class="cls_033">However, surfers can also utilize artificial</span></div>
<div style="position:absolute;left:362.48px;top:139.16px" class="cls_033"><span class="cls_033">N-grams matched:  ('blue', 'surfing', 'water') ('surface', 'water', 'sport') Score: 0.83</span></div>
<div style="position:absolute;left:244.63px;top:142.16px" class="cls_033"><span class="cls_033">rivers in the form of a</span></div>
<div style="position:absolute;left:270.30px;top:141.36px" class="cls_033"><span class="cls_033">waves such as those from boat wakes and the</span></div>
<div style="position:absolute;left:354.27px;top:141.96px" class="cls_033"><span class="cls_033">N-grams matched:  ('man', 'blue', 'surfing', 'water') ('surfing', 'surface', 'water', 'sport') Score: 0.65</span></div>
<div style="position:absolute;left:244.63px;top:144.97px" class="cls_033"><span class="cls_033">standing wave or tidal</span></div>
<div style="position:absolute;left:270.30px;top:144.17px" class="cls_033"><span class="cls_033">waves created in artificial wave pools.</span></div>
<div style="position:absolute;left:244.63px;top:147.78px" class="cls_033"><span class="cls_033">bore.</span></div>
<div style="position:absolute;left:290.89px;top:151.39px" class="cls_033"><span class="cls_033">Pre-processed text</span></div>
<div style="position:absolute;left:244.83px;top:161.02px" class="cls_034"><span class="cls_034">surfing surface water</span><span class="cls_032"> </span><span class="cls_034">sport</span><span class="cls_032"> wave rider, referred surfer, rides</span></div>
<div style="position:absolute;left:244.83px;top:163.83px" class="cls_032"><span class="cls_032">forward face moving wave, usually carries surfer towards shore.</span></div>
<div style="position:absolute;left:244.83px;top:166.64px" class="cls_032"><span class="cls_032">waves suitable surfing primarily found ocean, also found lakes</span></div>
<div style="position:absolute;left:244.83px;top:169.44px" class="cls_032"><span class="cls_032">rivers form standing wave tidal bore. however, surfers also utilize</span></div>
<div style="position:absolute;left:244.83px;top:172.25px" class="cls_032"><span class="cls_032">artificial waves boat wakes waves created artificial wave pools</span></div>
<div style="position:absolute;left:145.53px;top:189.32px" class="cls_018"><span class="cls_018">Figure 3.10: Visualizing the algorithm for Relevance of image in a web page</span></div>
<div style="position:absolute;left:108.00px;top:221.20px" class="cls_006"><span class="cls_006">to the topic being discussed.  These images are thus confusing and distracting and for</span></div>
<div style="position:absolute;left:108.00px;top:237.63px" class="cls_006"><span class="cls_006">someone who is using screen readers, the experience will be worse.</span></div>
<div style="position:absolute;left:108.00px;top:263.46px" class="cls_006"><span class="cls_006">The motivation behind this extension is to detect these irrelevant images from the text</span></div>
<div style="position:absolute;left:108.00px;top:279.90px" class="cls_006"><span class="cls_006">surrounding it and automatically flag these images so that they can be ignored by the</span></div>
<div style="position:absolute;left:108.00px;top:296.33px" class="cls_006"><span class="cls_006">screen readers.</span></div>
<div style="position:absolute;left:108.00px;top:322.16px" class="cls_006"><span class="cls_006">We have already built an image captioning system and evaluated metrics for testing</span></div>
<div style="position:absolute;left:108.00px;top:338.60px" class="cls_006"><span class="cls_006">caption-to-caption similarity using WMD. To extract the text surrounding the image,</span></div>
<div style="position:absolute;left:108.00px;top:355.03px" class="cls_006"><span class="cls_006">we can consider a window of w words around the image tag. Extracting such a data will</span></div>
<div style="position:absolute;left:108.00px;top:371.47px" class="cls_006"><span class="cls_006">be discussed in detail in subsection 4.3.2. So we have the generated caption, the words</span></div>
<div style="position:absolute;left:108.00px;top:387.90px" class="cls_006"><span class="cls_006">surrounding the image and we want to apply WMD to detect of the text and image</span></div>
<div style="position:absolute;left:108.00px;top:404.34px" class="cls_006"><span class="cls_006">caption are in context to each other or not.</span></div>
<div style="position:absolute;left:108.00px;top:430.17px" class="cls_006"><span class="cls_006">We initially pre-process both the caption text and the surrounding text as in section 3.3.</span></div>
<div style="position:absolute;left:108.00px;top:446.60px" class="cls_006"><span class="cls_006">Then we computed the WMD score. however, we were not getting proper results (the</span></div>
<div style="position:absolute;left:108.00px;top:463.04px" class="cls_006"><span class="cls_006">scores were always higher than 1.25).</span></div>
<div style="position:absolute;left:108.00px;top:488.87px" class="cls_006"><span class="cls_006">The caption text does not have a fixed length of words. So we cannot directly consider</span></div>
<div style="position:absolute;left:108.00px;top:505.30px" class="cls_006"><span class="cls_006">it as a document and compute WMD between the texts. Additionally, only a part of the</span></div>
<div style="position:absolute;left:108.00px;top:521.74px" class="cls_006"><span class="cls_006">surrounding text might be discussing the caption. So if we simply compute the WMD</span></div>
<div style="position:absolute;left:108.00px;top:538.17px" class="cls_006"><span class="cls_006">between the caption and surrounding text, it will return a high value, but that does not</span></div>
<div style="position:absolute;left:108.00px;top:554.61px" class="cls_006"><span class="cls_006">mean that they are not similar.</span></div>
<div style="position:absolute;left:108.00px;top:580.43px" class="cls_006"><span class="cls_006">However, at least one part of the surrounding text must be discussing the caption. We</span></div>
<div style="position:absolute;left:108.00px;top:596.87px" class="cls_006"><span class="cls_006">can detect this by splitting both the surrounding text and the caption into N-grams</span></div>
<div style="position:absolute;left:108.00px;top:613.31px" class="cls_006"><span class="cls_006">(62).  For example, if the caption has 5 words, we can consider 2-gram sequences, 3-</span></div>
<div style="position:absolute;left:108.00px;top:629.74px" class="cls_006"><span class="cls_006">gram sequences, 4-gram sequences, and 5-gram sequences between the caption and the</span></div>
<div style="position:absolute;left:108.00px;top:646.18px" class="cls_006"><span class="cls_006">surrounding text and then compute WMD similarity. The intuition is that at least one</span></div>
<div style="position:absolute;left:108.00px;top:662.61px" class="cls_006"><span class="cls_006">of these N-grams between the caption and the surrounding text should have a close</span></div>
<div style="position:absolute;left:108.00px;top:679.05px" class="cls_006"><span class="cls_006">match i.e have less WMD score.</span></div>
<div style="position:absolute;left:108.00px;top:704.87px" class="cls_006"><span class="cls_006">The algorithm to do this is described below:</span></div>
<div style="position:absolute;left:108.00px;top:730.70px" class="cls_006"><span class="cls_006">This process is visualized in Figure 3.10. It is clear for this scenario, simply computing</span></div>
<div style="position:absolute;left:108.00px;top:747.14px" class="cls_006"><span class="cls_006">the WMD score between the caption and surrounding text resulted in a high score of</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:33228px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background48.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">32</span></div>
<div style="position:absolute;left:255.40px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 3 Automatic Image Captioning System</span></div>
<div style="position:absolute;left:73.14px;top:81.88px" class="cls_025"><span class="cls_025">Preprocess  both the  caption  text and surrounding  text</span></div>
<div style="position:absolute;left:72.23px;top:92.84px" class="cls_025"><span class="cls_025">Compute  length  of  caption</span></div>
<div style="position:absolute;left:72.59px;top:103.80px" class="cls_025"><span class="cls_025">For  i  in  range  of</span></div>
<div style="position:absolute;left:171.99px;top:103.80px" class="cls_025"><span class="cls_025">2</span></div>
<div style="position:absolute;left:183.54px;top:103.80px" class="cls_025"><span class="cls_025">to  length  of  caption:</span></div>
<div style="position:absolute;left:95.05px;top:114.76px" class="cls_025"><span class="cls_025">Create  i−grams  of  caption  text</span></div>
<div style="position:absolute;left:95.05px;top:125.72px" class="cls_025"><span class="cls_025">Create  i−grams  of  surrounding  text</span></div>
<div style="position:absolute;left:94.71px;top:136.67px" class="cls_025"><span class="cls_025">For each such  i−gram pair</span></div>
<div style="position:absolute;left:116.46px;top:147.63px" class="cls_025"><span class="cls_025">Compute WMD between  caption  i−gram  and  surrounding  i−gram</span></div>
<div style="position:absolute;left:139.99px;top:158.59px" class="cls_025"><span class="cls_025">If WMD &lt; 1.15</span></div>
<div style="position:absolute;left:160.65px;top:169.55px" class="cls_025"><span class="cls_025">The  caption  i s  similar</span></div>
<div style="position:absolute;left:183.28px;top:180.51px" class="cls_025"><span class="cls_025">Return  true</span></div>
<div style="position:absolute;left:139.46px;top:191.47px" class="cls_025"><span class="cls_025">Else</span></div>
<div style="position:absolute;left:161.57px;top:202.43px" class="cls_025"><span class="cls_025">continue</span></div>
<div style="position:absolute;left:72.69px;top:213.39px" class="cls_025"><span class="cls_025">Return  false</span></div>
<div style="position:absolute;left:142.79px;top:213.39px" class="cls_025"><span class="cls_025">− the  caption  is  not  similar</span></div>
<div style="position:absolute;left:144.44px;top:230.62px" class="cls_018"><span class="cls_018">Listing 3.8: Algorithm for Relevance of image in a web page</span></div>
<div style="position:absolute;left:72.00px;top:271.46px" class="cls_006"><span class="cls_006">1.34.  However using the algorithm discussed, we get a much lower score and we can</span></div>
<div style="position:absolute;left:72.00px;top:287.90px" class="cls_006"><span class="cls_006">also visually see which parts of the text received a close match (shown in green in</span></div>
<div style="position:absolute;left:72.00px;top:304.33px" class="cls_006"><span class="cls_006">Figure 3.10).  By removal of stopwords, we have ensured that common words are not</span></div>
<div style="position:absolute;left:72.00px;top:320.77px" class="cls_006"><span class="cls_006">taken into the formation of N-grams.</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:34080px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background49.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:160.10px" class="cls_003"><span class="cls_003">Chapter 4</span></div>
<div style="position:absolute;left:108.00px;top:221.55px" class="cls_017"><span class="cls_017">Contextual link text detection</span></div>
<div style="position:absolute;left:108.00px;top:302.99px" class="cls_005"><span class="cls_005">4.1</span></div>
<div style="position:absolute;left:144.76px;top:302.99px" class="cls_005"><span class="cls_005">The problem</span></div>
<div style="position:absolute;left:108.00px;top:343.06px" class="cls_006"><span class="cls_006">As previously discussed in section 3.1, there exists a set of guidelines called Web Con-</span></div>
<div style="position:absolute;left:108.00px;top:359.49px" class="cls_006"><span class="cls_006">tent Accessibility Guidelines (WCAG), which aims to create and maintain a single set</span></div>
<div style="position:absolute;left:108.00px;top:375.93px" class="cls_006"><span class="cls_006">of guidelines and recommendations for individuals, organizations, and governments in-</span></div>
<div style="position:absolute;left:108.00px;top:392.36px" class="cls_006"><span class="cls_006">ternationally to follow to make web content more accessible and inclusive, especially for</span></div>
<div style="position:absolute;left:108.00px;top:408.80px" class="cls_006"><span class="cls_006">people with disabilities (58; 64).</span></div>
<div style="position:absolute;left:108.00px;top:434.63px" class="cls_006"><span class="cls_006">Guideline G91 (19) of the WCAG focuses on hyperlinks, specifically providing link text</span></div>
<div style="position:absolute;left:108.00px;top:451.06px" class="cls_006"><span class="cls_006">that describes the purpose of the link. The aim of this guideline is, that from the link</span></div>
<div style="position:absolute;left:108.00px;top:467.50px" class="cls_006"><span class="cls_006">text, users should get an idea about what the link is talking about. This project deals</span></div>
<div style="position:absolute;left:108.00px;top:483.93px" class="cls_006"><span class="cls_006">with a modification of this particular problem.</span></div>
<div style="position:absolute;left:108.00px;top:509.76px" class="cls_006"><span class="cls_006">People with reading disabilities, like dyslexia, cannot process large amounts of texts</span></div>
<div style="position:absolute;left:108.00px;top:526.19px" class="cls_006"><span class="cls_006">(32). Often, on many websites, we find hyperlinks occurring randomly, unrelated to the</span></div>
<div style="position:absolute;left:108.00px;top:542.63px" class="cls_006"><span class="cls_006">current topic being discussed. My aim in this project is to find out if the hyperlink text,</span></div>
<div style="position:absolute;left:108.00px;top:559.07px" class="cls_006"><span class="cls_006">the text surrounding the hyperlink and the text (and title) of the link it points to are</span></div>
<div style="position:absolute;left:108.00px;top:575.50px" class="cls_006"><span class="cls_006">in close context or not. This will help detect if the hyperlink is contextual and relevant</span></div>
<div style="position:absolute;left:108.00px;top:591.94px" class="cls_006"><span class="cls_006">to the topic being discussed.</span></div>
<div style="position:absolute;left:108.00px;top:636.08px" class="cls_005"><span class="cls_005">4.2</span></div>
<div style="position:absolute;left:144.76px;top:636.08px" class="cls_005"><span class="cls_005">Proposed solution</span></div>
<div style="position:absolute;left:108.00px;top:676.15px" class="cls_006"><span class="cls_006">The problem can be tackled in multiple approaches using Natural Language Processing</span></div>
<div style="position:absolute;left:108.00px;top:692.59px" class="cls_006"><span class="cls_006">and Deep Learning techniques.  One way to see this is as a topic modeling task; we</span></div>
<div style="position:absolute;left:108.00px;top:709.02px" class="cls_006"><span class="cls_006">detect the topic in the source link, the target link, and the hyperlink text and try to</span></div>
<div style="position:absolute;left:108.00px;top:725.46px" class="cls_006"><span class="cls_006">match if they are similar or not. However, having applied word embedding techniques</span></div>
<div style="position:absolute;left:310.50px;top:773.49px" class="cls_006"><span class="cls_006">33</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:34932px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background50.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">34</span></div>
<div style="position:absolute;left:292.13px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 4 Contextual link text detection</span></div>
<div style="position:absolute;left:163.32px;top:220.89px" class="cls_018"><span class="cls_018">Figure 4.1: Links to Wikipedia as entity labels (47)</span></div>
<div style="position:absolute;left:72.00px;top:253.32px" class="cls_006"><span class="cls_006">like word2vec (31) and document-based similarity metrics like WMD (23) in the sub-</span></div>
<div style="position:absolute;left:72.00px;top:269.75px" class="cls_006"><span class="cls_006">section 3.8.5, I wanted to explore how these solutions could be extended for the current</span></div>
<div style="position:absolute;left:72.00px;top:286.19px" class="cls_006"><span class="cls_006">problem.</span></div>
<div style="position:absolute;left:72.00px;top:312.01px" class="cls_006"><span class="cls_006">As we discussed in subsection 3.8.4, how word2vec (31) works is by learning the associ-</span></div>
<div style="position:absolute;left:72.00px;top:328.45px" class="cls_006"><span class="cls_006">ations between words that occur together in a large text corpus. It assumes that words</span></div>
<div style="position:absolute;left:72.00px;top:344.89px" class="cls_006"><span class="cls_006">that occur together in this corpus have a close context, i.e. are similar. For our image</span></div>
<div style="position:absolute;left:72.00px;top:361.32px" class="cls_006"><span class="cls_006">captioning task, we used a pre-trained word embedding of Google News (12) because</span></div>
<div style="position:absolute;left:72.00px;top:377.76px" class="cls_006"><span class="cls_006">detecting if two captions were similar or not was a generic task, for which pre-trained</span></div>
<div style="position:absolute;left:72.00px;top:394.19px" class="cls_006"><span class="cls_006">embedding was ideal. For the current task, it makes sense to consider a more specialized</span></div>
<div style="position:absolute;left:72.00px;top:410.63px" class="cls_006"><span class="cls_006">dataset, that can capture the associations between source and target hyperlinks.  So,</span></div>
<div style="position:absolute;left:72.00px;top:427.06px" class="cls_006"><span class="cls_006">using this specialized text corpus, the aim is to train a specialized word2vec model for</span></div>
<div style="position:absolute;left:72.00px;top:443.50px" class="cls_006"><span class="cls_006">this task and evaluate the results. By comparing the results with those obtained using</span></div>
<div style="position:absolute;left:72.00px;top:459.93px" class="cls_006"><span class="cls_006">pre-trained embeddings, we can also understand the need for using specialized datasets</span></div>
<div style="position:absolute;left:72.00px;top:476.37px" class="cls_006"><span class="cls_006">for natural language processing tasks in this domain.</span></div>
<div style="position:absolute;left:72.00px;top:502.20px" class="cls_006"><span class="cls_006">The overall process of the proposed solution is thus:</span></div>
<div style="position:absolute;left:85.33px;top:536.99px" class="cls_006"><span class="cls_006">1. Explore a suitable dataset that captures hyperlink information</span></div>
<div style="position:absolute;left:85.33px;top:562.39px" class="cls_006"><span class="cls_006">2. Build a text corpus</span></div>
<div style="position:absolute;left:85.33px;top:587.79px" class="cls_006"><span class="cls_006">3. Build and train a word2vec model on this corpus</span></div>
<div style="position:absolute;left:85.33px;top:613.20px" class="cls_006"><span class="cls_006">4. Evaluate the results</span></div>
<div style="position:absolute;left:85.33px;top:638.60px" class="cls_006"><span class="cls_006">5. Compare the results with those obtained using pre-trained word embeddings</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:35784px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background51.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 4 Contextual link text detection</span></div>
<div style="position:absolute;left:513.00px;top:47.41px" class="cls_006"><span class="cls_006">35</span></div>
<div style="position:absolute;left:108.00px;top:77.00px" class="cls_005"><span class="cls_005">4.3</span></div>
<div style="position:absolute;left:144.76px;top:77.00px" class="cls_005"><span class="cls_005">Building the dataset</span></div>
<div style="position:absolute;left:108.00px;top:116.41px" class="cls_002"><span class="cls_002">4.3.1</span></div>
<div style="position:absolute;left:149.10px;top:116.41px" class="cls_002"><span class="cls_002">Initial dataset</span></div>
<div style="position:absolute;left:108.00px;top:150.24px" class="cls_006"><span class="cls_006">The Wikilinks dataset (47) provides the URLs of web pages and each URL of a web</span></div>
<div style="position:absolute;left:108.00px;top:166.68px" class="cls_006"><span class="cls_006">page has the following:</span></div>
<div style="position:absolute;left:121.33px;top:203.80px" class="cls_006"><span class="cls_006">1. The link texts of all URLs in that page</span></div>
<div style="position:absolute;left:121.33px;top:229.01px" class="cls_006"><span class="cls_006">2. The target link to which the particular URL points to</span></div>
<div style="position:absolute;left:108.00px;top:266.12px" class="cls_006"><span class="cls_006">This is shown in Figure 4.1</span></div>
<div style="position:absolute;left:108.00px;top:291.86px" class="cls_006"><span class="cls_006">The number of URLs in total in this dataset is 10,893,248, the number of link texts</span></div>
<div style="position:absolute;left:108.00px;top:308.30px" class="cls_006"><span class="cls_006">is 40,323,863 and the number of unique target links is 2,933,659 (many links point</span></div>
<div style="position:absolute;left:108.00px;top:324.73px" class="cls_006"><span class="cls_006">to the same target).  An important factor to note here is that all the target links are</span></div>
<div style="position:absolute;left:108.00px;top:341.17px" class="cls_006"><span class="cls_006">Wikipedia links, so the model we develop will have this bias (18).  The data schema</span></div>
<div style="position:absolute;left:108.00px;top:357.60px" class="cls_006"><span class="cls_006">looks like:</span></div>
<div style="position:absolute;left:107.28px;top:379.02px" class="cls_025"><span class="cls_025">URL  u r l 1</span></div>
<div style="position:absolute;left:107.13px;top:392.31px" class="cls_025"><span class="cls_025">MENTION  m e n t i o n  1</span></div>
<div style="position:absolute;left:209.06px;top:392.31px" class="cls_025"><span class="cls_025">target link</span></div>
<div style="position:absolute;left:273.27px;top:392.31px" class="cls_025"><span class="cls_025">1</span></div>
<div style="position:absolute;left:107.13px;top:405.61px" class="cls_025"><span class="cls_025">MENTION  m e n t i o n  2</span></div>
<div style="position:absolute;left:209.06px;top:405.61px" class="cls_025"><span class="cls_025">target link</span></div>
<div style="position:absolute;left:273.27px;top:405.61px" class="cls_025"><span class="cls_025">2</span></div>
<div style="position:absolute;left:107.13px;top:432.19px" class="cls_025"><span class="cls_025">MENTION  m e n t i o n  k  t a r g e t  l i n k  k</span></div>
<div style="position:absolute;left:107.28px;top:472.07px" class="cls_025"><span class="cls_025">URL  u r l 2</span></div>
<div style="position:absolute;left:107.13px;top:485.36px" class="cls_025"><span class="cls_025">MENTION  m e n t i o n  1</span></div>
<div style="position:absolute;left:209.06px;top:485.36px" class="cls_025"><span class="cls_025">target link</span></div>
<div style="position:absolute;left:273.27px;top:485.36px" class="cls_025"><span class="cls_025">1</span></div>
<div style="position:absolute;left:107.13px;top:498.66px" class="cls_025"><span class="cls_025">MENTION  m e n t i o n  2</span></div>
<div style="position:absolute;left:209.06px;top:498.66px" class="cls_025"><span class="cls_025">target link</span></div>
<div style="position:absolute;left:273.27px;top:498.66px" class="cls_025"><span class="cls_025">2</span></div>
<div style="position:absolute;left:107.13px;top:525.24px" class="cls_025"><span class="cls_025">MENTION  mention m  t a r g e t  l i n k  m</span></div>
<div style="position:absolute;left:257.23px;top:545.63px" class="cls_018"><span class="cls_018">Listing 4.1: Data schema</span></div>
<div style="position:absolute;left:108.00px;top:579.10px" class="cls_006"><span class="cls_006">As we can see each URL has a number of mentions (links and link texts) and a number</span></div>
<div style="position:absolute;left:108.00px;top:595.53px" class="cls_006"><span class="cls_006">of target links. The first URL has k mentions and target links, while the second one has</span></div>
<div style="position:absolute;left:108.00px;top:611.97px" class="cls_006"><span class="cls_006">m. From this initial dataset, the plan is to extend it to build a text corpus on which we</span></div>
<div style="position:absolute;left:108.00px;top:628.40px" class="cls_006"><span class="cls_006">can train a deep learning model.</span></div>
<div style="position:absolute;left:108.00px;top:668.75px" class="cls_002"><span class="cls_002">4.3.2</span></div>
<div style="position:absolute;left:149.10px;top:668.75px" class="cls_002"><span class="cls_002">Extending the dataset</span></div>
<div style="position:absolute;left:108.00px;top:702.58px" class="cls_006"><span class="cls_006">The idea is to consider a sliding window of size w of words around the source link and</span></div>
<div style="position:absolute;left:108.00px;top:719.02px" class="cls_006"><span class="cls_006">to extract the words in this window.  For the target link, we extract the title and the</span></div>
<div style="position:absolute;left:108.00px;top:735.45px" class="cls_006"><span class="cls_006">first w words. This will enable us to build a corpus of link texts and the corresponding</span></div>
<div style="position:absolute;left:108.00px;top:751.89px" class="cls_006"><span class="cls_006">target link texts. The process is summarized in Figure 4.2</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:36636px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background52.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">36</span></div>
<div style="position:absolute;left:292.13px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 4 Contextual link text detection</span></div>
<div style="position:absolute;left:313.35px;top:88.15px" class="cls_035"><span class="cls_035">Target link title</span></div>
<div style="position:absolute;left:142.81px;top:95.21px" class="cls_036"><span class="cls_036">Lorem ipsum dolor sit amet, consectetur adipiscing elit.</span></div>
<div style="position:absolute;left:142.81px;top:100.16px" class="cls_036"><span class="cls_036">Nullam facilisis accumsan magna a iaculis. Cras sed</span></div>
<div style="position:absolute;left:313.35px;top:101.57px" class="cls_036"><span class="cls_036">Lorem ipsum dolor sit amet, consectetur adipiscing</span></div>
<div style="position:absolute;left:142.81px;top:105.10px" class="cls_036"><span class="cls_036">tortor eget dolor vulputate ultrices a at est. Aliquam</span></div>
<div style="position:absolute;left:253.94px;top:102.98px" class="cls_036"><span class="cls_036">Visit target link of link_text</span></div>
<div style="position:absolute;left:313.35px;top:106.51px" class="cls_036"><span class="cls_036">elit. Nullam facilisis accumsan magna a iaculis. Cras</span></div>
<div style="position:absolute;left:142.81px;top:110.04px" class="cls_036"><span class="cls_036">congue a libero ut consequat. </span><span class="cls_037">[link_text]</span><span class="cls_036"> Mauris</span></div>
<div style="position:absolute;left:313.35px;top:111.45px" class="cls_036"><span class="cls_036">sed tortor eget dolor vulputate ultrices a at est.</span></div>
<div style="position:absolute;left:142.81px;top:114.99px" class="cls_036"><span class="cls_036">porttitor ex faucibus ipsum ultrices, sed vehicula erat</span></div>
<div style="position:absolute;left:313.35px;top:116.40px" class="cls_036"><span class="cls_036">Aliquam congue a libero ut consequat. Mauris porttitor</span></div>
<div style="position:absolute;left:142.81px;top:119.93px" class="cls_036"><span class="cls_036">euismod. Vestibulum eget lacinia ante, in sollicitudin</span></div>
<div style="position:absolute;left:313.35px;top:121.34px" class="cls_036"><span class="cls_036">ex faucibus ipsum ultrices, sed vehicula erat euismod.</span></div>
<div style="position:absolute;left:142.81px;top:124.87px" class="cls_036"><span class="cls_036">leo. Etiam convallis eu eros a ullamcorper.</span></div>
<div style="position:absolute;left:313.35px;top:126.28px" class="cls_036"><span class="cls_036">Vestibulum eget lacinia ante, in sollicitudin leo. Etiam</span></div>
<div style="position:absolute;left:313.35px;top:131.23px" class="cls_036"><span class="cls_036">convallis eu eros a ullamcorper.</span></div>
<div style="position:absolute;left:171.41px;top:159.47px" class="cls_038"><span class="cls_038">Link text and 'w' words surrounding it</span></div>
<div style="position:absolute;left:339.13px;top:159.47px" class="cls_038"><span class="cls_038">First 'w' words of target link</span></div>
<div style="position:absolute;left:280.80px;top:188.42px" class="cls_038"><span class="cls_038">Text corpus</span></div>
<div style="position:absolute;left:185.44px;top:210.06px" class="cls_018"><span class="cls_018">Figure 4.2: Process to extend the dataset</span></div>
<div style="position:absolute;left:72.00px;top:240.00px" class="cls_006"><span class="cls_006">4.3.2.1</span></div>
<div style="position:absolute;left:120.09px;top:240.00px" class="cls_006"><span class="cls_006">Read the file for valid URLs</span></div>
<div style="position:absolute;left:72.00px;top:272.87px" class="cls_006"><span class="cls_006">To read the valid URLs from file we do the following:</span></div>
<div style="position:absolute;left:72.59px;top:294.48px" class="cls_025"><span class="cls_025">For each URL</span></div>
<div style="position:absolute;left:94.59px;top:307.78px" class="cls_025"><span class="cls_025">Check  i f  the URL  starts  with</span></div>
<div style="position:absolute;left:268.24px;top:307.78px" class="cls_025"><span class="cls_025">h t t p</span></div>
<div style="position:absolute;left:311.25px;top:307.78px" class="cls_025"><span class="cls_025">.  If  yes ,  go to  next  step.</span></div>
<div style="position:absolute;left:459.98px;top:307.78px" class="cls_025"><span class="cls_025">Else ,</span></div>
<div style="position:absolute;left:98.72px;top:321.07px" class="cls_025"><span class="cls_025">consider URL invalid</span></div>
<div style="position:absolute;left:94.42px;top:334.36px" class="cls_025"><span class="cls_025">Match  the URL with  the  regular  expression :</span></div>
<div style="position:absolute;left:332.85px;top:334.36px" class="cls_025"><span class="cls_025">(http|https)://\w.∗</span></div>
<div style="position:absolute;left:94.59px;top:347.66px" class="cls_025"><span class="cls_025">Check  i f  the URL ends  with</span></div>
<div style="position:absolute;left:261.48px;top:347.66px" class="cls_025"><span class="cls_025">. pdf</span></div>
<div style="position:absolute;left:305.18px;top:347.66px" class="cls_025"><span class="cls_025">or</span></div>
<div style="position:absolute;left:338.89px;top:347.66px" class="cls_025"><span class="cls_025">. doc</span></div>
<div style="position:absolute;left:382.59px;top:347.66px" class="cls_025"><span class="cls_025">or</span></div>
<div style="position:absolute;left:416.30px;top:347.66px" class="cls_025"><span class="cls_025">. docx</span></div>
<div style="position:absolute;left:463.49px;top:347.66px" class="cls_025"><span class="cls_025">− We</span></div>
<div style="position:absolute;left:98.72px;top:360.95px" class="cls_025"><span class="cls_025">consider  these URLs invalid</span></div>
<div style="position:absolute;left:94.27px;top:374.24px" class="cls_025"><span class="cls_025">Remove  the  invalid  URLs</span></div>
<div style="position:absolute;left:72.69px;top:387.54px" class="cls_025"><span class="cls_025">Return  filtered  l i s t  of URLs</span></div>
<div style="position:absolute;left:190.23px;top:408.02px" class="cls_018"><span class="cls_018">Listing 4.2: Red the file for valid URLs</span></div>
<div style="position:absolute;left:72.00px;top:441.87px" class="cls_006"><span class="cls_006">The regular expression will match http or https and then look for a word character</span></div>
<div style="position:absolute;left:72.00px;top:458.30px" class="cls_006"><span class="cls_006">[a-zA-Z0-9 ]. This is because URLs starting with punctuations are invalid.</span></div>
<div style="position:absolute;left:72.00px;top:499.40px" class="cls_006"><span class="cls_006">4.3.2.2</span></div>
<div style="position:absolute;left:120.09px;top:499.40px" class="cls_006"><span class="cls_006">Create a list of all link texts, source URLs and target URLs</span></div>
<div style="position:absolute;left:72.00px;top:532.27px" class="cls_006"><span class="cls_006">Now that we have detected all the valid URLs in the file, we want to create a single</span></div>
<div style="position:absolute;left:72.00px;top:548.71px" class="cls_006"><span class="cls_006">consolidated list of the link text source link and target link. This list has the form:</span></div>
<div style="position:absolute;left:73.49px;top:570.31px" class="cls_025"><span class="cls_025">[</span></div>
<div style="position:absolute;left:73.49px;top:583.61px" class="cls_025"><span class="cls_025">[link text1 ,  source link1 ,  target link1],</span></div>
<div style="position:absolute;left:73.49px;top:596.90px" class="cls_025"><span class="cls_025">[link text2 ,  source link2 ,  target link2],</span></div>
<div style="position:absolute;left:73.49px;top:623.49px" class="cls_025"><span class="cls_025">]</span></div>
<div style="position:absolute;left:148.45px;top:643.97px" class="cls_018"><span class="cls_018">Listing 4.3: List of link texts, source links and target links</span></div>
<div style="position:absolute;left:72.00px;top:677.82px" class="cls_006"><span class="cls_006">We use the following process to create this list:</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:37488px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background53.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 4 Contextual link text detection</span></div>
<div style="position:absolute;left:513.00px;top:47.41px" class="cls_006"><span class="cls_006">37</span></div>
<div style="position:absolute;left:108.93px;top:81.88px" class="cls_025"><span class="cls_025">Create an empty  list</span></div>
<div style="position:absolute;left:108.59px;top:92.84px" class="cls_025"><span class="cls_025">For each  line  in  the  file</span></div>
<div style="position:absolute;left:131.76px;top:103.80px" class="cls_025"><span class="cls_025">If  line  starts  with</span></div>
<div style="position:absolute;left:252.83px;top:103.80px" class="cls_025"><span class="cls_025">U R L</span></div>
<div style="position:absolute;left:131.13px;top:114.76px" class="cls_025"><span class="cls_025">Extract  the  source  link</span></div>
<div style="position:absolute;left:130.76px;top:125.72px" class="cls_025"><span class="cls_025">While the  next  line  starts  with   MENTION</span></div>
<div style="position:absolute;left:153.25px;top:136.67px" class="cls_025"><span class="cls_025">Extract  the  link  text</span></div>
<div style="position:absolute;left:153.25px;top:147.63px" class="cls_025"><span class="cls_025">Extract  the  target  link</span></div>
<div style="position:absolute;left:152.48px;top:158.59px" class="cls_025"><span class="cls_025">Append</span></div>
<div style="position:absolute;left:192.43px;top:158.59px" class="cls_025"><span class="cls_025">[link text ,  source link ,  target link ]  to  the  list</span></div>
<div style="position:absolute;left:131.04px;top:169.55px" class="cls_025"><span class="cls_025">Else ,  proceed  with  the  next  line</span></div>
<div style="position:absolute;left:108.18px;top:180.51px" class="cls_025"><span class="cls_025">The  l i s t  contains  the  required  data</span></div>
<div style="position:absolute;left:192.23px;top:197.75px" class="cls_018"><span class="cls_018">Listing 4.4: Process to organize source and target data</span></div>
<div style="position:absolute;left:109.84px;top:232.06px" class="cls_025"><span class="cls_025">Initialize  a  list  final corpus</span></div>
<div style="position:absolute;left:278.59px;top:232.06px" class="cls_025"><span class="cls_025">= []</span></div>
<div style="position:absolute;left:108.59px;top:243.02px" class="cls_025"><span class="cls_025">For  i</span></div>
<div style="position:absolute;left:140.36px;top:243.02px" class="cls_025"><span class="cls_025">= 0 to  length  of  source dataset</span></div>
<div style="position:absolute;left:131.96px;top:253.98px" class="cls_025"><span class="cls_025">Initialize  a  list  corpus</span></div>
<div style="position:absolute;left:267.53px;top:253.98px" class="cls_025"><span class="cls_025">= []</span></div>
<div style="position:absolute;left:131.05px;top:264.94px" class="cls_025"><span class="cls_025">Scrape data  for  ith  source link</span></div>
<div style="position:absolute;left:130.54px;top:275.90px" class="cls_025"><span class="cls_025">Get  the  link  from  the  scraped  data  which  points  to  the  ith  t a r g e t  l i n k</span></div>
<div style="position:absolute;left:130.54px;top:286.86px" class="cls_025"><span class="cls_025">Get  the  parent  tag  of  the  link</span></div>
<div style="position:absolute;left:130.71px;top:297.81px" class="cls_025"><span class="cls_025">For words  in  the  parent  tag</span></div>
<div style="position:absolute;left:153.18px;top:308.77px" class="cls_025"><span class="cls_025">Set a counter</span></div>
<div style="position:absolute;left:228.83px;top:308.77px" class="cls_025"><span class="cls_025">= 0</span></div>
<div style="position:absolute;left:152.54px;top:319.73px" class="cls_025"><span class="cls_025">Read  words  until  we  find  the  ith  l i n k  t e x t  and  increase  counter</span></div>
<div style="position:absolute;left:128.35px;top:330.69px" class="cls_025"><span class="cls_025">word  by  word</span></div>
<div style="position:absolute;left:152.48px;top:341.65px" class="cls_025"><span class="cls_025">Append  the  words  to  the  corpus  l i s t</span></div>
<div style="position:absolute;left:153.87px;top:352.61px" class="cls_025"><span class="cls_025">If  link text  is  found and counter</span></div>
<div style="position:absolute;left:339.14px;top:352.61px" class="cls_025"><span class="cls_025">> window size  break out  of  the</span></div>
<div style="position:absolute;left:128.97px;top:363.57px" class="cls_025"><span class="cls_025">loop</span></div>
<div style="position:absolute;left:131.05px;top:374.53px" class="cls_025"><span class="cls_025">Scrape  the  data  for  the  ith  target  link</span></div>
<div style="position:absolute;left:130.54px;top:385.49px" class="cls_025"><span class="cls_025">Get  the  heading</span></div>
<div style="position:absolute;left:219.56px;top:385.49px" class="cls_025"><span class="cls_025">(h1</span></div>
<div style="position:absolute;left:241.65px;top:385.49px" class="cls_025"><span class="cls_025">tag  with  id:  firstHeading) and  store  it  in  the</span></div>
<div style="position:absolute;left:128.92px;top:396.44px" class="cls_025"><span class="cls_025">corpus  list</span></div>
<div style="position:absolute;left:130.54px;top:407.40px" class="cls_025"><span class="cls_025">Get  a l l  the  paragraphs  of  the  page</span></div>
<div style="position:absolute;left:130.71px;top:418.36px" class="cls_025"><span class="cls_025">For each paragraph</span></div>
<div style="position:absolute;left:153.18px;top:429.32px" class="cls_025"><span class="cls_025">Set a counter</span></div>
<div style="position:absolute;left:228.83px;top:429.32px" class="cls_025"><span class="cls_025">= 0</span></div>
<div style="position:absolute;left:152.82px;top:440.28px" class="cls_025"><span class="cls_025">For words  in  the  paragraph</span></div>
<div style="position:absolute;left:174.59px;top:451.24px" class="cls_025"><span class="cls_025">Append  the  words  to  the  corpus  l i s t  and  increment  counter</span></div>
<div style="position:absolute;left:175.99px;top:462.20px" class="cls_025"><span class="cls_025">If  counter</span></div>
<div style="position:absolute;left:234.08px;top:462.20px" class="cls_025"><span class="cls_025">> window size  break out  of  the  loop</span></div>
<div style="position:absolute;left:153.87px;top:473.16px" class="cls_025"><span class="cls_025">If  counter</span></div>
<div style="position:absolute;left:211.96px;top:473.16px" class="cls_025"><span class="cls_025">&lt; window size</span></div>
<div style="position:absolute;left:175.17px;top:484.12px" class="cls_025"><span class="cls_025">Continue with  the  next paragraph</span></div>
<div style="position:absolute;left:130.36px;top:495.07px" class="cls_025"><span class="cls_025">Append  corpus  to  final  corpus</span></div>
<div style="position:absolute;left:108.18px;top:506.03px" class="cls_025"><span class="cls_025">The  l i s t :  final  corpus  contains  a l l  the  required  data</span></div>
<div style="position:absolute;left:214.67px;top:523.27px" class="cls_018"><span class="cls_018">Listing 4.5: Algorithm to extend the dataset</span></div>
<div style="position:absolute;left:108.00px;top:564.11px" class="cls_006"><span class="cls_006">4.3.2.3</span></div>
<div style="position:absolute;left:156.09px;top:564.11px" class="cls_006"><span class="cls_006">Scrape the data of the source URL and target URL using the sliding</span></div>
<div style="position:absolute;left:156.09px;top:580.55px" class="cls_006"><span class="cls_006">window</span></div>
<div style="position:absolute;left:108.00px;top:613.42px" class="cls_006"><span class="cls_006">We have already created a list of all link texts, source URLs and target URLs. Let us</span></div>
<div style="position:absolute;left:108.00px;top:629.85px" class="cls_006"><span class="cls_006">name this list as source dataset. Now we have to create a text corpus by extracting</span></div>
<div style="position:absolute;left:108.00px;top:646.29px" class="cls_006"><span class="cls_006">a window of words around the source URL and the target URL. We use Beautiful-</span></div>
<div style="position:absolute;left:108.00px;top:662.73px" class="cls_006"><span class="cls_006">Soup Python library (40) for scraping purpose.  Also, we maintain 3 separate lists:</span></div>
<div style="position:absolute;left:108.00px;top:679.16px" class="cls_006"><span class="cls_006">link texts, source links, target links for utility.  The algorithm is described in</span></div>
<div style="position:absolute;left:108.00px;top:695.60px" class="cls_006"><span class="cls_006">Listing 4.5.</span></div>
<div style="position:absolute;left:108.00px;top:721.42px" class="cls_006"><span class="cls_006">Before adding the words to the corpus, we also run a text preprocessing pipeline on it</span></div>
<div style="position:absolute;left:108.00px;top:737.86px" class="cls_006"><span class="cls_006">to remove any data discrepancies. The pipeline is exactly similar to the one we used in</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:38340px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background54.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">38</span></div>
<div style="position:absolute;left:292.13px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 4 Contextual link text detection</span></div>
<div style="position:absolute;left:73.49px;top:81.88px" class="cls_025"><span class="cls_025">[</span></div>
<div style="position:absolute;left:73.49px;top:92.84px" class="cls_025"><span class="cls_025">[words  for  source link1 ,  link text1 ,  words  for  target link1],</span></div>
<div style="position:absolute;left:73.49px;top:103.80px" class="cls_025"><span class="cls_025">[words  for  source link2 ,  link text2 ,  words  for  target link2],</span></div>
<div style="position:absolute;left:73.49px;top:125.72px" class="cls_025"><span class="cls_025">]</span></div>
<div style="position:absolute;left:193.52px;top:142.95px" class="cls_018"><span class="cls_018">Listing 4.6: Final corpus data schema</span></div>
<div style="position:absolute;left:77.98px;top:174.35px" class="cls_006"><span class="cls_006">Dataset statistics</span></div>
<div style="position:absolute;left:381.07px;top:174.35px" class="cls_006"><span class="cls_006">Value</span></div>
<div style="position:absolute;left:77.98px;top:188.29px" class="cls_006"><span class="cls_006">Number of words</span></div>
<div style="position:absolute;left:381.07px;top:188.29px" class="cls_006"><span class="cls_006">3806212</span></div>
<div style="position:absolute;left:77.98px;top:202.24px" class="cls_006"><span class="cls_006">URLs scraped</span></div>
<div style="position:absolute;left:381.07px;top:202.24px" class="cls_006"><span class="cls_006">21058</span></div>
<div style="position:absolute;left:77.98px;top:216.19px" class="cls_006"><span class="cls_006">Vocabulary size</span></div>
<div style="position:absolute;left:381.08px;top:216.19px" class="cls_006"><span class="cls_006">38435</span></div>
<div style="position:absolute;left:179.55px;top:239.48px" class="cls_018"><span class="cls_018">Table 4.1: Statistics of the extended dataset</span></div>
<div style="position:absolute;left:72.00px;top:269.42px" class="cls_006"><span class="cls_006">section 3.3</span></div>
<div style="position:absolute;left:72.00px;top:295.24px" class="cls_006"><span class="cls_006">All the words for each source link, link text and target link combination is now</span></div>
<div style="position:absolute;left:72.00px;top:311.68px" class="cls_006"><span class="cls_006">stored in the list final corpus. The data in final corpus schema is shown in Listing</span></div>
<div style="position:absolute;left:72.00px;top:328.11px" class="cls_006"><span class="cls_006">4.6.</span></div>
<div style="position:absolute;left:72.00px;top:353.94px" class="cls_006"><span class="cls_006">This schema is consistent with the one described in Figure 4.2.</span></div>
<div style="position:absolute;left:72.00px;top:395.04px" class="cls_006"><span class="cls_006">4.3.2.4</span></div>
<div style="position:absolute;left:120.09px;top:395.04px" class="cls_006"><span class="cls_006">Store the data in the disk</span></div>
<div style="position:absolute;left:72.00px;top:427.91px" class="cls_006"><span class="cls_006">The above section discusses the method of scraping out the relevant data. But as already</span></div>
<div style="position:absolute;left:72.00px;top:444.35px" class="cls_006"><span class="cls_006">mentioned, the number of URLs to scrape is massive and the code cannot simply be</span></div>
<div style="position:absolute;left:72.00px;top:460.78px" class="cls_006"><span class="cls_006">run in a single iteration. This is why it was necessary to divide the work into batches</span></div>
<div style="position:absolute;left:72.00px;top:477.22px" class="cls_006"><span class="cls_006">and use multiple computing instances to run the code. Google collaboratory (11) allows</span></div>
<div style="position:absolute;left:72.00px;top:493.65px" class="cls_006"><span class="cls_006">us to create multiple instances of our code and run them parallelly on multiple CPUs.</span></div>
<div style="position:absolute;left:72.00px;top:510.09px" class="cls_006"><span class="cls_006">Another advantage of using this software is that it allows us to mount Google Drive into</span></div>
<div style="position:absolute;left:72.00px;top:526.52px" class="cls_006"><span class="cls_006">the development environment so that we can easily save and load the datasets generated</span></div>
<div style="position:absolute;left:72.00px;top:542.96px" class="cls_006"><span class="cls_006">without having to clunk our local systems.</span></div>
<div style="position:absolute;left:72.00px;top:568.79px" class="cls_006"><span class="cls_006">To parallelize the process, we set a batch size of 1000, i.e when 1000 URLs are scraped,</span></div>
<div style="position:absolute;left:72.00px;top:585.22px" class="cls_006"><span class="cls_006">we dump the data into a new pickle file in Google Drive. Also, we maintain 2 new vari-</span></div>
<div style="position:absolute;left:72.00px;top:601.66px" class="cls_006"><span class="cls_006">ables lower limit and upper limit to keep track of how much data has been scraped.</span></div>
<div style="position:absolute;left:72.00px;top:618.09px" class="cls_006"><span class="cls_006">The    filenames    that    are</span></div>
<div style="position:absolute;left:244.48px;top:618.09px" class="cls_006"><span class="cls_006">dumped    to    drive    follow    the    syntax:</span></div>
<div style="position:absolute;left:72.00px;top:634.53px" class="cls_006"><span class="cls_006">corpus lower limit to upper limit.pickle. We can use pickles here instead of HD5</span></div>
<div style="position:absolute;left:72.00px;top:650.96px" class="cls_006"><span class="cls_006">files (61) as we are only considering a batch size of 1000 and the file sizes are not generally</span></div>
<div style="position:absolute;left:72.00px;top:667.40px" class="cls_006"><span class="cls_006">greater than 500MB. Some statistics of the extended dataset are in table 4.1.</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:39192px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background55.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 4 Contextual link text detection</span></div>
<div style="position:absolute;left:513.00px;top:47.41px" class="cls_006"><span class="cls_006">39</span></div>
<div style="position:absolute;left:176.42px;top:82.11px" class="cls_039"><span class="cls_039">word</span><span class="cls_040"><sub>11</sub></span></div>
<div style="position:absolute;left:204.38px;top:82.11px" class="cls_039"><span class="cls_039">word</span><span class="cls_040"><sub>12</sub></span></div>
<div style="position:absolute;left:233.22px;top:82.11px" class="cls_041"><span class="cls_041">word</span><span class="cls_042"><sub>13</sub></span></div>
<div style="position:absolute;left:261.87px;top:82.11px" class="cls_039"><span class="cls_039">word</span><span class="cls_040"><sub>14</sub></span></div>
<div style="position:absolute;left:290.62px;top:82.11px" class="cls_039"><span class="cls_039">word</span><span class="cls_040"><sub>15</sub></span></div>
<div style="position:absolute;left:319.91px;top:82.11px" class="cls_039"><span class="cls_039">word</span><span class="cls_043"><sub>21</sub></span></div>
<div style="position:absolute;left:348.12px;top:82.11px" class="cls_039"><span class="cls_039">word</span><span class="cls_040"><sub>22</sub></span></div>
<div style="position:absolute;left:376.87px;top:82.11px" class="cls_039"><span class="cls_039">word</span><span class="cls_040"><sub>23</sub></span></div>
<div style="position:absolute;left:405.62px;top:82.11px" class="cls_039"><span class="cls_039">word</span><span class="cls_040"><sub>24</sub></span></div>
<div style="position:absolute;left:434.37px;top:82.11px" class="cls_039"><span class="cls_039">word</span><span class="cls_040"><sub>25</sub></span></div>
<div style="position:absolute;left:197.27px;top:122.36px" class="cls_039"><span class="cls_039">words within context to word</span><span class="cls_040"><sub>13</sub></span></div>
<div style="position:absolute;left:341.21px;top:122.36px" class="cls_039"><span class="cls_039">words out of context to word</span><span class="cls_040"><sub>13</sub></span></div>
<div style="position:absolute;left:195.28px;top:145.96px" class="cls_018"><span class="cls_018">Figure 4.3: Words within and out of context example</span></div>
<div style="position:absolute;left:108.00px;top:173.36px" class="cls_005"><span class="cls_005">4.4</span></div>
<div style="position:absolute;left:144.76px;top:173.36px" class="cls_005"><span class="cls_005">Building the model</span></div>
<div style="position:absolute;left:108.00px;top:212.62px" class="cls_002"><span class="cls_002">4.4.1</span></div>
<div style="position:absolute;left:149.10px;top:212.62px" class="cls_002"><span class="cls_002">Negative sampling</span></div>
<div style="position:absolute;left:108.00px;top:246.29px" class="cls_006"><span class="cls_006">Now that we have a specialized corpus for our problem, we can create our model. We</span></div>
<div style="position:absolute;left:108.00px;top:262.73px" class="cls_006"><span class="cls_006">consider a variation of the word2vec architecture (31) called negative sampling.  The</span></div>
<div style="position:absolute;left:108.00px;top:279.16px" class="cls_006"><span class="cls_006">idea here is to sample words randomly from the dataset.  These words can either lie</span></div>
<div style="position:absolute;left:108.00px;top:295.60px" class="cls_006"><span class="cls_006">within a specified window of words (context word) or not (negative samples). Consider</span></div>
<div style="position:absolute;left:108.00px;top:312.03px" class="cls_006"><span class="cls_006">the Figure 4.3. In this figure Word ij represents Word number j for link i</span></div>
<div style="position:absolute;left:108.00px;top:337.62px" class="cls_006"><span class="cls_006">If we consider word 13 as the target and a window size of 2 (2 words to the left and</span></div>
<div style="position:absolute;left:108.00px;top:354.05px" class="cls_006"><span class="cls_006">right), the words within the context and the words out of context will be as shown</span></div>
<div style="position:absolute;left:108.00px;top:370.49px" class="cls_006"><span class="cls_006">above. So, if we sample (word 11, word 13) they are a contextual pair, so they should</span></div>
<div style="position:absolute;left:108.00px;top:386.92px" class="cls_006"><span class="cls_006">be similar.  If we sample (word 11 and word 23), they are negative samples and are</span></div>
<div style="position:absolute;left:108.00px;top:403.36px" class="cls_006"><span class="cls_006">not similar.</span></div>
<div style="position:absolute;left:108.00px;top:443.40px" class="cls_002"><span class="cls_002">4.4.2</span></div>
<div style="position:absolute;left:149.10px;top:443.40px" class="cls_002"><span class="cls_002">Creating the training dataset</span></div>
<div style="position:absolute;left:108.00px;top:477.07px" class="cls_006"><span class="cls_006">There is a key difference in our corpus and the corpus like Google News (12), which</span></div>
<div style="position:absolute;left:108.00px;top:493.51px" class="cls_006"><span class="cls_006">are usually used to train word embedding networks. Usually, the corpus contains a list</span></div>
<div style="position:absolute;left:108.00px;top:509.95px" class="cls_006"><span class="cls_006">of sentences, and the sentences usually are considered together as one single corpus.</span></div>
<div style="position:absolute;left:108.00px;top:526.38px" class="cls_006"><span class="cls_006">However, the scheme of our final corpus as discussed in subsubsection 4.3.2.3 is:</span></div>
<div style="position:absolute;left:109.49px;top:547.47px" class="cls_025"><span class="cls_025">[</span></div>
<div style="position:absolute;left:109.49px;top:560.76px" class="cls_025"><span class="cls_025">[words  for  source link1 ,  link text1 ,  words  for  target link1],</span></div>
<div style="position:absolute;left:109.49px;top:574.06px" class="cls_025"><span class="cls_025">[words  for  source link2 ,  link text2 ,  words  for  target link2],</span></div>
<div style="position:absolute;left:109.49px;top:600.64px" class="cls_025"><span class="cls_025">]</span></div>
<div style="position:absolute;left:229.52px;top:620.86px" class="cls_018"><span class="cls_018">Listing 4.7: Final corpus data schema</span></div>
<div style="position:absolute;left:108.00px;top:653.69px" class="cls_006"><span class="cls_006">We can think of the above schema in a simplified manner as:</span></div>
<div style="position:absolute;left:109.49px;top:674.78px" class="cls_025"><span class="cls_025">[</span></div>
<div style="position:absolute;left:109.49px;top:688.07px" class="cls_025"><span class="cls_025">[words  associated  with  link1],</span></div>
<div style="position:absolute;left:109.49px;top:701.36px" class="cls_025"><span class="cls_025">[words  associated  with  link2],</span></div>
<div style="position:absolute;left:109.49px;top:727.95px" class="cls_025"><span class="cls_025">]</span></div>
<div style="position:absolute;left:207.08px;top:748.17px" class="cls_018"><span class="cls_018">Listing 4.8: Final corpus data schema simplified</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:40044px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background56.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">40</span></div>
<div style="position:absolute;left:292.13px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 4 Contextual link text detection</span></div>
<div style="position:absolute;left:132.41px;top:135.70px" class="cls_018"><span class="cls_018">Figure 4.4: Words within and out of context example with tokens</span></div>
<div style="position:absolute;left:72.00px;top:167.58px" class="cls_006"><span class="cls_006">Words that are associated with link1 should not be associated with link2. In Figure 4.3,</span></div>
<div style="position:absolute;left:72.00px;top:184.01px" class="cls_006"><span class="cls_006">if we consider a fixed window size of 2, word 15 will be associated with word 21, which</span></div>
<div style="position:absolute;left:72.00px;top:200.45px" class="cls_006"><span class="cls_006">is actually wrong. word 15 is the last word for the first link, while word 21 is the first</span></div>
<div style="position:absolute;left:72.00px;top:216.88px" class="cls_006"><span class="cls_006">word of the second link.  These links have no relationship between them, and so these</span></div>
<div style="position:absolute;left:72.00px;top:233.32px" class="cls_006"><span class="cls_006">words should not be associated together.</span></div>
<div style="position:absolute;left:72.00px;top:259.15px" class="cls_006"><span class="cls_006">One solution is adding special tokens (the word: specialtoken) between these links. If</span></div>
<div style="position:absolute;left:72.00px;top:275.58px" class="cls_006"><span class="cls_006">the window size is w, we add w number of specialtoken tokens between the last word</span></div>
<div style="position:absolute;left:72.00px;top:292.02px" class="cls_006"><span class="cls_006">of one link and the first word of the next link. Refer to Figure 4.4.</span></div>
<div style="position:absolute;left:72.00px;top:317.85px" class="cls_006"><span class="cls_006">This system is for handling the edge words corresponding to each link. Now, word 15 will</span></div>
<div style="position:absolute;left:72.00px;top:334.28px" class="cls_006"><span class="cls_006">no longer be associated with word 21. This is a simple solution, but with the following</span></div>
<div style="position:absolute;left:72.00px;top:350.72px" class="cls_006"><span class="cls_006">drawbacks:</span></div>
<div style="position:absolute;left:85.33px;top:388.50px" class="cls_006"><span class="cls_006">1. It increases the space complexity</span></div>
<div style="position:absolute;left:85.33px;top:413.90px" class="cls_006"><span class="cls_006">2. The words might learn association with the specialtoken word, which does not</span></div>
<div style="position:absolute;left:99.27px;top:430.34px" class="cls_006"><span class="cls_006">make sense</span></div>
<div style="position:absolute;left:72.00px;top:468.12px" class="cls_006"><span class="cls_006">The space complexity is not really a concerning factor, as we are saving the pickle files</span></div>
<div style="position:absolute;left:72.00px;top:484.56px" class="cls_006"><span class="cls_006">remotely, and not on our local systems. The second problem, however, deserves closer</span></div>
<div style="position:absolute;left:72.00px;top:500.99px" class="cls_006"><span class="cls_006">attention. One solution is to sample words based on the frequency by which they occur</span></div>
<div style="position:absolute;left:72.00px;top:517.43px" class="cls_006"><span class="cls_006">in the corpus. Keras (8) offers a method by which we can generate a word rank-based</span></div>
<div style="position:absolute;left:72.00px;top:533.86px" class="cls_006"><span class="cls_006">probabilistic sampling table. These probabilities are inversely related to the frequency</span></div>
<div style="position:absolute;left:72.00px;top:550.30px" class="cls_006"><span class="cls_006">of word; so words with higher frequency will be sampled less. So the model will sample</span></div>
<div style="position:absolute;left:72.00px;top:566.73px" class="cls_006"><span class="cls_006">specialtoken fewer number of times and this would avoid words being frequently linked</span></div>
<div style="position:absolute;left:72.00px;top:583.17px" class="cls_006"><span class="cls_006">to this token.</span></div>
<div style="position:absolute;left:72.00px;top:609.00px" class="cls_006"><span class="cls_006">Now that we have solved this challenge, we can proceed with creating the final training</span></div>
<div style="position:absolute;left:72.00px;top:625.43px" class="cls_006"><span class="cls_006">set of our model. Given the dataset, we sample target words according to the sampling</span></div>
<div style="position:absolute;left:72.00px;top:641.87px" class="cls_006"><span class="cls_006">table. Also for each target word, we sample context words or out-of-context words using</span></div>
<div style="position:absolute;left:72.00px;top:658.30px" class="cls_006"><span class="cls_006">the sampling table. Thus, we create a list of word pairs and each word pair is in context</span></div>
<div style="position:absolute;left:72.00px;top:674.74px" class="cls_006"><span class="cls_006">(label of 1) or out-of-context (label of 0). Only two words within the window size w will</span></div>
<div style="position:absolute;left:72.00px;top:691.17px" class="cls_006"><span class="cls_006">be considered in context. Some examples considering Figure 4.4 are shown in table 4.2</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:40896px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background57.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 4 Contextual link text detection</span></div>
<div style="position:absolute;left:513.00px;top:47.41px" class="cls_006"><span class="cls_006">41</span></div>
<div style="position:absolute;left:113.98px;top:78.96px" class="cls_006"><span class="cls_006">Word pair (X)</span></div>
<div style="position:absolute;left:292.30px;top:78.96px" class="cls_006"><span class="cls_006">Label (y)</span></div>
<div style="position:absolute;left:113.98px;top:92.91px" class="cls_006"><span class="cls_006">word 11, word 12</span></div>
<div style="position:absolute;left:292.29px;top:92.91px" class="cls_006"><span class="cls_006">1</span></div>
<div style="position:absolute;left:113.98px;top:106.86px" class="cls_006"><span class="cls_006">word 13, word 22</span></div>
<div style="position:absolute;left:292.29px;top:106.86px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:113.98px;top:120.80px" class="cls_006"><span class="cls_006">word 24, word 22</span></div>
<div style="position:absolute;left:292.29px;top:120.80px" class="cls_006"><span class="cls_006">1</span></div>
<div style="position:absolute;left:113.98px;top:134.75px" class="cls_006"><span class="cls_006">word 22, word 11</span></div>
<div style="position:absolute;left:292.29px;top:134.75px" class="cls_006"><span class="cls_006">0</span></div>
<div style="position:absolute;left:248.09px;top:158.04px" class="cls_018"><span class="cls_018">Table 4.2: Sample word pairs</span></div>
<div style="position:absolute;left:249.34px;top:184.93px" class="cls_044"><span class="cls_044">Sparse vectors of size (vocab_size, vocab_size)</span></div>
<div style="position:absolute;left:181.94px;top:201.00px" class="cls_045"><span class="cls_045">Word 1</span></div>
<div style="position:absolute;left:228.33px;top:214.02px" class="cls_044"><span class="cls_044">One hot encoding</span></div>
<div style="position:absolute;left:338.62px;top:214.02px" class="cls_044"><span class="cls_044">Embedding matrix</span></div>
<div style="position:absolute;left:181.94px;top:227.05px" class="cls_045"><span class="cls_045">Word 2</span></div>
<div style="position:absolute;left:183.48px;top:245.72px" class="cls_045"><span class="cls_045">Label</span></div>
<div style="position:absolute;left:231.28px;top:247.02px" class="cls_044"><span class="cls_044">update weights through back propagation</span></div>
<div style="position:absolute;left:383.94px;top:255.71px" class="cls_044"><span class="cls_044">Dense vectors of size</span></div>
<div style="position:absolute;left:383.94px;top:261.78px" class="cls_044"><span class="cls_044">(vocab_size, embedding_size)</span></div>
<div style="position:absolute;left:221.40px;top:292.18px" class="cls_045"><span class="cls_045">Error</span></div>
<div style="position:absolute;left:266.40px;top:292.18px" class="cls_044"><span class="cls_044">Sigmoid activation</span></div>
<div style="position:absolute;left:340.22px;top:292.18px" class="cls_044"><span class="cls_044">Cosine similarity</span></div>
<div style="position:absolute;left:201.24px;top:325.31px" class="cls_018"><span class="cls_018">Figure 4.5: Negative Sampling Model Architecture</span></div>
<div style="position:absolute;left:108.00px;top:356.14px" class="cls_002"><span class="cls_002">4.4.3</span></div>
<div style="position:absolute;left:149.10px;top:356.14px" class="cls_002"><span class="cls_002">The model architecture</span></div>
<div style="position:absolute;left:108.00px;top:390.06px" class="cls_006"><span class="cls_006">We have created our training set as pairs of words and their corresponding labels. The</span></div>
<div style="position:absolute;left:108.00px;top:406.49px" class="cls_006"><span class="cls_006">next step is to create the model architecture.</span></div>
<div style="position:absolute;left:108.00px;top:432.32px" class="cls_006"><span class="cls_006">The architecture of the model (51) is shown in Figure 4.5</span></div>
<div style="position:absolute;left:108.00px;top:458.15px" class="cls_006"><span class="cls_006">The model receives a pair of words as input.  This pair has an associated label with</span></div>
<div style="position:absolute;left:108.00px;top:474.58px" class="cls_006"><span class="cls_006">the (1 or 0).  These words are one-hot encoded into a sparse vector of size equal to</span></div>
<div style="position:absolute;left:108.00px;top:491.02px" class="cls_006"><span class="cls_006">the vocabulary size of our corpus.  These vectors then pass to the Embedding layer</span></div>
<div style="position:absolute;left:108.00px;top:507.45px" class="cls_006"><span class="cls_006">which is basically a matrix of dimensions (vocabulary size, embedding size). The</span></div>
<div style="position:absolute;left:108.00px;top:523.89px" class="cls_006"><span class="cls_006">embedding size considered is 256.  The embedding size is generally optimized through</span></div>
<div style="position:absolute;left:108.00px;top:540.32px" class="cls_006"><span class="cls_006">experimentation. Using the embedding matrix we look up the corresponding dense 256</span></div>
<div style="position:absolute;left:108.00px;top:556.76px" class="cls_006"><span class="cls_006">length dense vector representations of the target and context (or out-of-context) word.</span></div>
<div style="position:absolute;left:108.00px;top:582.59px" class="cls_006"><span class="cls_006">At the next step, we compute the cosine similarity (28) between these two vectors.  If</span></div>
<div style="position:absolute;left:108.00px;top:599.02px" class="cls_006"><span class="cls_006">the two vectors are similar, they will have a higher similarity score.  We then use the</span></div>
<div style="position:absolute;left:108.00px;top:615.46px" class="cls_006"><span class="cls_006">sigmoid activation function to smoothen out the similarity score between 0 and 1 (59).</span></div>
<div style="position:absolute;left:108.00px;top:641.29px" class="cls_006"><span class="cls_006">After obtaining the normalized cosine similarity score (between 0 and 1) we compute</span></div>
<div style="position:absolute;left:108.00px;top:657.72px" class="cls_006"><span class="cls_006">the cross-entropy loss (10).  This loss is then used in backpropagation and tuning the</span></div>
<div style="position:absolute;left:108.00px;top:674.16px" class="cls_006"><span class="cls_006">weights of the embedding matrix.</span></div>
<div style="position:absolute;left:108.00px;top:699.99px" class="cls_006"><span class="cls_006">We train the model like this for many epochs and monitor the cross-entropy loss. After</span></div>
<div style="position:absolute;left:108.00px;top:716.42px" class="cls_006"><span class="cls_006">the loss has converged we stop training the model.  At this point, we can extract the</span></div>
<div style="position:absolute;left:108.00px;top:732.86px" class="cls_006"><span class="cls_006">embedding matrix out and save it on the disk.  The embedding matrix contains the</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:41748px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background58.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">42</span></div>
<div style="position:absolute;left:292.13px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 4 Contextual link text detection</span></div>
<div style="position:absolute;left:77.98px;top:78.96px" class="cls_006"><span class="cls_006">Word</span></div>
<div style="position:absolute;left:173.11px;top:78.96px" class="cls_006"><span class="cls_006">Top 5 most similar words</span></div>
<div style="position:absolute;left:77.98px;top:92.91px" class="cls_006"><span class="cls_006">happy</span></div>
<div style="position:absolute;left:173.11px;top:92.91px" class="cls_006"><span class="cls_006">glad, alive, joy, funny, smile</span></div>
<div style="position:absolute;left:77.98px;top:106.86px" class="cls_006"><span class="cls_006">india</span></div>
<div style="position:absolute;left:173.11px;top:106.86px" class="cls_006"><span class="cls_006">indian, India, jammu, maharashtra, hindustan</span></div>
<div style="position:absolute;left:77.98px;top:120.80px" class="cls_006"><span class="cls_006">mumbai</span></div>
<div style="position:absolute;left:173.11px;top:120.80px" class="cls_006"><span class="cls_006">kolkata, hyderabad, madras, chennai, pune</span></div>
<div style="position:absolute;left:77.98px;top:134.75px" class="cls_006"><span class="cls_006">england</span></div>
<div style="position:absolute;left:173.12px;top:134.75px" class="cls_006"><span class="cls_006">wales, essex, scotland, yorkshire, london</span></div>
<div style="position:absolute;left:77.98px;top:148.70px" class="cls_006"><span class="cls_006">obama</span></div>
<div style="position:absolute;left:173.12px;top:148.70px" class="cls_006"><span class="cls_006">barack, romney, sinclair, President Obama, Barack Obama</span></div>
<div style="position:absolute;left:77.98px;top:162.65px" class="cls_006"><span class="cls_006">football</span></div>
<div style="position:absolute;left:173.11px;top:162.65px" class="cls_006"><span class="cls_006">basketball, uefa, coach, hockey, soccer</span></div>
<div style="position:absolute;left:77.98px;top:176.59px" class="cls_006"><span class="cls_006">google</span></div>
<div style="position:absolute;left:173.12px;top:176.59px" class="cls_006"><span class="cls_006">youtube, blogs, doubleclick, facebook, internet</span></div>
<div style="position:absolute;left:166.10px;top:199.88px" class="cls_018"><span class="cls_018">Table 4.3: Some results of the learned embeddings</span></div>
<div style="position:absolute;left:72.00px;top:231.76px" class="cls_006"><span class="cls_006">learned vector representations of all words in our vocabulary.  These are our learned</span></div>
<div style="position:absolute;left:72.00px;top:248.19px" class="cls_006"><span class="cls_006">embeddings.</span></div>
<div style="position:absolute;left:72.00px;top:288.72px" class="cls_002"><span class="cls_002">4.4.4</span></div>
<div style="position:absolute;left:113.10px;top:288.72px" class="cls_002"><span class="cls_002">Learned embeddings results</span></div>
<div style="position:absolute;left:72.00px;top:322.64px" class="cls_006"><span class="cls_006">Using these embeddings we can plot the words in space using dimensionality reduction</span></div>
<div style="position:absolute;left:72.00px;top:339.08px" class="cls_006"><span class="cls_006">techniques like t-SNE (53) and also find the most similar words to a given word using</span></div>
<div style="position:absolute;left:72.00px;top:355.51px" class="cls_006"><span class="cls_006">Cosine Similarity metric (28). The table 4.3 shows some results of most similar words to</span></div>
<div style="position:absolute;left:72.00px;top:371.95px" class="cls_006"><span class="cls_006">some random words, and the words which the model has learned to be in context seem</span></div>
<div style="position:absolute;left:72.00px;top:388.38px" class="cls_006"><span class="cls_006">correct.</span></div>
<div style="position:absolute;left:72.00px;top:432.53px" class="cls_005"><span class="cls_005">4.5</span></div>
<div style="position:absolute;left:108.76px;top:432.53px" class="cls_005"><span class="cls_005">Evaluating the model</span></div>
<div style="position:absolute;left:72.00px;top:472.60px" class="cls_006"><span class="cls_006">As discussed previously, the output of the model is a matrix of learned vector embed-</span></div>
<div style="position:absolute;left:72.00px;top:489.04px" class="cls_006"><span class="cls_006">dings.  Using these embeddings, we can now detect the similarity between the source</span></div>
<div style="position:absolute;left:72.00px;top:505.47px" class="cls_006"><span class="cls_006">link data, source link text, and target hyperlinks and determine if they are contextual</span></div>
<div style="position:absolute;left:72.00px;top:521.91px" class="cls_006"><span class="cls_006">or not.</span></div>
<div style="position:absolute;left:72.00px;top:547.74px" class="cls_006"><span class="cls_006">We have already discussed in subsection 3.8.5, that Word Movers Distance (WMD)</span></div>
<div style="position:absolute;left:72.00px;top:564.17px" class="cls_006"><span class="cls_006">(23) is a metric that can give us a similarity score based on the distances between the</span></div>
<div style="position:absolute;left:72.00px;top:580.61px" class="cls_006"><span class="cls_006">documents, incorporating the features of word2vec (31).</span></div>
<div style="position:absolute;left:72.00px;top:606.44px" class="cls_006"><span class="cls_006">We assume for the remaining part of the document that the source link words be referred</span></div>
<div style="position:absolute;left:72.00px;top:622.87px" class="cls_006"><span class="cls_006">to as source text, the words in the hyperlink be referred to as source link text and</span></div>
<div style="position:absolute;left:72.00px;top:639.31px" class="cls_006"><span class="cls_006">the target link words be referred to as target text</span></div>
<div style="position:absolute;left:72.00px;top:679.84px" class="cls_002"><span class="cls_002">4.5.1</span></div>
<div style="position:absolute;left:113.10px;top:679.84px" class="cls_002"><span class="cls_002">Evaluating similarity between the source text and target text</span></div>
<div style="position:absolute;left:72.90px;top:709.54px" class="cls_025"><span class="cls_025">Normalize  the word2vec embedded vectors</span></div>
<div style="position:absolute;left:72.59px;top:722.83px" class="cls_025"><span class="cls_025">For each  source and target  text  in  the  test  dataset</span></div>
<div style="position:absolute;left:94.89px;top:736.12px" class="cls_025"><span class="cls_025">Convert  all  words to  lowercase</span></div>
<div style="position:absolute;left:94.27px;top:749.42px" class="cls_025"><span class="cls_025">Remove  a l l  stopwords</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:42600px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background59.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 4 Contextual link text detection</span></div>
<div style="position:absolute;left:513.00px;top:47.41px" class="cls_006"><span class="cls_006">43</span></div>
<div style="position:absolute;left:113.98px;top:78.96px" class="cls_006"><span class="cls_006">Corpus used</span></div>
<div style="position:absolute;left:209.12px;top:78.96px" class="cls_006"><span class="cls_006">Corresponding</span></div>
<div style="position:absolute;left:304.25px;top:78.96px" class="cls_006"><span class="cls_006">Mean of WMD   Median</span></div>
<div style="position:absolute;left:472.46px;top:78.96px" class="cls_006"><span class="cls_006">of</span></div>
<div style="position:absolute;left:209.11px;top:92.51px" class="cls_006"><span class="cls_006">source and tar-</span></div>
<div style="position:absolute;left:399.39px;top:92.51px" class="cls_006"><span class="cls_006">WMD</span></div>
<div style="position:absolute;left:209.11px;top:106.06px" class="cls_006"><span class="cls_006">get (Yes/No)</span></div>
<div style="position:absolute;left:113.98px;top:120.01px" class="cls_006"><span class="cls_006">Generated corpus   Yes</span></div>
<div style="position:absolute;left:304.25px;top:120.01px" class="cls_006"><span class="cls_006">1.08</span></div>
<div style="position:absolute;left:399.39px;top:120.01px" class="cls_006"><span class="cls_006">1.10</span></div>
<div style="position:absolute;left:113.98px;top:133.95px" class="cls_006"><span class="cls_006">Google News</span></div>
<div style="position:absolute;left:209.12px;top:133.95px" class="cls_006"><span class="cls_006">Yes</span></div>
<div style="position:absolute;left:304.25px;top:133.95px" class="cls_006"><span class="cls_006">1.11</span></div>
<div style="position:absolute;left:399.39px;top:133.95px" class="cls_006"><span class="cls_006">1.13</span></div>
<div style="position:absolute;left:113.98px;top:147.90px" class="cls_006"><span class="cls_006">Generated corpus</span></div>
<div style="position:absolute;left:209.11px;top:147.90px" class="cls_006"><span class="cls_006">No</span></div>
<div style="position:absolute;left:304.25px;top:147.90px" class="cls_006"><span class="cls_006">1.23</span></div>
<div style="position:absolute;left:399.38px;top:147.90px" class="cls_006"><span class="cls_006">1.24</span></div>
<div style="position:absolute;left:113.98px;top:161.85px" class="cls_006"><span class="cls_006">Google News</span></div>
<div style="position:absolute;left:209.12px;top:161.85px" class="cls_006"><span class="cls_006">No</span></div>
<div style="position:absolute;left:304.25px;top:161.85px" class="cls_006"><span class="cls_006">1.22</span></div>
<div style="position:absolute;left:399.39px;top:161.85px" class="cls_006"><span class="cls_006">1.23</span></div>
<div style="position:absolute;left:163.09px;top:185.14px" class="cls_018"><span class="cls_018">Table 4.4: Results for similarity between source text and target text</span></div>
<div style="position:absolute;left:130.35px;top:218.95px" class="cls_025"><span class="cls_025">Compute  the WMD s i m i l a r i t y</span></div>
<div style="position:absolute;left:131.22px;top:232.25px" class="cls_025"><span class="cls_025">Store  this  similarity  score  in a  list</span></div>
<div style="position:absolute;left:108.23px;top:245.54px" class="cls_025"><span class="cls_025">Compute  the  mean  of  the  s i m i l a r i t y  scores</span></div>
<div style="position:absolute;left:151.45px;top:266.02px" class="cls_018"><span class="cls_018">Listing 4.9: Evaluating similarity between the source text and target text</span></div>
<div style="position:absolute;left:108.00px;top:299.87px" class="cls_006"><span class="cls_006">This algorithm is exactly the same as the one discussed in subsection 3.8.6, and thus</span></div>
<div style="position:absolute;left:108.00px;top:316.31px" class="cls_006"><span class="cls_006">promotes a lot of code re-usability. This algorithm can be used because the source and</span></div>
<div style="position:absolute;left:108.00px;top:332.74px" class="cls_006"><span class="cls_006">target text will have comparable lengths and can be considered as two documents and</span></div>
<div style="position:absolute;left:108.00px;top:349.18px" class="cls_006"><span class="cls_006">Word Movers Distance can thus be directly computed on them.</span></div>
<div style="position:absolute;left:108.00px;top:375.01px" class="cls_006"><span class="cls_006">Using this algorithm we can figure out, that given a source text, does it make sense for</span></div>
<div style="position:absolute;left:108.00px;top:391.44px" class="cls_006"><span class="cls_006">that text to contain a link that points to the target text or not.  This is not directly</span></div>
<div style="position:absolute;left:108.00px;top:407.88px" class="cls_006"><span class="cls_006">linked to the WCAG guideline G91 (19), but if the source text and target text are not</span></div>
<div style="position:absolute;left:108.00px;top:424.31px" class="cls_006"><span class="cls_006">related, then the hyperlink should not ideally exist and can be ignored by screen readers.</span></div>
<div style="position:absolute;left:108.00px;top:440.75px" class="cls_006"><span class="cls_006">That way, information clutter can be reduced and a much smoother user experience can</span></div>
<div style="position:absolute;left:108.00px;top:457.18px" class="cls_006"><span class="cls_006">be provided.</span></div>
<div style="position:absolute;left:108.00px;top:483.01px" class="cls_006"><span class="cls_006">First, we run the model to find WMD between the source text and the corresponding</span></div>
<div style="position:absolute;left:108.00px;top:499.45px" class="cls_006"><span class="cls_006">target text. The WMD score should ideally be low in this case. Then, we also run the</span></div>
<div style="position:absolute;left:108.00px;top:515.88px" class="cls_006"><span class="cls_006">model to find WMD between the source text but not the corresponding target text, but</span></div>
<div style="position:absolute;left:108.00px;top:532.32px" class="cls_006"><span class="cls_006">some other randomly sampled target texts. So, we would expect the WMD score to be</span></div>
<div style="position:absolute;left:108.00px;top:548.75px" class="cls_006"><span class="cls_006">higher. We perform the above process for the model trained on our text corpus and also</span></div>
<div style="position:absolute;left:108.00px;top:565.19px" class="cls_006"><span class="cls_006">on the word2vec model trained on the Google news dataset (12). The results are shown</span></div>
<div style="position:absolute;left:108.00px;top:581.62px" class="cls_006"><span class="cls_006">in table 4.4</span></div>
<div style="position:absolute;left:108.00px;top:607.45px" class="cls_006"><span class="cls_006">We have considered both the mean and median scores as, for some documents the WMD</span></div>
<div style="position:absolute;left:108.00px;top:623.89px" class="cls_006"><span class="cls_006">scores may be too high, and we do not want these outliers to have a huge impact on the</span></div>
<div style="position:absolute;left:108.00px;top:640.32px" class="cls_006"><span class="cls_006">analysis. Medians are less sensitive to outliers.</span></div>
<div style="position:absolute;left:108.00px;top:666.15px" class="cls_006"><span class="cls_006">An important fact to note here is that we are dealing with an unsupervised learning</span></div>
<div style="position:absolute;left:108.00px;top:682.58px" class="cls_006"><span class="cls_006">task and we do not have a fixed, established metric to compare the two approaches used.</span></div>
<div style="position:absolute;left:108.00px;top:699.02px" class="cls_006"><span class="cls_006">But we can definitely conclude from the above table that the results on the generated</span></div>
<div style="position:absolute;left:108.00px;top:715.46px" class="cls_006"><span class="cls_006">corpus look promising. There exists a significant difference in both the mean and median</span></div>
<div style="position:absolute;left:108.00px;top:731.89px" class="cls_006"><span class="cls_006">scores for our method when we are evaluating for corresponding source and target texts.</span></div>
<div style="position:absolute;left:108.00px;top:748.33px" class="cls_006"><span class="cls_006">This suggests that the model is good at detecting the similar source and target texts for</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:43452px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background60.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">44</span></div>
<div style="position:absolute;left:292.13px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 4 Contextual link text detection</span></div>
<div style="position:absolute;left:77.98px;top:78.96px" class="cls_006"><span class="cls_006">Corpus used</span></div>
<div style="position:absolute;left:173.12px;top:78.96px" class="cls_006"><span class="cls_006">Corresponding</span></div>
<div style="position:absolute;left:268.25px;top:78.96px" class="cls_006"><span class="cls_006">Mean of WMD   Median</span></div>
<div style="position:absolute;left:436.46px;top:78.96px" class="cls_006"><span class="cls_006">of</span></div>
<div style="position:absolute;left:173.11px;top:92.51px" class="cls_006"><span class="cls_006">source and tar-</span></div>
<div style="position:absolute;left:363.39px;top:92.51px" class="cls_006"><span class="cls_006">WMD</span></div>
<div style="position:absolute;left:173.11px;top:106.06px" class="cls_006"><span class="cls_006">get (Yes/No)</span></div>
<div style="position:absolute;left:77.98px;top:120.01px" class="cls_006"><span class="cls_006">Generated corpus   Yes</span></div>
<div style="position:absolute;left:268.25px;top:120.01px" class="cls_006"><span class="cls_006">1.21</span></div>
<div style="position:absolute;left:363.39px;top:120.01px" class="cls_006"><span class="cls_006">1.25</span></div>
<div style="position:absolute;left:77.98px;top:133.95px" class="cls_006"><span class="cls_006">Google News</span></div>
<div style="position:absolute;left:173.12px;top:133.95px" class="cls_006"><span class="cls_006">Yes</span></div>
<div style="position:absolute;left:268.25px;top:133.95px" class="cls_006"><span class="cls_006">1.27</span></div>
<div style="position:absolute;left:363.39px;top:133.95px" class="cls_006"><span class="cls_006">1.32</span></div>
<div style="position:absolute;left:77.98px;top:147.90px" class="cls_006"><span class="cls_006">Generated corpus</span></div>
<div style="position:absolute;left:173.11px;top:147.90px" class="cls_006"><span class="cls_006">No</span></div>
<div style="position:absolute;left:268.25px;top:147.90px" class="cls_006"><span class="cls_006">1.43</span></div>
<div style="position:absolute;left:363.38px;top:147.90px" class="cls_006"><span class="cls_006">1.42</span></div>
<div style="position:absolute;left:77.98px;top:161.85px" class="cls_006"><span class="cls_006">Google News</span></div>
<div style="position:absolute;left:173.12px;top:161.85px" class="cls_006"><span class="cls_006">No</span></div>
<div style="position:absolute;left:268.25px;top:161.85px" class="cls_006"><span class="cls_006">1.45</span></div>
<div style="position:absolute;left:363.39px;top:161.85px" class="cls_006"><span class="cls_006">1.42</span></div>
<div style="position:absolute;left:116.68px;top:185.14px" class="cls_018"><span class="cls_018">Table 4.5: Results for similarity between source link text and source text</span></div>
<div style="position:absolute;left:72.00px;top:217.01px" class="cls_006"><span class="cls_006">hyperlinks. However when we feed the model random target texts for source texts the</span></div>
<div style="position:absolute;left:72.00px;top:233.45px" class="cls_006"><span class="cls_006">average WMD scores increase, as expected, but there is not much difference from that</span></div>
<div style="position:absolute;left:72.00px;top:249.88px" class="cls_006"><span class="cls_006">of the model pre-trained on Google News dataset.</span></div>
<div style="position:absolute;left:72.00px;top:290.41px" class="cls_002"><span class="cls_002">4.5.2</span></div>
<div style="position:absolute;left:113.10px;top:290.41px" class="cls_002"><span class="cls_002">Evaluating similarity between the source link text and target</span></div>
<div style="position:absolute;left:113.10px;top:307.33px" class="cls_002"><span class="cls_002">text/source text</span></div>
<div style="position:absolute;left:72.00px;top:341.25px" class="cls_006"><span class="cls_006">To evaluate the similarity between the source link text and target text/source text, we</span></div>
<div style="position:absolute;left:72.00px;top:357.69px" class="cls_006"><span class="cls_006">use the same algorithm as discussed in the previous section.  However, since the link</span></div>
<div style="position:absolute;left:72.00px;top:374.12px" class="cls_006"><span class="cls_006">texts are generally shorter than the source or target texts, we expect the WMD score to</span></div>
<div style="position:absolute;left:72.00px;top:390.56px" class="cls_006"><span class="cls_006">be higher.</span></div>
<div style="position:absolute;left:72.00px;top:416.39px" class="cls_006"><span class="cls_006">From the table 4.5, we can observe that the average WMD scores are higher, as expected.</span></div>
<div style="position:absolute;left:72.00px;top:432.82px" class="cls_006"><span class="cls_006">There exists a significant difference in both the mean and median scores for our method</span></div>
<div style="position:absolute;left:72.00px;top:449.26px" class="cls_006"><span class="cls_006">when we are evaluating for corresponding link texts and source texts. This suggests that</span></div>
<div style="position:absolute;left:72.00px;top:465.69px" class="cls_006"><span class="cls_006">the model is good at detecting the similar link texts and source texts. However when we</span></div>
<div style="position:absolute;left:72.00px;top:482.13px" class="cls_006"><span class="cls_006">feed the model random target texts for source texts the average WMD scores increase, as</span></div>
<div style="position:absolute;left:72.00px;top:498.56px" class="cls_006"><span class="cls_006">expected, but the pre-trained model on Google News dataset performs slightly better.</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:44304px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background61.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:160.10px" class="cls_003"><span class="cls_003">Chapter 5</span></div>
<div style="position:absolute;left:108.00px;top:221.55px" class="cls_017"><span class="cls_017">Conclusion</span></div>
<div style="position:absolute;left:108.00px;top:302.99px" class="cls_005"><span class="cls_005">5.1</span></div>
<div style="position:absolute;left:144.76px;top:302.99px" class="cls_005"><span class="cls_005">Reflection</span></div>
<div style="position:absolute;left:108.00px;top:343.06px" class="cls_006"><span class="cls_006">For the Automatic Image Captioning task, we have created a model for automatically</span></div>
<div style="position:absolute;left:108.00px;top:359.49px" class="cls_006"><span class="cls_006">captioning images based on research and experimentation.  Additionally, word embed-</span></div>
<div style="position:absolute;left:108.00px;top:375.93px" class="cls_006"><span class="cls_006">ding based similarity metrics have been applied to evaluate the performance of the</span></div>
<div style="position:absolute;left:108.00px;top:392.36px" class="cls_006"><span class="cls_006">model. Through this, we have tried to capture not only word-to-word matches but con-</span></div>
<div style="position:absolute;left:108.00px;top:408.80px" class="cls_006"><span class="cls_006">text and synonyms in the real and generated captions. This metric has been extended</span></div>
<div style="position:absolute;left:108.00px;top:425.23px" class="cls_006"><span class="cls_006">and a method has been developed to detect image-text-similarity in web pages. Exten-</span></div>
<div style="position:absolute;left:108.00px;top:441.67px" class="cls_006"><span class="cls_006">sions like object detection and how it can be applied in web accessibility domain has</span></div>
<div style="position:absolute;left:108.00px;top:458.10px" class="cls_006"><span class="cls_006">also been addressed.  Through this project, a wide variety of AI techniques have been</span></div>
<div style="position:absolute;left:108.00px;top:474.54px" class="cls_006"><span class="cls_006">explored to help improve the experience on the web for the disabled.</span></div>
<div style="position:absolute;left:108.00px;top:500.37px" class="cls_006"><span class="cls_006">For the Contextual hyperlink detection task, we created our own corpus and the per-</span></div>
<div style="position:absolute;left:108.00px;top:516.80px" class="cls_006"><span class="cls_006">formance of a word2vec model on that corpus was compared against that of pre-trained</span></div>
<div style="position:absolute;left:108.00px;top:533.24px" class="cls_006"><span class="cls_006">embeddings. The results for the model created on the generated dataset look promising.</span></div>
<div style="position:absolute;left:108.00px;top:549.67px" class="cls_006"><span class="cls_006">An important fact to note is that the pre-trained model we compare it to has been</span></div>
<div style="position:absolute;left:108.00px;top:566.11px" class="cls_006"><span class="cls_006">trained on a much larger corpus.  As we extend our dataset more and more, it should</span></div>
<div style="position:absolute;left:108.00px;top:582.54px" class="cls_006"><span class="cls_006">capture further complexities and the performance should increase. However, while the</span></div>
<div style="position:absolute;left:108.00px;top:598.98px" class="cls_006"><span class="cls_006">models have been compared using WMD score, we cannot say that our model is better</span></div>
<div style="position:absolute;left:108.00px;top:615.41px" class="cls_006"><span class="cls_006">than the pre-trained one, because this is an unsupervised task, and there are no true</span></div>
<div style="position:absolute;left:108.00px;top:631.85px" class="cls_006"><span class="cls_006">labels in the dataset for us to compare with.  But, one can definitely see that there is</span></div>
<div style="position:absolute;left:108.00px;top:648.28px" class="cls_006"><span class="cls_006">future scope of applying DL and NLP techniques to this problem, and also there is a</span></div>
<div style="position:absolute;left:108.00px;top:664.72px" class="cls_006"><span class="cls_006">scope of developing further specialized datasets for this purpose.</span></div>
<div style="position:absolute;left:108.00px;top:690.55px" class="cls_006"><span class="cls_006">AI and web accessibility has largely been an unrelated field.  With this project, I sin-</span></div>
<div style="position:absolute;left:108.00px;top:706.98px" class="cls_006"><span class="cls_006">cerely hope to motivate others to apply these technologies in this field, and help people</span></div>
<div style="position:absolute;left:108.00px;top:723.42px" class="cls_006"><span class="cls_006">experience the web better.</span></div>
<div style="position:absolute;left:310.50px;top:773.49px" class="cls_006"><span class="cls_006">45</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:45156px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background62.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">46</span></div>
<div style="position:absolute;left:384.19px;top:47.41px" class="cls_006"><span class="cls_006">Chapter 5 Conclusion</span></div>
<div style="position:absolute;left:72.00px;top:77.00px" class="cls_005"><span class="cls_005">5.2</span></div>
<div style="position:absolute;left:108.76px;top:77.00px" class="cls_005"><span class="cls_005">Further Research</span></div>
<div style="position:absolute;left:72.00px;top:117.07px" class="cls_006"><span class="cls_006">One of the motivations for doing this project has been to set a good foundation of</span></div>
<div style="position:absolute;left:72.00px;top:133.50px" class="cls_006"><span class="cls_006">applying certain AI techniques in the domain of web accessibility, so that it can encourage</span></div>
<div style="position:absolute;left:72.00px;top:149.94px" class="cls_006"><span class="cls_006">future work. Certain possible modifications and extensions to this project are discussed</span></div>
<div style="position:absolute;left:72.00px;top:166.37px" class="cls_006"><span class="cls_006">below:</span></div>
<div style="position:absolute;left:85.33px;top:204.16px" class="cls_006"><span class="cls_006">1. The Automatic Image Captioning module can be made to output a confidence</span></div>
<div style="position:absolute;left:99.27px;top:220.59px" class="cls_006"><span class="cls_006">score for the generated caption for better user experience</span></div>
<div style="position:absolute;left:85.33px;top:245.99px" class="cls_006"><span class="cls_006">2. Like WMD, other methods to compute document similarity like Doc2Vec, which</span></div>
<div style="position:absolute;left:99.27px;top:262.43px" class="cls_006"><span class="cls_006">is an extension of word2vec (31) can be considered and compared with the current</span></div>
<div style="position:absolute;left:99.27px;top:278.86px" class="cls_006"><span class="cls_006">system</span></div>
<div style="position:absolute;left:85.33px;top:304.27px" class="cls_006"><span class="cls_006">3. All the techniques discussed can be integrated together into a complete web solu-</span></div>
<div style="position:absolute;left:99.27px;top:320.70px" class="cls_006"><span class="cls_006">tion</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:46008px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background63.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:161.75px" class="cls_017"><span class="cls_017">Bibliography</span></div>
<div style="position:absolute;left:113.46px;top:231.91px" class="cls_006"><span class="cls_006">[1]</span></div>
<div style="position:absolute;left:130.42px;top:231.91px" class="cls_006"><span class="cls_006">Y. Bengio, P. Simard, and P. Frasconi, Learning Long-Term Dependencies</span></div>
<div style="position:absolute;left:130.42px;top:248.35px" class="cls_006"><span class="cls_006">with Gradient Descent is Difficult, IEEE Transactions on Neural Networks, 5 (1994),</span></div>
<div style="position:absolute;left:130.42px;top:264.78px" class="cls_006"><span class="cls_006">pp. 157-166.</span></div>
<div style="position:absolute;left:113.46px;top:289.61px" class="cls_006"><span class="cls_006">[2]</span></div>
<div style="position:absolute;left:130.42px;top:289.61px" class="cls_006"><span class="cls_006">S. Bird, L. Edward, and K. Ewan, Natural Language Processing with Python,</span></div>
<div style="position:absolute;left:130.42px;top:306.04px" class="cls_006"><span class="cls_006">O’Reilly Media Inc., 2009.</span></div>
<div style="position:absolute;left:113.45px;top:330.87px" class="cls_006"><span class="cls_006">[3]</span></div>
<div style="position:absolute;left:130.42px;top:330.87px" class="cls_006"><span class="cls_006">J. Brownlee, Caption generation with the inject and merge encoder-decoder mod-</span></div>
<div style="position:absolute;left:130.42px;top:347.31px" class="cls_006"><span class="cls_006">els, 2017.</span></div>
<div style="position:absolute;left:113.45px;top:372.13px" class="cls_006"><span class="cls_006">[4]</span></div>
<div style="position:absolute;left:153.34px;top:372.13px" class="cls_006"><span class="cls_006">, A gentle introduction to calculating the bleu score for text in python, 2017.</span></div>
<div style="position:absolute;left:113.46px;top:396.96px" class="cls_006"><span class="cls_006">[5]</span></div>
<div style="position:absolute;left:153.34px;top:396.96px" class="cls_006"><span class="cls_006">, A gentle introduction to transfer learning for deep learning, 2017.</span></div>
<div style="position:absolute;left:113.46px;top:421.79px" class="cls_006"><span class="cls_006">[6]</span></div>
<div style="position:absolute;left:153.34px;top:421.79px" class="cls_006"><span class="cls_006">, How to clean text for machine learning with python, 2017.</span></div>
<div style="position:absolute;left:113.46px;top:446.62px" class="cls_006"><span class="cls_006">[7]</span></div>
<div style="position:absolute;left:130.42px;top:445.71px" class="cls_006"><span class="cls_006">K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares,</span></div>
<div style="position:absolute;left:130.42px;top:463.05px" class="cls_006"><span class="cls_006">H.  Schwenk,  and  Y.  Bengio,  Learning  phrase  representations  using  RNN</span></div>
<div style="position:absolute;left:130.42px;top:479.49px" class="cls_006"><span class="cls_006">encoder-decoder for statistical machine translation, EMNLP 2014 - 2014 Confer-</span></div>
<div style="position:absolute;left:130.42px;top:495.92px" class="cls_006"><span class="cls_006">ence on Empirical Methods in Natural Language Processing, Proceedings of the</span></div>
<div style="position:absolute;left:130.42px;top:512.36px" class="cls_006"><span class="cls_006">Conference, (2014), pp. 1724-1734.</span></div>
<div style="position:absolute;left:113.46px;top:537.18px" class="cls_006"><span class="cls_006">[8]</span></div>
<div style="position:absolute;left:130.42px;top:537.18px" class="cls_006"><span class="cls_006">F. Chollet et al., Keras. </span><A HREF="https://keras.io">https://keras.io</A>, 2015.</div>
<div style="position:absolute;left:113.46px;top:562.01px" class="cls_006"><span class="cls_006">[9]</span></div>
<div style="position:absolute;left:130.42px;top:562.01px" class="cls_006"><span class="cls_006">V. Dumbrav, Using Probability  Impact Matrix in Analysis and Risk Assessment</span></div>
<div style="position:absolute;left:130.42px;top:578.45px" class="cls_006"><span class="cls_006">Projects, tech. rep., 2013.</span></div>
<div style="position:absolute;left:108.00px;top:603.27px" class="cls_006"><span class="cls_006">[10]</span></div>
<div style="position:absolute;left:130.42px;top:603.27px" class="cls_006"><span class="cls_006">B. Fortuner, M. Viana, and B. Kowshik, Loss Functions  ML Cheatsheet</span></div>
<div style="position:absolute;left:130.42px;top:619.71px" class="cls_006"><span class="cls_006">documentation, 2018.</span></div>
<div style="position:absolute;left:108.00px;top:644.54px" class="cls_006"><span class="cls_006">[11]</span></div>
<div style="position:absolute;left:130.42px;top:644.54px" class="cls_006"><span class="cls_006">Google, Welcome to Colaboratory!</span></div>
<div style="position:absolute;left:108.00px;top:669.36px" class="cls_006"><span class="cls_006">[12]</span></div>
<div style="position:absolute;left:130.42px;top:669.36px" class="cls_006"><span class="cls_006">Google Code Archive, word2vec, 2013.</span></div>
<div style="position:absolute;left:108.00px;top:694.19px" class="cls_006"><span class="cls_006">[13]</span></div>
<div style="position:absolute;left:130.42px;top:694.19px" class="cls_006"><span class="cls_006">K. He,  X. Zhang,  S. Ren,  and J. Sun, Deep Residual Learning for Image</span></div>
<div style="position:absolute;left:130.42px;top:710.63px" class="cls_006"><span class="cls_006">Recognition, (2015).</span></div>
<div style="position:absolute;left:108.00px;top:735.45px" class="cls_006"><span class="cls_006">[14]</span></div>
<div style="position:absolute;left:130.42px;top:735.45px" class="cls_006"><span class="cls_006">R. Hermawan, Natural language processing with python, vol. 1, O’Reilly Media</span></div>
<div style="position:absolute;left:130.42px;top:751.89px" class="cls_006"><span class="cls_006">Inc, 2011.</span></div>
<div style="position:absolute;left:310.50px;top:773.49px" class="cls_006"><span class="cls_006">47</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:46860px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background64.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">48</span></div>
<div style="position:absolute;left:400.72px;top:47.41px" class="cls_006"><span class="cls_006">BIBLIOGRAPHY</span></div>
<div style="position:absolute;left:72.00px;top:80.43px" class="cls_006"><span class="cls_006">[15]</span></div>
<div style="position:absolute;left:94.42px;top:80.43px" class="cls_006"><span class="cls_006">G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R.</span></div>
<div style="position:absolute;left:94.42px;top:96.87px" class="cls_006"><span class="cls_006">Salakhutdinov, Improving neural networks by preventing co-adaptation of feature</span></div>
<div style="position:absolute;left:94.42px;top:113.31px" class="cls_006"><span class="cls_006">detectors, tech. rep., 2012.</span></div>
<div style="position:absolute;left:72.00px;top:138.41px" class="cls_006"><span class="cls_006">[16]</span></div>
<div style="position:absolute;left:94.42px;top:138.41px" class="cls_006"><span class="cls_006">S. Hochreiter and J. Urgen Schmidhuber, Lstm, Neural Computation, 9</span></div>
<div style="position:absolute;left:94.42px;top:154.84px" class="cls_006"><span class="cls_006">(1997), pp. 1735-1780.</span></div>
<div style="position:absolute;left:72.00px;top:179.94px" class="cls_006"><span class="cls_006">[17]</span></div>
<div style="position:absolute;left:94.42px;top:179.94px" class="cls_006"><span class="cls_006">M. Hodosh, P. Young, J. Hockenmaier, Micah Hodosh, Peter Young,</span></div>
<div style="position:absolute;left:94.42px;top:196.38px" class="cls_006"><span class="cls_006">Julia Hockenmaier, M. Hodosh, P. Young, and J. Hockenmaier, Framing</span></div>
<div style="position:absolute;left:94.42px;top:212.81px" class="cls_006"><span class="cls_006">Image Description as a Ranking Task :  Data , Models and Evaluation Metrics (</span></div>
<div style="position:absolute;left:94.42px;top:229.25px" class="cls_006"><span class="cls_006">Extended Abstract ) , Tech. Rep. Ijcai, 2015.</span></div>
<div style="position:absolute;left:72.00px;top:254.35px" class="cls_006"><span class="cls_006">[18]</span></div>
<div style="position:absolute;left:94.42px;top:254.35px" class="cls_006"><span class="cls_006">IBM, AI and bias - IBM Research, 2018.</span></div>
<div style="position:absolute;left:72.00px;top:279.45px" class="cls_006"><span class="cls_006">[19]</span></div>
<div style="position:absolute;left:94.42px;top:279.45px" class="cls_006"><span class="cls_006">I. Information and R. Techniques, G91: Providing link text that describes the</span></div>
<div style="position:absolute;left:94.42px;top:295.89px" class="cls_006"><span class="cls_006">purpose of a link, 2018.</span></div>
<div style="position:absolute;left:72.00px;top:320.99px" class="cls_006"><span class="cls_006">[20]</span></div>
<div style="position:absolute;left:94.42px;top:320.99px" class="cls_006"><span class="cls_006">IWorld Internet Users Statistics and 2016 World Population Stats,</span></div>
<div style="position:absolute;left:94.42px;top:337.42px" class="cls_006"><span class="cls_006">World Internet Users Statistics and 2016 World Population Stats, 2016.</span></div>
<div style="position:absolute;left:72.00px;top:362.52px" class="cls_006"><span class="cls_006">[21]</span></div>
<div style="position:absolute;left:94.42px;top:362.52px" class="cls_006"><span class="cls_006">B. KAMGAR-PARSI and B. KAMGAR-PARSI, Hopfield Model and Optimiza-</span></div>
<div style="position:absolute;left:94.42px;top:378.96px" class="cls_006"><span class="cls_006">tion Problems, Neural Networks for Perception, (1992), pp. 94-110.</span></div>
<div style="position:absolute;left:72.00px;top:404.06px" class="cls_006"><span class="cls_006">[22]</span></div>
<div style="position:absolute;left:94.42px;top:404.06px" class="cls_006"><span class="cls_006">A. Krizhevsky, I. Sutskever, and G. E. Hinton, ImageNet classification with</span></div>
<div style="position:absolute;left:94.42px;top:420.50px" class="cls_006"><span class="cls_006">deep convolutional neural networks, tech. rep., 2012.</span></div>
<div style="position:absolute;left:72.00px;top:445.60px" class="cls_006"><span class="cls_006">[23]</span></div>
<div style="position:absolute;left:94.42px;top:445.60px" class="cls_006"><span class="cls_006">M. J. Kusner, Y. Sun, N. I. Kolkin, and K. Q. Weinberger, From word</span></div>
<div style="position:absolute;left:94.42px;top:462.03px" class="cls_006"><span class="cls_006">embeddings to document distances, tech. rep., 2015.</span></div>
<div style="position:absolute;left:72.00px;top:487.13px" class="cls_006"><span class="cls_006">[24]</span></div>
<div style="position:absolute;left:94.42px;top:487.13px" class="cls_006"><span class="cls_006">A. Lavie and A. Agarwal, METEOR: An Automatic Metric for MT Evaluation</span></div>
<div style="position:absolute;left:94.42px;top:503.57px" class="cls_006"><span class="cls_006">with Improved Correlation with Human Judgments, tech. rep., 2007.</span></div>
<div style="position:absolute;left:72.00px;top:528.67px" class="cls_006"><span class="cls_006">[25]</span></div>
<div style="position:absolute;left:94.42px;top:528.67px" class="cls_006"><span class="cls_006">L.-J. Li,  K. Li,  F. F. Li,  J. Deng,  W. Dong,  R. Socher,  and L. Fei-</span></div>
<div style="position:absolute;left:94.42px;top:545.11px" class="cls_006"><span class="cls_006">Fei, ImageNet:  a Large-Scale Hierarchical Image Database Shrimp Project View</span></div>
<div style="position:absolute;left:94.42px;top:561.54px" class="cls_006"><span class="cls_006">project hybrid intrusion detction systems View project ImageNet:  A Large-Scale</span></div>
<div style="position:absolute;left:94.42px;top:577.98px" class="cls_006"><span class="cls_006">Hierarchical Image Database, in 2009 IEEE Conference on Computer Vision and</span></div>
<div style="position:absolute;left:94.42px;top:594.41px" class="cls_006"><span class="cls_006">Pattern Recognition, IEEE, jun 2009, pp. 248-255.</span></div>
<div style="position:absolute;left:72.00px;top:619.51px" class="cls_006"><span class="cls_006">[26]</span></div>
<div style="position:absolute;left:94.42px;top:619.51px" class="cls_006"><span class="cls_006">C. Y. Lin, Rouge: A package for automatic evaluation of summaries, Tech. Rep. 1,</span></div>
<div style="position:absolute;left:94.42px;top:635.95px" class="cls_006"><span class="cls_006">2004.</span></div>
<div style="position:absolute;left:72.00px;top:661.05px" class="cls_006"><span class="cls_006">[27]</span></div>
<div style="position:absolute;left:94.42px;top:661.05px" class="cls_006"><span class="cls_006">T. Y. Lin,  M. Maire,  S. Belongie,  J. Hays,  P. Perona,  D. Ramanan,</span></div>
<div style="position:absolute;left:94.42px;top:676.57px" class="cls_006"><span class="cls_006">P. Dollár, and C. L. Zitnick, Microsoft COCO: Common objects in context,</span></div>
<div style="position:absolute;left:94.42px;top:693.92px" class="cls_006"><span class="cls_006">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial</span></div>
<div style="position:absolute;left:94.42px;top:710.35px" class="cls_006"><span class="cls_006">Intelligence and Lecture Notes in Bioinformatics), 8693 LNCS (2014), pp. 740-755.</span></div>
<div style="position:absolute;left:72.00px;top:735.45px" class="cls_006"><span class="cls_006">[28]</span></div>
<div style="position:absolute;left:94.42px;top:735.45px" class="cls_006"><span class="cls_006">Machine Learning Plus, Cosine similarity  understanding the math and how it</span></div>
<div style="position:absolute;left:94.42px;top:751.89px" class="cls_006"><span class="cls_006">works (with python codes), 2019.</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:47712px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background65.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">BIBLIOGRAPHY</span></div>
<div style="position:absolute;left:513.00px;top:47.41px" class="cls_006"><span class="cls_006">49</span></div>
<div style="position:absolute;left:108.00px;top:80.43px" class="cls_006"><span class="cls_006">[29]</span></div>
<div style="position:absolute;left:130.42px;top:80.43px" class="cls_006"><span class="cls_006">S. Makridakis, The forthcoming Artificial Intelligence (AI) revolution: Its impact</span></div>
<div style="position:absolute;left:130.42px;top:96.87px" class="cls_006"><span class="cls_006">on society and firms, Futures, 90 (2017), pp. 46-60.</span></div>
<div style="position:absolute;left:108.00px;top:121.91px" class="cls_006"><span class="cls_006">[30]</span></div>
<div style="position:absolute;left:130.42px;top:121.91px" class="cls_006"><span class="cls_006">Microsoft Azure Documentation, Detect common objects in images, 2019.</span></div>
<div style="position:absolute;left:108.00px;top:146.95px" class="cls_006"><span class="cls_006">[31]</span></div>
<div style="position:absolute;left:130.42px;top:146.95px" class="cls_006"><span class="cls_006">T. Mikolov, K. Chen, G. Corrado, and J. Dean, Efficient Estimation of</span></div>
<div style="position:absolute;left:130.42px;top:163.39px" class="cls_006"><span class="cls_006">Word Representations in Vector Space, (2013).</span></div>
<div style="position:absolute;left:108.00px;top:188.43px" class="cls_006"><span class="cls_006">[32]</span></div>
<div style="position:absolute;left:130.42px;top:188.43px" class="cls_006"><span class="cls_006">National Institute of Neurological Disorders and Stroke, Dyslexia in-</span></div>
<div style="position:absolute;left:130.42px;top:204.86px" class="cls_006"><span class="cls_006">formation page, 2019.</span></div>
<div style="position:absolute;left:108.00px;top:229.91px" class="cls_006"><span class="cls_006">[33]</span></div>
<div style="position:absolute;left:130.42px;top:229.91px" class="cls_006"><span class="cls_006">J. Oldman, 10 Free Screen Readers For Blind Or Visually Impaired Users, 2012.</span></div>
<div style="position:absolute;left:108.00px;top:254.95px" class="cls_006"><span class="cls_006">[34]</span></div>
<div style="position:absolute;left:130.42px;top:254.95px" class="cls_006"><span class="cls_006">P. Oram,  WordNet:  An electronic lexical database. Christiane Fellbaum (Ed.).</span></div>
<div style="position:absolute;left:130.42px;top:271.38px" class="cls_006"><span class="cls_006">Cambridge, MA: MIT Press, 1998. Pp. 423. , Applied Psycholinguistics, 22 (2001),</span></div>
<div style="position:absolute;left:130.42px;top:287.82px" class="cls_006"><span class="cls_006">pp. 131-134.</span></div>
<div style="position:absolute;left:108.00px;top:312.86px" class="cls_006"><span class="cls_006">[35]</span></div>
<div style="position:absolute;left:130.42px;top:311.95px" class="cls_006"><span class="cls_006">Oranización Mundial de la Salud, WHO — World report on disability, Who,</span></div>
<div style="position:absolute;left:130.42px;top:329.29px" class="cls_006"><span class="cls_006">(2016).</span></div>
<div style="position:absolute;left:108.00px;top:354.34px" class="cls_006"><span class="cls_006">[36]</span></div>
<div style="position:absolute;left:130.42px;top:354.34px" class="cls_006"><span class="cls_006">K. Papineni,  S. Roukos,  T. Ward,  and W.-J. Zhu, BLEU: a Method for</span></div>
<div style="position:absolute;left:130.42px;top:370.77px" class="cls_006"><span class="cls_006">Automatic Evaluation of Machine Translation, tech. rep., 2002.</span></div>
<div style="position:absolute;left:108.00px;top:395.81px" class="cls_006"><span class="cls_006">[37]</span></div>
<div style="position:absolute;left:130.42px;top:395.81px" class="cls_006"><span class="cls_006">J. Ramos, Using TF-IDF to Determine Word Relevance in Document Queries,</span></div>
<div style="position:absolute;left:130.42px;top:412.25px" class="cls_006"><span class="cls_006">Tech. Rep. 4, 2003.</span></div>
<div style="position:absolute;left:108.00px;top:437.29px" class="cls_006"><span class="cls_006">[38]</span></div>
<div style="position:absolute;left:130.42px;top:437.29px" class="cls_006"><span class="cls_006">H. Reese, Microsoft’s new AI app to assist the blind could be a ’game changer’ in</span></div>
<div style="position:absolute;left:130.42px;top:453.72px" class="cls_006"><span class="cls_006">accessibility - TechRepublic.</span></div>
<div style="position:absolute;left:108.00px;top:478.77px" class="cls_006"><span class="cls_006">[39]</span></div>
<div style="position:absolute;left:130.42px;top:476.01px" class="cls_006"><span class="cls_006">R.Řehůřek and P. Sojka, Software Framework for Topic Modelling with Large</span></div>
<div style="position:absolute;left:130.42px;top:495.20px" class="cls_006"><span class="cls_006">Corpora, in Proceedings of the LREC 2010 Workshop on New Challenges for NLP</span></div>
<div style="position:absolute;left:130.42px;top:511.64px" class="cls_006"><span class="cls_006">Frameworks, Valletta, Malta, May 2010, ELRA, pp. 45-50. </span><A HREF="http://is.muni.cz/publication/884893/en">http://is.muni.cz/</A> </div>
<div style="position:absolute;left:130.42px;top:528.07px" class="cls_006"><span class="cls_006"> </span><A HREF="http://is.muni.cz/publication/884893/en">publication/884893/en</A>.</div>
<div style="position:absolute;left:108.00px;top:553.11px" class="cls_006"><span class="cls_006">[40]</span></div>
<div style="position:absolute;left:130.42px;top:553.11px" class="cls_006"><span class="cls_006">L. Richardson, Beautiful Soup Documentation, 2016.</span></div>
<div style="position:absolute;left:108.00px;top:578.15px" class="cls_006"><span class="cls_006">[41]</span></div>
<div style="position:absolute;left:130.42px;top:578.15px" class="cls_006"><span class="cls_006">B. Rohrer, How LSTM and RNN work, 2017.</span></div>
<div style="position:absolute;left:108.00px;top:603.20px" class="cls_006"><span class="cls_006">[42]</span></div>
<div style="position:absolute;left:130.42px;top:603.20px" class="cls_006"><span class="cls_006">O.  Russakovsky,  J.  Deng,  H.  Su,  J.  Krause,  S.  Satheesh,  S.  Ma,</span></div>
<div style="position:absolute;left:130.42px;top:619.63px" class="cls_006"><span class="cls_006">Z. Huang,  A. Karpathy,  A. Khosla,  M. Bernstein,  A. C. Berg,  and</span></div>
<div style="position:absolute;left:130.42px;top:636.07px" class="cls_006"><span class="cls_006">L. Fei-Fei, ImageNet Large Scale Visual Recognition Challenge, Tech. Rep. 3,</span></div>
<div style="position:absolute;left:130.42px;top:652.50px" class="cls_006"><span class="cls_006">2015.</span></div>
<div style="position:absolute;left:108.00px;top:677.54px" class="cls_006"><span class="cls_006">[43]</span></div>
<div style="position:absolute;left:130.42px;top:676.63px" class="cls_006"><span class="cls_006">Sébastien Pertus, How Equadex used Cognitive Services to help people with lan-</span></div>
<div style="position:absolute;left:130.42px;top:693.98px" class="cls_006"><span class="cls_006">guage disorders — Microsoft Technical Case Studies, 2017.</span></div>
<div style="position:absolute;left:108.00px;top:719.02px" class="cls_006"><span class="cls_006">[44]</span></div>
<div style="position:absolute;left:130.42px;top:719.02px" class="cls_006"><span class="cls_006">P. Sharma, N. Ding, S. Goodman, and R. Soricut, Conceptual captions: A</span></div>
<div style="position:absolute;left:130.42px;top:735.45px" class="cls_006"><span class="cls_006">cleaned, hypernymed, image alt-text dataset for automatic image captioning, tech.</span></div>
<div style="position:absolute;left:130.42px;top:751.89px" class="cls_006"><span class="cls_006">rep., 2018.</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:48564px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background66.jpg" width=595 height=842></div>
<div style="position:absolute;left:72.00px;top:47.41px" class="cls_006"><span class="cls_006">50</span></div>
<div style="position:absolute;left:400.72px;top:47.41px" class="cls_006"><span class="cls_006">BIBLIOGRAPHY</span></div>
<div style="position:absolute;left:72.00px;top:80.43px" class="cls_006"><span class="cls_006">[45]</span></div>
<div style="position:absolute;left:94.42px;top:80.43px" class="cls_006"><span class="cls_006">A. Sherstinsky, Fundamentals of Recurrent Neural Network (RNN) and Long</span></div>
<div style="position:absolute;left:94.42px;top:96.87px" class="cls_006"><span class="cls_006">Short-Term Memory (LSTM) Network, tech. rep., 2018.</span></div>
<div style="position:absolute;left:72.00px;top:122.27px" class="cls_006"><span class="cls_006">[46]</span></div>
<div style="position:absolute;left:94.42px;top:122.27px" class="cls_006"><span class="cls_006">K. Simonyan and A. Zisserman, Very Deep Convolutional Networks for Large-</span></div>
<div style="position:absolute;left:94.42px;top:138.71px" class="cls_006"><span class="cls_006">Scale Image Recognition, tech. rep., 2014.</span></div>
<div style="position:absolute;left:72.00px;top:164.11px" class="cls_006"><span class="cls_006">[47]</span></div>
<div style="position:absolute;left:94.42px;top:164.11px" class="cls_006"><span class="cls_006">S. Singh, A. Subramanya, F. Pereira, and A. McCallum, Wikilinks:  A</span></div>
<div style="position:absolute;left:94.42px;top:180.54px" class="cls_006"><span class="cls_006">large-scale cross-document coreference corpus labeled via links to Wikipedia, tech.</span></div>
<div style="position:absolute;left:94.42px;top:196.98px" class="cls_006"><span class="cls_006">rep., 2012.</span></div>
<div style="position:absolute;left:72.00px;top:222.38px" class="cls_006"><span class="cls_006">[48]</span></div>
<div style="position:absolute;left:94.42px;top:222.38px" class="cls_006"><span class="cls_006">I. Sutskever, O. Vinyals, and Q. V. Le, Sequence to sequence learning with</span></div>
<div style="position:absolute;left:94.42px;top:238.82px" class="cls_006"><span class="cls_006">neural networks, Tech. Rep. January, 2014.</span></div>
<div style="position:absolute;left:72.00px;top:264.22px" class="cls_006"><span class="cls_006">[49]</span></div>
<div style="position:absolute;left:94.42px;top:264.22px" class="cls_006"><span class="cls_006">C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er-</span></div>
<div style="position:absolute;left:94.42px;top:280.65px" class="cls_006"><span class="cls_006">han, V. Vanhoucke, and A. Rabinovich, Going Deeper with Convolutions,</span></div>
<div style="position:absolute;left:94.42px;top:297.09px" class="cls_006"><span class="cls_006">tech. rep.</span></div>
<div style="position:absolute;left:72.00px;top:322.49px" class="cls_006"><span class="cls_006">[50]</span></div>
<div style="position:absolute;left:94.42px;top:322.49px" class="cls_006"><span class="cls_006">M. Tanti, A. Gatt, and K. P. Camilleri, Where to put the image in an image</span></div>
<div style="position:absolute;left:94.42px;top:338.92px" class="cls_006"><span class="cls_006">caption generator, Tech. Rep. 3, 2018.</span></div>
<div style="position:absolute;left:72.00px;top:364.33px" class="cls_006"><span class="cls_006">[51]</span></div>
<div style="position:absolute;left:94.42px;top:364.33px" class="cls_006"><span class="cls_006">A. Thomas, Recurrent neural networks and LSTM tutorial in Python and Tensor-</span></div>
<div style="position:absolute;left:94.42px;top:380.76px" class="cls_006"><span class="cls_006">Flow - Adventures in Machine Learning, 2017.</span></div>
<div style="position:absolute;left:72.00px;top:406.16px" class="cls_006"><span class="cls_006">[52]</span></div>
<div style="position:absolute;left:117.34px;top:406.16px" class="cls_006"><span class="cls_006">, Word2vec word embedding tutorial in python and tensorflow, 2019.</span></div>
<div style="position:absolute;left:72.00px;top:431.56px" class="cls_006"><span class="cls_006">[53]</span></div>
<div style="position:absolute;left:94.42px;top:431.56px" class="cls_006"><span class="cls_006">L. Van Der Maaten and G. Hinton, Visualizing Data using t-SNE, tech. rep.,</span></div>
<div style="position:absolute;left:94.42px;top:448.00px" class="cls_006"><span class="cls_006">2008.</span></div>
<div style="position:absolute;left:72.00px;top:473.40px" class="cls_006"><span class="cls_006">[54]</span></div>
<div style="position:absolute;left:94.42px;top:473.40px" class="cls_006"><span class="cls_006">R. Vedantam, C. L. Zitnick, and D. Parikh, CIDEr: Consensus-based image</span></div>
<div style="position:absolute;left:94.42px;top:489.83px" class="cls_006"><span class="cls_006">description evaluation, tech. rep., 2015.</span></div>
<div style="position:absolute;left:72.00px;top:515.24px" class="cls_006"><span class="cls_006">[55]</span></div>
<div style="position:absolute;left:94.42px;top:515.24px" class="cls_006"><span class="cls_006">G. von Zitzewitz, Survey of neural networks in autonomous driving, (2017).</span></div>
<div style="position:absolute;left:72.00px;top:540.64px" class="cls_006"><span class="cls_006">[56]</span></div>
<div style="position:absolute;left:94.42px;top:540.64px" class="cls_006"><span class="cls_006">W3C, H37:  Using alt attributes on img elements — Techniques for WCAG 2.0,</span></div>
<div style="position:absolute;left:94.42px;top:557.07px" class="cls_006"><span class="cls_006">2008.</span></div>
<div style="position:absolute;left:72.00px;top:582.47px" class="cls_006"><span class="cls_006">[57]</span></div>
<div style="position:absolute;left:117.34px;top:582.47px" class="cls_006"><span class="cls_006">, How WAI Develops Accessibility Standards through the W3C Process: Mile-</span></div>
<div style="position:absolute;left:94.42px;top:598.91px" class="cls_006"><span class="cls_006">stones and Opportunities to Contribute — Web Accessibility Initiative (WAI) —</span></div>
<div style="position:absolute;left:94.42px;top:615.34px" class="cls_006"><span class="cls_006">W3C, 2019.</span></div>
<div style="position:absolute;left:72.00px;top:640.75px" class="cls_006"><span class="cls_006">[58]</span></div>
<div style="position:absolute;left:94.42px;top:640.75px" class="cls_006"><span class="cls_006">W3cdevs, Web Content Accessibility Guidelines (WCAG) Overview — Web Ac-</span></div>
<div style="position:absolute;left:94.42px;top:657.18px" class="cls_006"><span class="cls_006">cessibility Initiative (WAI) — W3C, 2019.</span></div>
<div style="position:absolute;left:72.00px;top:682.58px" class="cls_006"><span class="cls_006">[59]</span></div>
<div style="position:absolute;left:94.42px;top:682.58px" class="cls_006"><span class="cls_006">Weisstein, Eric W, ”sigmoid function.” from mathworld-a wolfram web resource.</span></div>
<div style="position:absolute;left:72.00px;top:707.98px" class="cls_006"><span class="cls_006">[60]</span></div>
<div style="position:absolute;left:94.42px;top:707.98px" class="cls_006"><span class="cls_006">Wikipedia contributors, Euclidean distance — Wikipedia, the free encyclopedia,</span></div>
<div style="position:absolute;left:94.42px;top:724.42px" class="cls_006"><span class="cls_006">2019. [Online; accessed 4-September-2019].</span></div>
</div>
<div style="position:absolute;left:50%;margin-left:-297px;top:49416px;width:595px;height:842px;border-style:outset;overflow:hidden">
<div style="position:absolute;left:0px;top:0px">
<img src="d9fd3574-213d-11ec-a980-0cc47a792c0a_id_d9fd3574-213d-11ec-a980-0cc47a792c0a_files/background67.jpg" width=595 height=842></div>
<div style="position:absolute;left:108.00px;top:47.41px" class="cls_006"><span class="cls_006">BIBLIOGRAPHY</span></div>
<div style="position:absolute;left:513.00px;top:47.41px" class="cls_006"><span class="cls_006">51</span></div>
<div style="position:absolute;left:108.00px;top:80.43px" class="cls_006"><span class="cls_006">[61]</span></div>
<div style="position:absolute;left:153.34px;top:80.43px" class="cls_006"><span class="cls_006">, Hierarchical data format — Wikipedia, the free encyclopedia, 2019. [Online;</span></div>
<div style="position:absolute;left:130.42px;top:96.87px" class="cls_006"><span class="cls_006">accessed 4-September-2019].</span></div>
<div style="position:absolute;left:108.00px;top:122.27px" class="cls_006"><span class="cls_006">[62]</span></div>
<div style="position:absolute;left:153.34px;top:122.27px" class="cls_006"><span class="cls_006">, N-gram — Wikipedia,  the free encyclopedia,  2019.</span></div>
<div style="position:absolute;left:424.99px;top:122.27px" class="cls_006"><span class="cls_006">[Online;  accessed 5-</span></div>
<div style="position:absolute;left:130.42px;top:138.71px" class="cls_006"><span class="cls_006">September-2019].</span></div>
<div style="position:absolute;left:108.00px;top:164.11px" class="cls_006"><span class="cls_006">[63]</span></div>
<div style="position:absolute;left:153.34px;top:164.11px" class="cls_006"><span class="cls_006">, Softmax function — Wikipedia, the free encyclopedia, 2019. [Online; accessed</span></div>
<div style="position:absolute;left:130.42px;top:180.54px" class="cls_006"><span class="cls_006">4-September-2019].</span></div>
<div style="position:absolute;left:108.00px;top:205.94px" class="cls_006"><span class="cls_006">[64]</span></div>
<div style="position:absolute;left:153.34px;top:205.94px" class="cls_006"><span class="cls_006">, Web content accessibility guidelines — Wikipedia, the free encyclopedia, 2019.</span></div>
<div style="position:absolute;left:130.42px;top:222.38px" class="cls_006"><span class="cls_006">[Online; accessed 4-September-2019].</span></div>
<div style="position:absolute;left:108.00px;top:247.78px" class="cls_006"><span class="cls_006">[65] K. Xu, J. L. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. S.</span></div>
<div style="position:absolute;left:130.42px;top:264.22px" class="cls_006"><span class="cls_006">Zemel, and Y. Bengio, Show, attend and tell: Neural image caption generation</span></div>
<div style="position:absolute;left:130.42px;top:280.65px" class="cls_006"><span class="cls_006">with visual attention, tech. rep., 2015.</span></div>
<div style="position:absolute;left:108.00px;top:306.05px" class="cls_006"><span class="cls_006">[66] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, How transferable are features</span></div>
<div style="position:absolute;left:130.42px;top:322.49px" class="cls_006"><span class="cls_006">in deep neural networks?, Advances in Neural Information Processing Systems, 4</span></div>
<div style="position:absolute;left:130.42px;top:338.92px" class="cls_006"><span class="cls_006">(2014), pp. 3320-3328.</span></div>
</div>

</body>
</html>
